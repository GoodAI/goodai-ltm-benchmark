.
├── Dockerfile
├── __pycache__
│   └── config.cpython-312.pyc
├── config.py
├── data
│   ├── 02c5010b543e.db
│   ├── 3d900cef16ed.db
│   ├── 43518d53f98d.db
│   ├── 58a0e7a8334f.db
│   ├── 6288678806d5.db
│   ├── 90f419cf105f.db
│   ├── da6316bea3ca.db
│   └── f87914c34446.db
├── docker-compose.yml
├── docker_requirements.txt
├── docs
│   └── README.md
├── json_output
│   ├── Memory_1.json
│   ├── Memory_2.json
│   ├── Memory_3.json
│   ├── Memory_4.json
│   └── Memory_5.json
├── logs
│   ├── 20240712_124040_fba618fa6ca7
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_124625_a495eb43508d
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_130127_b912ba994df8
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_130128_b912ba994df8
│   │   └── app.log
│   ├── 20240712_130607_ec771763c014
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_133657_40478d8aa5b4
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_134059_3d900cef16ed
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_134351_02c5010b543e
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_134656_6288678806d5
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_135147_da6316bea3ca
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_142151_58a0e7a8334f
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_155221_43518d53f98d
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_160046_90f419cf105f
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240712_160900_f87914c34446
│   │   ├── app.log
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   └── docker_agents
│       ├── 20240621_122849_c4871e6f4b4c
│       │   ├── chat.log
│       │   ├── master.log
│       │   └── memory.log
│       ├── 20240621_131848_0c9335780884
│       │   ├── chat.log
│       │   ├── master.log
│       │   └── memory.log
│       ├── 20240621_134307_9108ac89f851
│       │   ├── chat.log
│       │   ├── master.log
│       │   └── memory.log
│       ├── 20240621_134614_240eb5a9fa25
│       │   ├── chat.log
│       │   ├── master.log
│       │   └── memory.log
│       ├── 20240621_134832_e32f8be7080c
│       │   ├── chat.log
│       │   ├── master.log
│       │   └── memory.log
│       └── 20240621_140806_5d95a2ed862b
│           ├── chat.log
│           ├── master.log
│           └── memory.log
├── memory.db
├── notebooks
│   └── Multi_Agent_RAG_LangGraph.ipynb
├── scripts
│   ├── _docker_reset.sh
│   ├── generate_tree.sh
│   ├── logs_to_docs.sh
│   ├── manage_docker.sh
│   └── reset_logs.sh
├── src
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── __init__.cpython-312.pyc
│   │   ├── controller.cpython-310.pyc
│   │   └── controller.cpython-312.pyc
│   ├── agents
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-310.pyc
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   ├── processing_agent.cpython-312.pyc
│   │   │   ├── response_agent.cpython-312.pyc
│   │   │   ├── retrieval_agent.cpython-310.pyc
│   │   │   └── retrieval_agent.cpython-312.pyc
│   │   └── agent.py
│   ├── api.py
│   ├── app.py
│   ├── cli.py
│   ├── memory
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   └── memory_manager.cpython-312.pyc
│   │   └── memory_manager.py
│   └── utils
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-312.pyc
│       │   ├── data_utils.cpython-312.pyc
│       │   └── json_utils.cpython-312.pyc
│       ├── controller.py
│       ├── enhanced_logging.py
│       ├── error_handling.py
│       ├── log_sync.py
│       ├── logging_config.py
│       ├── logging_setup.py
│       ├── structured_logging.py
│       ├── tracing.py
│       └── visualizer.py
└── tree.txt

36 directories, 141 files

Note: The following list of files and directories are excluded only from the appended file contents section:

__pycache__, .git, .env, text_docs, scripts, logs, tree.txt, generate_tree.sh, README, *.ipynb, *.pdf, *.db

File Contents:

-e 
File: ./.dockerignore

__pycache__
*.pyc
*.pyo
*.pyd
*.pdb
*.egg
*.egg-info
dist
build
eggs
parts
var
develop-eggs
.installed.cfg
lib
lib64
*.log
logs
notebooks
data/raw/*
text_docs/*
.DS_Store
.vscode
-e 
File: ./config.py

import os
from dotenv import load_dotenv

class Config:
    def __init__(self):
        load_dotenv()
        
        # General settings
        self.MODEL_NAME = os.getenv("MODEL_NAME", "llama3-70b-8192")
        
        # Database settings
        self.personal_db_path = self._get_personal_db_path()
        
        # API Keys
        self.OPENAI_API_KEY = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
        self.TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
        self.GROQ_API_KEY = os.getenv("GROQ_API_KEY")
        
        # Other settings
        self.LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG")
        
        # Processing agent settings
        self.PROCESSING_AGENT_MEMORIES_INCLUDED = int(os.getenv("PROCESSING_AGENT_MEMORIES_INCLUDED", "5"))
        self.MEMORY_RETRIEVAL_THRESHOLD = float(os.getenv("MEMORY_RETRIEVAL_THRESHOLD", "0.75"))
        self.MEMORY_RETRIEVAL_LIMIT = int(os.getenv("MEMORY_RETRIEVAL_LIMIT", "10"))

    def _get_memory_db_path(self):
        return "/app/memory.db" if os.path.exists("/.dockerenv") else "memory.db"
    
    def _get_personal_db_path(self):
        container_id = os.environ.get('HOSTNAME', 'local')
        return f"/app/data/{container_id}.db" if os.path.exists("/.dockerenv") else f"{container_id}.db"

    def validate_api_keys(self):
        return bool(self.GROQ_API_KEY and self.TAVILY_API_KEY)

    L2_NORM_THRESHOLD = float(os.getenv("L2_NORM_THRESHOLD", "0.75"))
    COSINE_SIMILARITY_THRESHOLD = float(os.getenv("COSINE_SIMILARITY_THRESHOLD", "0.75"))
    BM25_THRESHOLD = float(os.getenv("BM25_THRESHOLD", "0.5"))
    JACCARD_SIMILARITY_THRESHOLD = float(os.getenv("JACCARD_SIMILARITY_THRESHOLD", "0.3"))

config = Config()-e 
File: ./docker-compose.yml

services:

  loki:
    image: grafana/loki:2.8.0
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./loki-config.yaml:/etc/loki/local-config.yaml

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    volumes:
      - ./grafana-provisioning:/etc/grafana/provisioning

  promtail:
    image: grafana/promtail:2.8.0
    volumes:
      - ./logs:/var/log
      - ./promtail-config.yaml:/etc/promtail/config.yaml
    command: -config.file=/etc/promtail/config.yaml
    
  multi-agent-rag:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    ports:
      - "8080:8000"
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GOODAI_OPENAI_API_KEY_LTM01=${GOODAI_OPENAI_API_KEY_LTM01}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    command: ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]

  multi-agent-rag-cli:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GOODAI_OPENAI_API_KEY_LTM01=${GOODAI_OPENAI_API_KEY_LTM01}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./local_logs:/app/local_logs
    command: ["python", "-m", "src.cli"]
    stdin_open: true
    tty: true-e 
File: ./Dockerfile

FROM python:3.12

# Create a non-root user
RUN useradd -m appuser

WORKDIR /app

COPY . /app

RUN mkdir -p /app/logs /app/data
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Create and activate virtual environment
RUN python -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Upgrade pip and install dependencies with increased timeout and using a mirror
RUN pip install --upgrade pip

# Install numpy and scipy separately with increased timeout
RUN pip install --no-cache-dir numpy==1.26.4 scipy==1.14.0 --timeout 600

# Install scikit-learn separately with an even longer timeout
RUN pip install --no-cache-dir scikit-learn==1.4.0 --timeout 900

# Install other dependencies
RUN pip install --no-cache-dir -r docker_requirements.txt --timeout 600 --index-url https://pypi.org/simple --trusted-host pypi.org

RUN pip install fastapi uvicorn

RUN pip install networkx matplotlib

RUN pip install structlog==24.1.0

RUN pip install structlog aiologger

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "-u", "src/utils/log_sync.py", "uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]-e 
File: ./docker_requirements.txt

langchain==0.2.3
langchain-openai==0.1.8
langchain-community==0.2.4
langchain-core==0.2.5
langchain-text-splitters==0.2.1
langsmith==0.1.77
openai==1.34.0
pydantic==2.7.4
pydantic-core==2.18.4
dataclasses-json==0.6.7
python-dotenv==1.0.1
aiosqlite
rank_bm25==0.2.2
cachetools==5.3.0
groq
fastapi
uvicorn
opentelemetry-api==1.22.0
opentelemetry-sdk==1.22.0
opentelemetry-instrumentation-fastapi==0.43b0
aiologger==0.7.0
aiofiles-e 
File: ./docs/README.md

# Multi-Agent RAG System with Continual Learning

## Project Overview

This project implements a vertical slice of a multi-agent RAG (Retrieval-Augmented Generation) system with continual learning capabilities. It uses LangChain, LangGraph, and LLM API calls to create a sophisticated question-answering system that learns from interactions.

## Key Features

- Multi-agent architecture for complex query processing
- RAG (Retrieval-Augmented Generation) for enhanced responses
- Continual learning to improve performance over time
- Distributed memory management with personal and master databases
- CLI and API interfaces for versatile interaction
- Comprehensive logging system for debugging and analysis
- Docker support for easy deployment and scaling

## System Architecture

The system consists of the following main components:

1. **Controller**: Orchestrates the overall flow of query processing.
2. **Agents**: Specialized modules for retrieval, processing, and response generation.
3. **Memory Manager**: Handles storage and retrieval of past interactions.
4. **Logging System**: Provides detailed logs for system operations, including database interactions.

## Directory Structure

```
.
├── Dockerfile
├── README.md
├── config.py
├── data/
├── docker-compose.yml
├── docs/
├── logs/
├── notebooks/
├── scripts/
├── src/
│   ├── agents/
│   ├── memory/
│   ├── utils/
│   ├── api.py
│   ├── app.py
│   └── cli.py
└── tests/
```

## Setup and Installation

### Prerequisites

- Python 3.12+
- Docker and Docker Compose

### Local Setup

1. Clone the repository:
   ```
   git clone https://github.com/your-username/multi-agent-rag.git
   cd multi-agent-rag
   ```

2. Create a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   Create a `.env` file in the root directory with the following content:
   ```
   OPENAI_API_KEY=your_openai_api_key
   TAVILY_API_KEY=your_tavily_api_key
   ```

### Docker Setup

1. Build the Docker image:
   ```
   docker-compose build
   ```

2. Run the container:
   ```
   docker-compose up
   ```

## Usage

### CLI Interface

Run the CLI interface with:

```
python src/cli.py
```

Available commands:
- `query`: Enter a query to process
- `memories`: Retrieve recent memories
- `consistency`: Run a consistency check on the databases
- `quit`: Exit the program

### API Interface

Start the API server with:

```
uvicorn src.api:app --host 0.0.0.0 --port 8000
```

API endpoints:
- POST `/query`: Submit a query for processing
- GET `/consistency_check`: Run a consistency check on the databases
- GET `/health`: Check the health status of the API
- GET `/memory_distribution`: Get the distribution of memories across containers

## Logging

Logs are stored in the `logs/` directory, organized by timestamp and container ID. There are separate log files for:

- Master log (`master.log`)
- Chat log (`chat.log`)
- Memory operations log (`memory.log`)
- Database operations log (`database.log`)

## Testing

Run the test suite with:

```
pytest tests/
```
-e 
File: ./json_output/Memory_1.json

{
    "title": "Memory_1",
    "query": "Today is Wednesday, June 19th.",
    "result": "I apologize for the mistake in my initial response. Today is indeed Tuesday, June 18th. Thank you for bringing it to my attention.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_2.json

{
    "title": "Memory_2",
    "query": "What is the day today? ",
    "result": "Today is Tuesday, June 18th.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_3.json

{
    "title": "Memory_3",
    "query": "This is a test. Respond with how tall I am. ",
    "result": "I apologize for the incorrect response earlier. As a language model AI, I do not have the ability to determine your height. If you have any other questions or need assistance with something else, feel free to ask!",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_4.json

{
    "title": "Memory_4",
    "query": "What is my name? Use your context.",
    "result": "I'm sorry, I made an error in my initial response. Without knowing your name, I cannot accurately determine what it is based on the context provided. If you would like to share your name with me, I can assist you further.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_5.json

{
    "title": "Memory_5",
    "query": "What is my name? ",
    "result": "I apologize for the confusion. Without knowing your actual name, I can only provide possible guesses. If you could kindly provide me with your name, I would be happy to address you correctly.",
    "tags": "retrieved, processed"
}-e 
File: ./src/agents/agent.py

import logging
from groq import Groq # type: ignore
from config import config
import json
import asyncio

from src.utils.structured_logging import get_logger
from src.utils.enhanced_logging import log_execution_time

logger = get_logger("agent")

class Agent:
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager
        self.groq_client = Groq(api_key=config.GROQ_API_KEY)
        
    @log_execution_time
    async def process_query(self, query: str) -> str:
        relevant_memories = await self.memory_manager.retrieve_relevant_memories(query)
        
        messages = [
            {
                "role": "system",
                "content": """You are a highly efficient, no-nonsense administrator at the core of a sophisticated memory system. Your primary role is to analyze retrieved memories and provide concise, relevant summaries or indicate their lack of relevance. You value brevity and directness above all else.

    The retrieved memories you will be processing are structured as follows:

    1. Each memory entry contains: <memory_id>, <previous_query>, <previous_response>, <similarity_score>
    2. Some entries may have an additional field indicating if they are linked to another memory.

    Analyze these memories in relation to the current query efficiently, paying special attention to linked memories."""
            },
            {
                "role": "user",
                "content": f"""QUERY: {query}

    RETRIEVED MEMORIES:
    {relevant_memories}

    Your task:

    1. Assess if the retrieved memories contain ANY relevant information to the query.
    2. If NO relevant information exists, respond ONLY with: "NO RELEVANT INFORMATION"
    3. If relevant information exists, provide a brief, bullet-point summary of ONLY the most pertinent points. Be extremely concise.
    4. Pay special attention to linked memories and their relevance to the current query.

    Your response should either be "NO RELEVANT INFORMATION" or a brief, focused summary. Exclude any explanation or elaboration."""
            }
        ]

        # Use asyncio.to_thread to run the synchronous API call in a separate thread
        response = await asyncio.to_thread(
            self.groq_client.chat.completions.create,
            messages=messages,
            model=config.MODEL_NAME,
        )

        return response.choices[0].message.content-e 
File: ./src/agents/__init__.py

-e 
File: ./src/api.py

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging
from src.utils.tracing import setup_tracing, instrument_fastapi, tracer
from config import config
from src.utils.error_handling import global_exception_handler
from src.utils.structured_logging import get_logger
from src.utils.tracing import setup_tracing, instrument_fastapi, tracer
from src.utils.error_handling import global_exception_handler
import os

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    response: str

# Initialize logging
master_logger, chat_logger, memory_logger, database_logger = setup_logging()
logger = get_logger("api")
app = FastAPI()
app.add_exception_handler(Exception, global_exception_handler)
setup_tracing()
instrument_fastapi(app)
memory_analyzer = None

controller = None

@app.on_event("startup")
async def startup_event():
    global controller
    logger.info("Starting up the API server")
    controller = Controller()
    await controller.initialize()
    logger.info("API server initialization complete")

@app.on_event("shutdown")
async def shutdown_event():
    master_logger.info("Shutting down the API server")

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    with tracer.start_as_current_span("query_processing"):
        try:
            logger.info(f"Received query: {request.query}")
            if controller is None or controller.agent is None:
                raise ValueError("Controller or Agent not initialized")
            response = await controller.execute_query(request.query)
            logger.info(f"Query processed successfully")
            return QueryResponse(response=response)
        except Exception as e:
            logger.error(f"Error processing query: {str(e)}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")
    
@app.get("/memory_stats")
async def memory_stats_endpoint():
    try:
        stats = await controller.memory_manager.get_memory_stats()
        return {"stats": stats}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/link_distribution")
async def link_distribution_endpoint():
    try:
        distribution = await controller.memory_manager.analyze_link_distribution()
        return {"distribution": distribution}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/visualize_network")
async def visualize_network_endpoint():
    try:
        await controller.memory_manager.visualize_network()
        return {"message": "Network visualization generated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    master_logger.error(f"Unhandled exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": f"An unexpected error occurred: {str(exc)}"}
    )

if __name__ == "__main__":
    import uvicorn
    master_logger.info("Running the API server directly")
    uvicorn.run(app, host="0.0.0.0", port=8000)-e 
File: ./src/app.py

import asyncio
from config import Config
from MultiAgentRAG.src.utils.controller import Controller
from MultiAgentRAG.src.utils.logging_setup import setup_logging
import logging

async def process_query(controller, query, chat_logger, memory_logger):
    chat_logger.info(f"Query: {query}")
    response = await controller.execute_query(query)
    chat_logger.info(f"Response: {response}")
    
    memories = await controller.get_recent_memories(5)
    memory_logger.info("Recent Memories:")
    for memory in memories:
        memory_logger.info(f"Query: {memory[0]}, Result: {memory[1]}")

async def main():
    # Initialize logging
    master_logger, chat_logger, memory_logger = setup_logging()

    # Set the log level for the memory logger
    memory_logger.setLevel(logging.DEBUG)

    try:
        master_logger.info("Starting the Multi-Agent RAG System")
        config = Config()
        
        if not config.validate_api_keys():
            raise EnvironmentError("API keys not found in environment variables")

        master_logger.info("API keys successfully loaded")
        master_logger.info(f"Using memory database path: {config.MEMORY_DB_PATH}")

        master_logger.debug("Initializing controller")
        controller = Controller(config)

        while True:
            try:
                query = input("Enter your query (or 'quit' to exit): ")
                if query.lower() == "quit":
                    master_logger.info("Exiting the program")
                    break

                master_logger.info(f"Executing query: {query}")
                await process_query(controller, query, chat_logger, memory_logger)

            except EOFError:
                master_logger.info("Received EOF, exiting the program")
                break
            except Exception as e:
                master_logger.error(f"An error occurred while processing the query: {e}", exc_info=True)
    except Exception as e:
        master_logger.error(f"An error occurred in the main program: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())-e 
File: ./src/cli.py

import asyncio
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging
from src.utils.structured_logging import get_logger

master_logger, chat_logger, memory_logger, database_logger = setup_logging()
controller = Controller()
similarity_analyzer = None
logger = get_logger("cli")

async def initialize():
    await controller.initialize()
    global similarity_analyzer

async def main():
    await initialize()
    while True:
        try:
            command = input("Enter a command (query/stats/distribution/visualize/quit): ").strip().lower()
            
            if command == 'quit':
                break
            elif command == 'query':
                query = input("Enter your query: ")
                response = await controller.execute_query(query)
                print(f"Response: {response}")
            elif command == 'stats':
                stats = await controller.memory_manager.get_memory_stats()
                print("Memory Statistics:")
                for key, value in stats.items():
                    print(f"{key}: {value}")
            elif command == 'distribution':
                distribution = await controller.memory_manager.analyze_link_distribution()
                print("Link Type Distribution:")
                for link_type, count in distribution.items():
                    print(f"{link_type}: {count}")
            elif command == 'visualize':
                await controller.memory_manager.visualize_network()
                print("Network visualization generated and saved as 'memory_network.png'")
            else:
                print("Invalid command. Please try again.")
        except Exception as e:
            print(f"An error occurred: {str(e)}. Please try again.")

if __name__ == "__main__":
    asyncio.run(main())-e 
File: ./src/memory/memory_manager.py

import logging
import aiosqlite # type: ignore
from typing import List, Tuple, Union, Dict, Optional
from langchain_openai import OpenAIEmbeddings
import numpy as np
from config import Config
import os
from src.utils.enhanced_logging import log_execution_time
from src.utils.visualizer import visualize_memory_network
from sklearn.feature_extraction.text import TfidfVectorizer
from rank_bm25 import BM25Okapi # type: ignore
from datetime import datetime
import time
from scipy.spatial.distance import cosine
from src.utils.structured_logging import get_logger
from src.utils.logging_config import setup_logging
import asyncio

from src.utils.structured_logging import get_logger
from src.utils.enhanced_logging import log_execution_time

logger = get_logger('memory')

class MemoryManager:
    def __init__(self, api_key: str):
        self.logger = get_logger(__name__)
        self.config = Config()
        self.personal_db_path = self.get_personal_db_path()
        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        self.container_id = self.get_container_id()
        self.tfidf_vectorizer: Optional[TfidfVectorizer] = None
        self.bm25: Optional[BM25Okapi] = None
        self.corpus: List[str] = []
        self.async_logger = None

    async def initialize(self):
        self.async_logger = await setup_logging(self.container_id)
        os.makedirs("/app/data", exist_ok=True)
        await self.create_tables(self.personal_db_path)
        await self._load_corpus()
        await self._update_indexing()

    def get_container_id(self):
        return os.environ.get('HOSTNAME', 'local')

    def get_personal_db_path(self):
        container_id = self.get_container_id()
        return f"/app/data/{container_id}.db"

    async def create_tables(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS memories (
                    id INTEGER PRIMARY KEY,
                    query TEXT NOT NULL,
                    result TEXT NOT NULL,
                    embedding BLOB NOT NULL,
                    embedding_shape TEXT NOT NULL,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Create memory_links table
            await db.execute("""
                CREATE TABLE IF NOT EXISTS memory_links (
                    id INTEGER PRIMARY KEY,
                    source_memory_id INTEGER,
                    target_memory_id INTEGER,
                    link_type TEXT,
                    FOREIGN KEY (source_memory_id) REFERENCES memories (id),
                    FOREIGN KEY (target_memory_id) REFERENCES memories (id)
                )
            """)
            await db.commit()
        logger.debug(f"Ensured memories and memory_links tables exist in {db_path}")

    @log_execution_time
    async def create_memory_link(self, source_id: int, target_id: int, link_type: str):
        async with aiosqlite.connect(self.personal_db_path) as db:
            await db.execute("""
                INSERT INTO memory_links (source_memory_id, target_memory_id, link_type)
                VALUES (?, ?, ?)
            """, (source_id, target_id, link_type))
            await db.commit()

    @log_execution_time
    async def get_linked_memories(self, memory_id: int) -> List[Tuple[int, str]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("""
                SELECT target_memory_id, link_type FROM memory_links
                WHERE source_memory_id = ?
            """, (memory_id,)) as cursor:
                return await cursor.fetchall()


    @log_execution_time
    async def save_memory(self, query: str, result: str):
        try:
            embedding = await self.embeddings.aembed_query(query)
            
            # Convert the list to a numpy array
            embedding_array = np.array(embedding, dtype=np.float32)
            embedding_shape = embedding_array.shape
            embedding_bytes = embedding_array.tobytes()
            
            async with aiosqlite.connect(self.personal_db_path) as db:
                cursor = await db.execute("""
                    INSERT INTO memories (query, result, embedding, embedding_shape) 
                    VALUES (?, ?, ?, ?)
                """, (query, result, embedding_bytes, str(embedding_shape)))
                memory_id = cursor.lastrowid
                await db.commit()
            
            self.db_logger.log_access(memory_id)
            
            logger.debug(f"Saved memory for query: {query} with result: {result}")
            
            # Update corpus and indexing
            await self._load_corpus()
            await self._update_indexing()

            # Call auto_link_memories with the new memory_id
            await self.auto_link_memories(memory_id)
            
            await self.async_logger.info("Memory saved", query=query, memory_id=memory_id)
        except Exception as e:
            await self.async_logger.error("Error saving memory", query=query, error=str(e))
            raise

    async def _load_corpus(self):
        start_time = time.time()
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT query, result FROM memories") as cursor:
                memories = await cursor.fetchall()
        self.corpus = [f"{query} {result}" for query, result in memories]
        logger.info(f"Loaded corpus with {len(self.corpus)} entries in {time.time() - start_time:.2f} seconds")
    
    @log_execution_time
    async def _update_indexing(self):
        if self.corpus:
            start_time = time.time()
            self.tfidf_vectorizer = TfidfVectorizer()
            self.tfidf_vectorizer.fit(self.corpus)
            tokenized_corpus = [doc.split() for doc in self.corpus]
            self.bm25 = BM25Okapi(tokenized_corpus)
            self.logger.info(f"Updated indexing with {len(self.corpus)} documents in {time.time() - start_time:.2f} seconds")
        else:
            self.logger.warning("Corpus is empty. Skipping indexing update.")


    @log_execution_time
    async def retrieve_relevant_memories(self, query: str, threshold: float = 0.75) -> str:
        query_embedding = await self.embeddings.aembed_query(query)
        
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT id, query, result, embedding FROM memories") as cursor:
                all_memories = await cursor.fetchall()
        
        relevant_memories = self._calculate_cosine_similarity(all_memories, query_embedding, threshold)
        
        # Retrieve linked memories
        linked_memories = set()
        for memory in relevant_memories:
            memory_id = memory[0]  # Assuming the first element is the memory_id
            linked = await self.get_linked_memories(int(memory_id))
            linked_memories.update(linked)
        
        # Fetch details of linked memories
        for linked_id, link_type in linked_memories:
            memory = await self.get_memory(linked_id)
            relevant_memories.append((str(memory['id']), memory['query'], memory['result'], 1.0, f"Linked ({link_type})"))
        
        # Sort memories by similarity
        relevant_memories.sort(key=lambda x: x[3], reverse=True)
        
        # Format the output
        formatted_output = []
        for memory in relevant_memories:
            if len(memory) == 5:
                memory_id, query, result, similarity, link_info = memory
                formatted_output.append(f"<{memory_id}>, <{query}>, <{result}>, <{similarity:.2f}>, <{link_info}>")
            else:
                memory_id, query, result, similarity = memory
                formatted_output.append(f"<{memory_id}>, <{query}>, <{result}>, <{similarity:.2f}>")
        
        return " ".join(formatted_output)

    def _calculate_l2_norm(self, memories: List[Tuple], query_embedding: np.ndarray, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        return [
            (str(memory[0]), memory[1], memory[2], np.linalg.norm(query_embedding - np.frombuffer(memory[3])), memory[4])
            for memory in memories
            if np.linalg.norm(query_embedding - np.frombuffer(memory[3])) <= threshold
        ]

    def _calculate_cosine_similarity(self, memories: List[Tuple], query_embedding: Union[List[float], np.ndarray], threshold: float) -> List[Tuple[str, str, str, float, str]]:
        results = []
        
        # Ensure query_embedding is a numpy array
        if isinstance(query_embedding, list):
            query_embedding = np.array(query_embedding, dtype=np.float32)
        
        for memory in memories:
            memory_id, query, result, embedding_bytes = memory
            memory_embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
            
            # Reshape memory_embedding if necessary
            if memory_embedding.shape != query_embedding.shape:
                logger.warning(f"Reshaping embedding for memory {memory_id}: {memory_embedding.shape} to {query_embedding.shape}")
                try:
                    memory_embedding = memory_embedding.reshape(query_embedding.shape)
                except ValueError:
                    logger.error(f"Cannot reshape embedding for memory {memory_id}. Skipping.")
                    continue
            
            # Compute cosine similarity
            similarity = np.dot(query_embedding, memory_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(memory_embedding))
            
            if similarity >= threshold:
                results.append((str(memory_id), query, result, float(similarity), ""))
        
        return results

    def _calculate_bm25(self, memories: List[Tuple], query: str, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)
        max_score = np.max(scores) if scores.size > 0 else 1
        normalized_scores = scores / max_score

        return [
            (str(memory[0]), memory[1], memory[2], score, memory[4])
            for memory, score in zip(memories, normalized_scores)
            if score >= threshold
        ]
    
    def _calculate_jaccard_similarity(self, memories: List[Tuple], query: str, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        query_set = set(query.lower().split())
        return [
            (str(memory[0]), memory[1], memory[2],
             len(set(memory[1].lower().split()) & query_set) / len(set(memory[1].lower().split()) | query_set),
             memory[4])
            for memory in memories
            if len(set(memory[1].lower().split()) & query_set) / len(set(memory[1].lower().split()) | query_set) >= threshold
        ]

    def _format_memories(self, all_memories: Dict[str, List[Tuple[str, str, str, float, str]]]) -> str:
        formatted_output = []
        seen_memories = set()

        for metric, memories in all_memories.items():
            if not memories:
                continue

            formatted_output.append(f"Similar by {metric} (ordered by timestamp - ascending):")
            sorted_memories = sorted(memories, key=lambda x: datetime.fromisoformat(x[4]))

            for memory in sorted_memories:
                memory_id, query, result, score, timestamp = memory
                if memory_id not in seen_memories:
                    formatted_output.append(f"<{memory_id}>, <{query}>, <{result}>, <{timestamp}>, <{score:.2f}>")
                    seen_memories.add(memory_id)
                else:
                    formatted_output.append(f"<{memory_id}>, <{score:.2f}>")

        return " ".join(formatted_output)

    async def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("""
                SELECT query, result FROM memories ORDER BY timestamp DESC LIMIT ?
            """, (limit,)) as cursor:
                memories = await cursor.fetchall()
        logger.debug(f"Retrieved {len(memories)} memories")
        return memories

    async def get_all_memories(self) -> List[Dict]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT id, query, result, embedding FROM memories") as cursor:
                rows = await cursor.fetchall()
                return [{
                    'id': row[0],
                    'query': row[1],
                    'result': row[2],
                    'embedding': np.frombuffer(row[3], dtype=np.float32)
                } for row in rows]

    async def _add_to_personal_db(self, memory: Tuple):
            query, result, embedding = memory
            async with aiosqlite.connect(self.personal_db_path) as db:
                await db.execute("""
                    INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)
                """, (query, result, embedding))
                await db.commit()
            logger.info(f"Added missing memory to personal DB: {query}")

    async def analyze_memory_distribution(self) -> Dict[str, int]:
        """
        Analyze the distribution of memories across different similarity metrics.
        """
        distribution = {
            "L2 norm": 0,
            "Cosine Similarity": 0,
            "BM25": 0,
            "Jaccard Similarity": 0
        }

        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT query, result FROM memories") as cursor:
                memories = await cursor.fetchall()

        for memory in memories:
            query, result = memory
            _, metadata = await self.retrieve_relevant_memories(query, return_metadata=True)
            distribution["L2 norm"] += metadata["l2_count"]
            distribution["Cosine Similarity"] += metadata["cosine_count"]
            distribution["BM25"] += metadata["bm25_count"]
            distribution["Jaccard Similarity"] += metadata["jaccard_count"]

        total = sum(distribution.values())
        for key in distribution:
            distribution[key] = round((distribution[key] / total) * 100, 2) if total > 0 else 0

        return distribution

    async def get_memory_stats(self) -> Dict[str, Union[int, float]]:
        """
        Get statistics about the current state of memories.
        """
        try:
            async with aiosqlite.connect(self.personal_db_path) as db:
                async with db.execute("SELECT COUNT(*) FROM memories") as cursor:
                    (total_memories,) = await cursor.fetchone()
                
                async with db.execute("SELECT MIN(timestamp), MAX(timestamp) FROM memories") as cursor:
                    (oldest_memory, newest_memory) = await cursor.fetchone()

            distribution = await self.analyze_memory_distribution()

            return {
                "total_memories": total_memories,
                "oldest_memory": oldest_memory,
                "newest_memory": newest_memory,
                "distribution": distribution
            }
        except Exception as e:
            logger.error(f"Error getting memory stats: {str(e)}", exc_info=True)
            raise
        
    @log_execution_time
    async def auto_link_memories(self, new_memory_id: int, threshold: float = 0.8):
        new_memory = await self.get_memory(new_memory_id)
        all_memories = await self.get_all_memories()
        
        for memory in all_memories:
            if memory['id'] != new_memory_id:
                embedding_similarity = self._calculate_embedding_similarity(new_memory['embedding'], memory['embedding'])
                keyword_similarity = self._calculate_keyword_similarity(new_memory['query'], memory['query'])
                
                combined_similarity = (embedding_similarity + keyword_similarity) / 2
                
                if combined_similarity >= threshold:
                    await self.create_memory_link(new_memory_id, memory['id'], "auto_similar")
                    await self.create_memory_link(memory['id'], new_memory_id, "auto_similar")

    async def get_memory(self, memory_id: int) -> Dict:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT id, query, result, embedding FROM memories WHERE id = ?", (memory_id,)) as cursor:
                row = await cursor.fetchone()
                return {
                    'id': row[0],
                    'query': row[1],
                    'result': row[2],
                    'embedding': np.frombuffer(row[3], dtype=np.float32)
                }

    async def get_all_memories(self) -> List[Dict]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT id, query, result, embedding FROM memories") as cursor:
                rows = await cursor.fetchall()
                return [{
                    'id': row[0],
                    'query': row[1],
                    'result': row[2],
                    'embedding': np.frombuffer(row[3], dtype=np.float32)
                } for row in rows]

    def _calculate_embedding_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        return 1 - cosine(emb1, emb2)

    def _calculate_keyword_similarity(self, query1: str, query2: str) -> float:
        tokens1 = set(query1.lower().split())
        tokens2 = set(query2.lower().split())
        return len(tokens1 & tokens2) / len(tokens1 | tokens2)
    
    async def get_memory_stats(self) -> Dict[str, Union[int, float]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT COUNT(*) FROM memories") as cursor:
                total_memories = (await cursor.fetchone())[0]
            async with db.execute("SELECT COUNT(*) FROM memory_links") as cursor:
                total_links = (await cursor.fetchone())[0]
            async with db.execute("SELECT AVG(link_count) FROM (SELECT COUNT(*) as link_count FROM memory_links GROUP BY source_memory_id)") as cursor:
                avg_links_per_memory = (await cursor.fetchone())[0]
        
        return {
            "total_memories": total_memories,
            "total_links": total_links,
            "avg_links_per_memory": avg_links_per_memory
        }

    async def analyze_link_distribution(self) -> Dict[str, int]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT link_type, COUNT(*) FROM memory_links GROUP BY link_type") as cursor:
                distribution = dict(await cursor.fetchall())
        return distribution
    
    async def visualize_network(self):
        memories = await self.get_all_memories()
        links = await self.get_all_links()
        
        memory_tuples = [(m['id'], m['query'], m['result']) for m in memories]
        visualize_memory_network(memory_tuples, links)

    async def get_all_links(self) -> List[Tuple[int, int, str]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT source_memory_id, target_memory_id, link_type FROM memory_links") as cursor:
                return await cursor.fetchall()-e 
File: ./src/memory/__init__.py

-e 
File: ./src/utils/controller.py

import logging
from typing import List, Tuple
from src.agents.agent import Agent
from src.memory.memory_manager import MemoryManager
from config import config
from src.utils.logging_config import setup_logging
from src.utils.structured_logging import get_logger
import os

from src.utils.structured_logging import get_logger

class Controller:
    def __init__(self):
        self.logger = get_logger(__name__)
        self.memory_manager = None
        self.agent = None

    async def initialize(self):
        container_id = os.environ.get('HOSTNAME', 'local')
        self.memory_manager = MemoryManager(config.GROQ_API_KEY)
        await self.memory_manager.initialize()
        self.agent = Agent(self.memory_manager)

    async def execute_query(self, query: str) -> str:
        try:
            response = await self.agent.process_query(query)
            self.logger.debug("Generated response", response=response)
            await self.memory_manager.save_memory(query, response)
            return response
        except Exception as e:
            self.logger.error(f"Error executing query", query=query, error=str(e), exc_info=True)
            return f"An error occurred while processing your query: {str(e)}"

    async def get_recent_memories(self, limit: int) -> List[Tuple[str, str]]:
        return await self.memory_manager.get_memories(limit)-e 
File: ./src/utils/enhanced_logging.py

from functools import wraps
import time
from src.utils.structured_logging import get_logger
import asyncio

performance_logger = get_logger("performance")

def log_execution_time(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        execution_time = time.time() - start_time
        performance_logger.info(f"{func.__name__} executed",
                                execution_time=execution_time,
                                function=func.__name__)
        return result
    return wrapper-e 
File: ./src/utils/error_handling.py

from fastapi import Request
from fastapi.responses import JSONResponse
from src.utils.structured_logging import get_logger

error_logger = get_logger("error")

async def global_exception_handler(request: Request, exc: Exception):
    error_logger.error("Unhandled exception", 
                       url=str(request.url),
                       method=request.method,
                       error=str(exc),
                       exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": f"An unexpected error occurred: {str(exc)}"}
    )-e 
File: ./src/utils/logging_config.py

import logging
import logging.config
import os
from datetime import datetime
from aiologger import Logger
from aiologger.handlers.files import AsyncFileHandler
from aiologger.formatters.base import Formatter
import asyncio

async def setup_logging(container_id):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    docker_log_directory = f'/app/logs/{timestamp}_{container_id}'
    local_log_directory = f'./logs/{timestamp}_{container_id}'
    
    os.makedirs(docker_log_directory, exist_ok=True)
    os.makedirs(local_log_directory, exist_ok=True)

    config = {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'standard': {
                'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            },
        },
        'handlers': {
            'console': {
                'class': 'logging.StreamHandler',
                'formatter': 'standard',
                'level': 'DEBUG',
            },
            'docker_file': {
                'class': 'logging.handlers.RotatingFileHandler',
                'formatter': 'standard',
                'level': 'DEBUG',
                'filename': f'{docker_log_directory}/app.log',
                'maxBytes': 10485760,  # 10MB
                'backupCount': 5,
            },
            'local_file': {
                'class': 'logging.handlers.RotatingFileHandler',
                'formatter': 'standard',
                'level': 'DEBUG',
                'filename': f'{local_log_directory}/app.log',
                'maxBytes': 10485760,  # 10MB
                'backupCount': 5,
            },
        },
        'loggers': {
            '': {  # root logger
                'handlers': ['console', 'docker_file', 'local_file'],
                'level': 'DEBUG',
                'propagate': True
            },
        }
    }
    
    logging.config.dictConfig(config)
    return await setup_async_logging(docker_log_directory, local_log_directory)

async def setup_async_logging(docker_log_directory, local_log_directory):
    formatter = Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    docker_handler = AsyncFileHandler(filename=f'{docker_log_directory}/async_app.log')
    docker_handler.formatter = formatter

    local_handler = AsyncFileHandler(filename=f'{local_log_directory}/async_app.log')
    local_handler.formatter = formatter

    async_logger = Logger(name="async_logger")
    async_logger.add_handler(docker_handler)  # Remove await
    async_logger.add_handler(local_handler)   # Remove await

    return async_logger-e 
File: ./src/utils/logging_setup.py

import logging
import os
import socket
from datetime import datetime

def setup_logging():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    container_id = socket.gethostname() if is_running_in_docker() else 'local'
    log_directory = f'logs/{timestamp}_{container_id}'

    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    master_logger = logging.getLogger('master')
    master_logger.setLevel(logging.DEBUG)
    file_handler = logging.FileHandler(os.path.join(log_directory, "master.log"))
    file_handler.setFormatter(log_formatter)
    master_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(message)s'))
    master_logger.addHandler(console_handler)

    chat_logger = logging.getLogger('chat')
    chat_logger.setLevel(logging.DEBUG)
    chat_file_handler = logging.FileHandler(os.path.join(log_directory, 'chat.log'))
    chat_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
    chat_logger.addHandler(chat_file_handler)

    memory_logger = logging.getLogger('memory')
    memory_logger.setLevel(logging.DEBUG)
    memory_file_handler = logging.FileHandler(os.path.join(log_directory, 'memory.log'))
    memory_file_handler.setFormatter(log_formatter)
    memory_logger.addHandler(memory_file_handler)

    database_logger = logging.getLogger('database')
    database_logger.setLevel(logging.DEBUG)
    database_file_handler = logging.FileHandler(os.path.join(log_directory, 'database.log'))
    database_file_handler.setFormatter(log_formatter)
    database_logger.addHandler(database_file_handler)

    master_logger.info(f"Logging setup complete. Log directory: {log_directory}")
    return master_logger, chat_logger, memory_logger, database_logger

def is_running_in_docker() -> bool:
    return os.path.exists('/.dockerenv')-e 
File: ./src/utils/log_sync.py

import os
import shutil
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class LogSyncHandler(FileSystemEventHandler):
    def __init__(self, src_path, dest_path):
        self.src_path = src_path
        self.dest_path = dest_path

    def on_modified(self, event):
        if not event.is_directory:
            src_file = event.src_path
            dest_file = os.path.join(self.dest_path, os.path.basename(src_file))
            shutil.copy2(src_file, dest_file)

def start_log_sync(src_path, dest_path):
    event_handler = LogSyncHandler(src_path, dest_path)
    observer = Observer()
    observer.schedule(event_handler, src_path, recursive=True)
    observer.start()
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

if __name__ == "__main__":
    start_log_sync("/app/logs", "/app/local_logs")-e 
File: ./src/utils/structured_logging.py

import structlog

def setup_structured_logging():
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

def get_logger(name):
    return structlog.get_logger(name)-e 
File: ./src/utils/tracing.py

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

def setup_tracing():
    trace.set_tracer_provider(TracerProvider())
    trace.get_tracer_provider().add_span_processor(
        BatchSpanProcessor(ConsoleSpanExporter())
    )

def instrument_fastapi(app):
    FastAPIInstrumentor.instrument_app(app)

tracer = trace.get_tracer(__name__)-e 
File: ./src/utils/visualizer.py

import networkx as nx
import matplotlib.pyplot as plt
from typing import List, Tuple

def visualize_memory_network(memories: List[Tuple[int, str, str]], links: List[Tuple[int, int, str]]):
    G = nx.Graph()
    
    # Add nodes
    for memory_id, query, _ in memories:
        G.add_node(memory_id, label=query[:20])  # Use first 20 characters of query as label
    
    # Add edges
    for source_id, target_id, link_type in links:
        G.add_edge(source_id, target_id, label=link_type)
    
    # Set up the plot
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G)
    
    # Draw nodes
    nx.draw_networkx_nodes(G, pos, node_size=700, node_color='lightblue')
    nx.draw_networkx_labels(G, pos, {node: data['label'] for node, data in G.nodes(data=True)})
    
    # Draw edges
    nx.draw_networkx_edges(G, pos)
    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
    
    plt.title("Memory Network Visualization")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig('memory_network.png')
    plt.close()

# Usage in MemoryManager:
# async def visualize_network(self):
#     memories = await self.get_all_memories()
#     links = await self.get_all_links()
#     visualize_memory_network([(m['id'], m['query'], m['result']) for m in memories], links)-e 
File: ./src/utils/__init__.py

-e 
File: ./src/__init__.py

