.
├── Dockerfile
├── config.py
├── data
│   ├── 6cb49552b425.db
│   ├── ec6ab11fc693.db
│   └── master.db
├── docker-compose.yml
├── docker_requirements.txt
├── docs
│   └── README.md
├── logs
│   ├── 20240702_160538_6169c67053ff
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240702_181204_ec6ab11fc693
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   └── 20240703_070750_6cb49552b425
│       ├── chat.log
│       ├── database.log
│       ├── master.log
│       └── memory.log
├── notebooks
│   └── Multi_Agent_RAG_LangGraph.ipynb
├── scripts
│   ├── _docker_reset.sh
│   ├── generate_tree.sh
│   ├── logs_to_docs.sh
│   ├── manage_docker.sh
│   └── reset_logs.sh
├── src
│   ├── __init__.py
│   ├── __pycache__
│   │   └── __init__.cpython-310.pyc
│   ├── agents
│   │   ├── __init__.py
│   │   └── agent.py
│   ├── api.py
│   ├── app.py
│   ├── cli.py
│   ├── memory
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-310.pyc
│   │   │   └── memory_manager.cpython-310.pyc
│   │   ├── enhanced_memory_manager.py
│   │   └── memory_manager.py
│   └── utils
│       ├── __init__.py
│       ├── api_utils.py
│       ├── controller.py
│       ├── enhanced_logging.py
│       ├── logging_setup.py
│       ├── memory_analysis.py
│       ├── reset_database.py
│       └── similarity_analysis.py
└── tree.txt

14 directories, 47 files

Note: The following list of files and directories are excluded only from the appended file contents section:

__pycache__, .git, .env, text_docs, scripts, logs, tree.txt, generate_tree.sh, README, *.ipynb, *.pdf, *.db

File Contents:

-e 
File: ./src/memory/__init__.py

-e 
File: ./src/memory/enhanced_memory_manager.py

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from rank_bm25 import BM25Okapi
import aiosqlite
from typing import List, Tuple, Dict, Optional
from src.memory.memory_manager import MemoryManager
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging
import time
from config import Config

logger = logging.getLogger(__name__)

class EnhancedMemoryManager(MemoryManager):
    def __init__(self, api_key: str):
        super().__init__(api_key)
        self.tfidf_vectorizer: Optional[TfidfVectorizer] = None
        self.bm25: Optional[BM25Okapi] = None
        self.corpus: List[str] = []
        self.config = Config()

    async def initialize(self):
        """Initialize the EnhancedMemoryManager by loading the corpus and updating indexing."""
        await super().initialize()
        await self._load_corpus()
        self._update_indexing()

    async def _load_corpus(self):
        start_time = time.time()
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT query, result FROM memories") as cursor:
                memories = await cursor.fetchall()
        self.corpus = [f"{query} {result}" for query, result in memories]
        logger.info(f"Loaded corpus with {len(self.corpus)} entries in {time.time() - start_time:.2f} seconds")

    def _update_indexing(self):
        if self.corpus:
            start_time = time.time()
            self.tfidf_vectorizer = TfidfVectorizer()
            self.tfidf_vectorizer.fit(self.corpus)
            tokenized_corpus = [doc.split() for doc in self.corpus]
            self.bm25 = BM25Okapi(tokenized_corpus)
            logger.info(f"Updated indexing with {len(self.corpus)} documents in {time.time() - start_time:.2f} seconds")
        else:
            logger.warning("Corpus is empty. Skipping indexing update.")

    async def retrieve_relevant_memories(self, query: str, threshold: float = 0.75, return_metadata: bool = False) -> str:
        if not self.corpus:
            logger.warning("Corpus is empty. No memories to retrieve.")
            return ""

        start_time = time.time()
        query_embedding = np.array(await self.embeddings.aembed_query(query))
        
        try:
            async with aiosqlite.connect(self.personal_db_path) as db:
                async with db.execute("SELECT rowid, query, result, embedding, timestamp FROM memories") as cursor:
                    all_memories = await cursor.fetchall()
        except aiosqlite.Error as e:
            logger.error(f"Database error: {e}")
            return ""

        logger.debug(f"Retrieved {len(all_memories)} memories from database")

        with ThreadPoolExecutor(max_workers=4) as executor:
            l2_future = executor.submit(self._calculate_l2_norm, all_memories, query_embedding, threshold)
            cosine_future = executor.submit(self._calculate_cosine_similarity, all_memories, query_embedding, threshold)
            bm25_future = executor.submit(self._calculate_bm25, all_memories, query, threshold)
            jaccard_future = executor.submit(self._calculate_jaccard_similarity, all_memories, query, threshold)

            l2_memories = l2_future.result()
            cosine_memories = cosine_future.result()
            bm25_memories = bm25_future.result()
            jaccard_memories = jaccard_future.result()

        logger.debug(f"L2 memories: {len(l2_memories)}, Cosine memories: {len(cosine_memories)}, BM25 memories: {len(bm25_memories)}, Jaccard memories: {len(jaccard_memories)}")

        all_memories = {
            'L2 norm': l2_memories,
            'Cosine Similarity': cosine_memories,
            'BM25': bm25_memories,
            'Jaccard Similarity': jaccard_memories
        }

        formatted_output = self._format_memories(all_memories)

        logger.info(f"Retrieved and processed memories in {time.time() - start_time:.2f} seconds")

        if return_metadata:
            metadata = {
                "l2_count": len(l2_memories),
                "cosine_count": len(cosine_memories),
                "bm25_count": len(bm25_memories),
                "jaccard_count": len(jaccard_memories),
                "total_memories": len(all_memories),
                "processing_time": time.time() - start_time
            }
            return formatted_output, metadata
        else:
            return formatted_output

    def _calculate_l2_norm(self, memories: List[Tuple], query_embedding: np.ndarray, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        """Calculate L2 norm similarity."""
        return [
            (str(memory[0]), memory[1], memory[2], np.linalg.norm(query_embedding - np.frombuffer(memory[3])), memory[4])
            for memory in memories
            if np.linalg.norm(query_embedding - np.frombuffer(memory[3])) <= threshold
        ]

    def _calculate_cosine_similarity(self, memories: List[Tuple], query_embedding: np.ndarray, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        """Calculate cosine similarity."""
        return [
            (str(memory[0]), memory[1], memory[2], 
             np.dot(query_embedding, np.frombuffer(memory[3])) / (np.linalg.norm(query_embedding) * np.linalg.norm(np.frombuffer(memory[3]))),
             memory[4])
            for memory in memories
            if np.dot(query_embedding, np.frombuffer(memory[3])) / (np.linalg.norm(query_embedding) * np.linalg.norm(np.frombuffer(memory[3]))) >= threshold
        ]

    def _calculate_bm25(self, memories: List[Tuple], query: str, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        """Calculate BM25 similarity."""
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)
        max_score = np.max(scores) if scores.size > 0 else 1  # Use np.max instead of max
        normalized_scores = scores / max_score

        return [
            (str(memory[0]), memory[1], memory[2], score, memory[4])
            for memory, score in zip(memories, normalized_scores)
            if score >= threshold
        ]
    
    def _calculate_jaccard_similarity(self, memories: List[Tuple], query: str, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        """Calculate Jaccard similarity."""
        query_set = set(query.lower().split())
        return [
            (str(memory[0]), memory[1], memory[2],
             len(set(memory[1].lower().split()) & query_set) / len(set(memory[1].lower().split()) | query_set),
             memory[4])
            for memory in memories
            if len(set(memory[1].lower().split()) & query_set) / len(set(memory[1].lower().split()) | query_set) >= threshold
        ]
    
    async def save_memory(self, query: str, result: str):
        await super().save_memory(query, result)
        await self._load_corpus()
        self._update_indexing()

    def _format_memories(self, all_memories: Dict[str, List[Tuple[str, str, str, float, str]]]) -> str:
        formatted_output = []
        seen_memories = set()

        for metric, memories in all_memories.items():
            if not memories:
                continue

            formatted_output.append(f"Similar by {metric} (ordered by timestamp - ascending):")
            sorted_memories = sorted(memories, key=lambda x: datetime.fromisoformat(x[4]))

            for memory in sorted_memories:
                memory_id, query, result, score, timestamp = memory
                if memory_id not in seen_memories:
                    formatted_output.append(f"<{memory_id}>, <{query}>, <{result}>, <{timestamp}>, <{score:.2f}>")
                    seen_memories.add(memory_id)
                else:
                    formatted_output.append(f"<{memory_id}>, <{score:.2f}>")

        return " ".join(formatted_output)-e 
File: ./src/memory/memory_manager.py

import logging
import aiosqlite
from typing import List, Tuple, Union, Dict
from langchain_openai import OpenAIEmbeddings
import numpy as np
from config import config
import os
from src.utils.enhanced_logging import log_execution_time, DatabaseLogger
import json
import csv
import io

logger = logging.getLogger('memory')

class MemoryManager:
    def __init__(self, api_key: str):
        self.master_db_path = "/app/data/master.db"
        self.personal_db_path = self.get_personal_db_path()
        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        self.container_id = self.get_container_id()
        self.db_logger = DatabaseLogger(logging.getLogger('database'))

    async def initialize(self):
        await self.initialize_databases()

    async def initialize_databases(self):
        os.makedirs("/app/data", exist_ok=True)
        await self.create_tables(self.master_db_path)
        await self.create_tables(self.personal_db_path)
        await self.create_changelog_table(self.master_db_path)
        await self.create_changelog_table(self.personal_db_path)

    @log_execution_time(logger)
    async def inspect_database(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            async with db.execute("SELECT * FROM memories") as cursor:
                columns = [description[0] for description in cursor.description]
                rows = await cursor.fetchall()
        return {"columns": columns, "rows": rows}

    @log_execution_time(logger)
    async def compare_databases(self):
        personal_db = await self.inspect_database(self.personal_db_path)
        master_db = await self.inspect_database(self.master_db_path)
        
        personal_ids = set(row[0] for row in personal_db['rows'])
        master_ids = set(row[0] for row in master_db['rows'])
        
        only_in_personal = personal_ids - master_ids
        only_in_master = master_ids - personal_ids
        in_both = personal_ids.intersection(master_ids)
        
        return {
            "only_in_personal": list(only_in_personal),
            "only_in_master": list(only_in_master),
            "in_both": list(in_both)
        }

    def get_container_id(self):
        return os.environ.get('HOSTNAME', 'local')

    def get_personal_db_path(self):
        container_id = self.get_container_id()
        return f"/app/data/{container_id}.db"

    async def create_tables(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            if db_path == self.master_db_path:
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS memories (
                        id INTEGER PRIMARY KEY,
                        query TEXT,
                        result TEXT,
                        embedding BLOB,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        author TEXT
                    )
                """)
            else:
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS memories (
                        id INTEGER PRIMARY KEY,
                        query TEXT,
                        result TEXT,
                        embedding BLOB,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                """)
            await db.commit()
        logger.debug(f"Ensured memories table exists in {db_path}")

    async def create_changelog_table(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS changelog (
                    id INTEGER PRIMARY KEY,
                    operation TEXT,
                    memory_id INTEGER,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            await db.commit()

    async def log_change(self, db_path, operation, memory_id):
        async with aiosqlite.connect(db_path) as db:
            await db.execute("""
                INSERT INTO changelog (operation, memory_id) VALUES (?, ?)
            """, (operation, memory_id))
            await db.commit()

    async def save_memory(self, query: str, result: str):
        try:
            embedding = np.array(await self.embeddings.aembed_query(query)).tobytes()
            
            async with aiosqlite.connect(self.personal_db_path) as db:
                self.db_logger.log_query("INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)")
                cursor = await db.execute("""
                    INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)
                """, (query, result, embedding))
                memory_id = cursor.lastrowid
                await db.commit()
            self.db_logger.log_access(memory_id)
            await self.log_change(self.personal_db_path, "INSERT", memory_id)
            
            async with aiosqlite.connect(self.master_db_path) as db:
                self.db_logger.log_query("INSERT INTO memories (query, result, embedding, author) VALUES (?, ?, ?, ?)")
                cursor = await db.execute("""
                    INSERT INTO memories (query, result, embedding, author) VALUES (?, ?, ?, ?)
                """, (query, result, embedding, self.container_id))
                memory_id = cursor.lastrowid
                await db.commit()
            self.db_logger.log_access(memory_id)
            await self.log_change(self.master_db_path, "INSERT", memory_id)
            
            logger.debug(f"Saved memory for query: {query} with result: {result}")
            
            # Reload the corpus after saving a new memory
            await self._load_corpus()
        except Exception as e:
            logger.error(f"Error saving memory for query '{query}': {str(e)}", exc_info=True)
            raise
        
    async def retrieve_relevant_memories(self, query: str, threshold: float = config.MEMORY_RETRIEVAL_THRESHOLD, return_metadata: bool = False) -> Union[List[Tuple[str, str, str]], Dict]:
        try:
            query_embedding = np.array(await self.embeddings.aembed_query(query))
            
            personal_memories = await self._retrieve_from_db(self.personal_db_path, query_embedding, threshold)
            self.db_logger.log_query("SELECT query, result, embedding, timestamp FROM memories")
            
            metadata = {"queried_databases": ["personal"]}
            
            if len(personal_memories) < 3:
                master_memories = await self._retrieve_from_db(self.master_db_path, query_embedding, threshold, exclude_author=self.container_id)
                combined_memories = personal_memories + master_memories
                combined_memories.sort(key=lambda x: x[2], reverse=True)
                relevant_memories = combined_memories[:config.MEMORY_RETRIEVAL_LIMIT]
                metadata["queried_databases"].append("master")
            else:
                relevant_memories = personal_memories[:config.MEMORY_RETRIEVAL_LIMIT]
            
            if return_metadata:
                metadata["similarity_scores"] = [memory[2] for memory in relevant_memories]
                return {
                    "memories": [(memory[0], memory[1], memory[3]) for memory in relevant_memories],
                    "metadata": metadata
                }
            else:
                return [(memory[0], memory[1], memory[3]) for memory in relevant_memories]
        except Exception as e:
            logger.error(f"Error retrieving relevant memories for query '{query}': {str(e)}", exc_info=True)
            raise

    async def _retrieve_from_db(self, db_path: str, query_embedding: np.ndarray, threshold: float, exclude_author: str = None) -> List[Tuple[str, str, float, str]]:
        async with aiosqlite.connect(db_path) as db:
            if exclude_author:
                async with db.execute("SELECT query, result, embedding, timestamp FROM memories WHERE author != ?", (exclude_author,)) as cursor:
                    memories = await cursor.fetchall()
            else:
                async with db.execute("SELECT query, result, embedding, timestamp FROM memories") as cursor:
                    memories = await cursor.fetchall()

        relevant_memories = []
        for memory in memories:
            memory_query, memory_result, memory_embedding, timestamp = memory
            memory_embedding = np.frombuffer(memory_embedding)
            similarity = np.dot(query_embedding, memory_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(memory_embedding))
            if similarity >= threshold:
                relevant_memories.append((memory_query, memory_result, similarity, timestamp))

        relevant_memories.sort(key=lambda x: x[2], reverse=True)
        return relevant_memories

    async def get_memories(self, limit: int = config.MEMORY_RETRIEVAL_LIMIT) -> List[Tuple[str, str]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("""
                SELECT query, result FROM memories ORDER BY timestamp DESC LIMIT ?
            """, (limit,)) as cursor:
                memories = await cursor.fetchall()
        logger.debug(f"Retrieved {len(memories)} memories")
        return memories
    
    async def export_memories(self, db_path, format='json'):
        async with aiosqlite.connect(db_path) as db:
            async with db.execute("SELECT * FROM memories") as cursor:
                rows = await cursor.fetchall()
                columns = [description[0] for description in cursor.description]

        if format == 'json':
            data = [dict(zip(columns, row)) for row in rows]
            return json.dumps(data, default=str)
        elif format == 'csv':
            output = io.StringIO()
            writer = csv.writer(output)
            writer.writerow(columns)
            writer.writerows(rows)
            return output.getvalue()
        else:
            raise ValueError("Unsupported format. Use 'json' or 'csv'.")
        
    async def import_memories(self, db_path, data, format='json'):
        if format == 'json':
            memories = json.loads(data)
        elif format == 'csv':
            reader = csv.DictReader(io.StringIO(data))
            memories = list(reader)
        else:
            raise ValueError("Unsupported format. Use 'json' or 'csv'.")

        async with aiosqlite.connect(db_path) as db:
            for memory in memories:
                await db.execute("""
                    INSERT INTO memories (query, result, embedding, timestamp, author)
                    VALUES (?, ?, ?, ?, ?)
                """, (memory['query'], memory['result'], memory['embedding'], memory['timestamp'], memory.get('author')))
            await db.commit()

    async def explain_memory_retrieval(self, query: str, threshold: float = config.MEMORY_RETRIEVAL_THRESHOLD):
        explanation = []
        query_embedding = np.array(await self.embeddings.aembed_query(query))
        
        explanation.append(f"1. Converted query '{query}' to embedding.")
        
        personal_memories = await self._retrieve_from_db(self.personal_db_path, query_embedding, threshold)
        explanation.append(f"2. Retrieved {len(personal_memories)} memories from personal database.")
        
        if len(personal_memories) < 3:
            explanation.append("3. Less than 3 relevant memories found in personal database. Searching master database.")
            master_memories = await self._retrieve_from_db(self.master_db_path, query_embedding, threshold, exclude_author=self.container_id)
            explanation.append(f"4. Retrieved {len(master_memories)} additional memories from master database.")
            combined_memories = personal_memories + master_memories
            combined_memories.sort(key=lambda x: x[2], reverse=True)
            relevant_memories = combined_memories[:config.MEMORY_RETRIEVAL_LIMIT]
        else:
            explanation.append("3. Sufficient memories found in personal database. Not searching master database.")
            relevant_memories = personal_memories[:config.MEMORY_RETRIEVAL_LIMIT]
        
        explanation.append(f"5. Selected top {len(relevant_memories)} memories based on similarity.")
        for i, memory in enumerate(relevant_memories, 1):
            explanation.append(f"   {i}. Memory ID: {memory[0]}, Similarity: {memory[2]:.4f}")
        
        return "\n".join(explanation)
    
    async def perform_consistency_check(self):
        discrepancies = []
        
        async with aiosqlite.connect(self.personal_db_path) as personal_db, \
                   aiosqlite.connect(self.master_db_path) as master_db:
            async with personal_db.execute("SELECT id, query, result FROM memories") as personal_cursor:
                personal_memories = await personal_cursor.fetchall()

            for p_id, p_query, p_result in personal_memories:
                async with master_db.execute("SELECT id, query, result FROM memories WHERE author = ? AND query = ?", 
                                             (self.container_id, p_query)) as master_cursor:
                    master_memory = await master_cursor.fetchone()

                if master_memory is None:
                    discrepancies.append({
                        "type": "missing_in_master",
                        "personal_id": p_id,
                        "query": p_query
                    })
                elif master_memory[2] != p_result:
                    discrepancies.append({
                        "type": "content_mismatch",
                        "personal_id": p_id,
                        "master_id": master_memory[0],
                        "query": p_query,
                        "personal_result": p_result,
                        "master_result": master_memory[2]
                    })

            # Check for memories in master that are not in personal
            async with master_db.execute("SELECT id, query FROM memories WHERE author = ?", 
                                         (self.container_id,)) as master_cursor:
                master_memories = await master_cursor.fetchall()

            personal_queries = set(memory[1] for memory in personal_memories)
            for m_id, m_query in master_memories:
                if m_query not in personal_queries:
                    discrepancies.append({
                        "type": "missing_in_personal",
                        "master_id": m_id,
                        "query": m_query
                    })

        return discrepancies

    async def fix_discrepancies(self, discrepancies):
        for discrepancy in discrepancies:
            if discrepancy["type"] == "missing_in_master":
                await self._copy_memory_to_master(discrepancy["personal_id"])
            elif discrepancy["type"] == "missing_in_personal":
                await self._copy_memory_to_personal(discrepancy["master_id"])
            elif discrepancy["type"] == "content_mismatch":
                await self._resolve_content_mismatch(discrepancy)

    async def _copy_memory_to_master(self, personal_id):
        async with aiosqlite.connect(self.personal_db_path) as personal_db, \
                   aiosqlite.connect(self.master_db_path) as master_db:
            async with personal_db.execute("SELECT query, result, embedding FROM memories WHERE id = ?", 
                                           (personal_id,)) as cursor:
                memory = await cursor.fetchone()
            
            if memory:
                await master_db.execute("""
                    INSERT INTO memories (query, result, embedding, author)
                    VALUES (?, ?, ?, ?)
                """, (*memory, self.container_id))
                await master_db.commit()

    async def _copy_memory_to_personal(self, master_id):
        async with aiosqlite.connect(self.master_db_path) as master_db, \
                   aiosqlite.connect(self.personal_db_path) as personal_db:
            async with master_db.execute("SELECT query, result, embedding FROM memories WHERE id = ?", 
                                         (master_id,)) as cursor:
                memory = await cursor.fetchone()
            
            if memory:
                await personal_db.execute("""
                    INSERT INTO memories (query, result, embedding)
                    VALUES (?, ?, ?)
                """, memory)
                await personal_db.commit()

    async def _resolve_content_mismatch(self, discrepancy):
        # For this example, we'll always use the master version
        # In a real-world scenario, you might want a more sophisticated resolution strategy
        async with aiosqlite.connect(self.master_db_path) as master_db, \
                   aiosqlite.connect(self.personal_db_path) as personal_db:
            async with master_db.execute("SELECT result, embedding FROM memories WHERE id = ?", 
                                         (discrepancy["master_id"],)) as cursor:
                master_memory = await cursor.fetchone()
            
            if master_memory:
                await personal_db.execute("""
                    UPDATE memories
                    SET result = ?, embedding = ?
                    WHERE id = ?
                """, (*master_memory, discrepancy["personal_id"]))
                await personal_db.commit()

    async def run_consistency_check_and_fix(self):
        discrepancies = await self.perform_consistency_check()
        if discrepancies:
            logger.info(f"Found {len(discrepancies)} discrepancies. Fixing...")
            await self.fix_discrepancies(discrepancies)
            logger.info("Discrepancies fixed.")
        else:
            logger.info("No discrepancies found.")-e 
File: ./src/__init__.py

-e 
File: ./src/agents/__init__.py

-e 
File: ./src/agents/agent.py

import logging
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from typing import List, Tuple
from config import config
import json
from src.utils.api_utils import rate_limited, exponential_backoff, cached

logger = logging.getLogger('master')
chat_logger = logging.getLogger('chat')

class Agent:
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager
        self.chat_model = ChatOpenAI(model_name=config.MODEL_NAME)

    @cached
    @rate_limited(max_calls=6, period=60)  # Adjust these values based on your API limits
    @exponential_backoff(max_retries=3, base_delay=2)
    async def process_query(self, query: str) -> str:
        relevant_memories = await self.memory_manager.retrieve_relevant_memories(query)
        
        messages = [
            HumanMessage(content=f"""REQUEST = "{query}". 

CONTEXT = "{relevant_memories}"

INSTRUCTIONS = The 'CONTEXT' provided contains relevant memories retrieved based on similarity to the current request. The format of the context is as follows:

Similar by [Similarity Metric] (ordered by timestamp - ascending):
<memory_id>, <previous_query>, <previous_response>, <timestamp>, <similarity_score>

For memories that have been mentioned before, the format is shortened to:
<memory_id>, <similarity_score>

Use this context to inform your response to the current request. Pay attention to the similarity metrics and scores to gauge the relevance of each memory. Memories with higher similarity scores and more recent timestamps may be more pertinent to the current query.

Please respond to the REQUEST based on this context and your general knowledge.""")
        ]
        
        # Log the full message sent to the API
        chat_logger.info(f"Full ChatGPT API request: {json.dumps([m.dict() for m in messages], indent=2)}")
        
        response = await self.chat_model.ainvoke(messages)
        
        # Log the full API response
        chat_logger.info(f"Full ChatGPT API response: {json.dumps(response.dict(), indent=2)}")
        
        logger.debug(f"Processed query: {query} with context: {relevant_memories}")
        
        return response.content-e 
File: ./src/app.py

import asyncio
from config import Config
from MultiAgentRAG.src.utils.controller import Controller
from MultiAgentRAG.src.utils.logging_setup import setup_logging
import logging

async def process_query(controller, query, chat_logger, memory_logger):
    chat_logger.info(f"Query: {query}")
    response = await controller.execute_query(query)
    chat_logger.info(f"Response: {response}")
    
    memories = await controller.get_recent_memories(5)
    memory_logger.info("Recent Memories:")
    for memory in memories:
        memory_logger.info(f"Query: {memory[0]}, Result: {memory[1]}")

async def main():
    # Initialize logging
    master_logger, chat_logger, memory_logger = setup_logging()

    # Set the log level for the memory logger
    memory_logger.setLevel(logging.DEBUG)

    try:
        master_logger.info("Starting the Multi-Agent RAG System")
        config = Config()
        
        if not config.validate_api_keys():
            raise EnvironmentError("API keys not found in environment variables")

        master_logger.info("API keys successfully loaded")
        master_logger.info(f"Using memory database path: {config.MEMORY_DB_PATH}")

        master_logger.debug("Initializing controller")
        controller = Controller(config)

        while True:
            try:
                query = input("Enter your query (or 'quit' to exit): ")
                if query.lower() == "quit":
                    master_logger.info("Exiting the program")
                    break

                master_logger.info(f"Executing query: {query}")
                await process_query(controller, query, chat_logger, memory_logger)

            except EOFError:
                master_logger.info("Received EOF, exiting the program")
                break
            except Exception as e:
                master_logger.error(f"An error occurred while processing the query: {e}", exc_info=True)
    except Exception as e:
        master_logger.error(f"An error occurred in the main program: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())-e 
File: ./src/utils/__init__.py

-e 
File: ./src/utils/enhanced_logging.py

import time
import logging
from functools import wraps

def log_execution_time(logger):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            result = await func(*args, **kwargs)
            end_time = time.time()
            execution_time = end_time - start_time
            logger.info(f"{func.__name__} executed in {execution_time:.4f} seconds")
            return result
        return wrapper
    return decorator

class DatabaseLogger:
    def __init__(self, logger):
        self.logger = logger
        file_handler = logging.FileHandler('database.log')
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        self.logger.addHandler(file_handler)

    def log_query(self, query, params=None):
        self.logger.debug(f"Executing query: {query}, Params: {params}")

    def log_memory_size(self, memory_size):
        self.logger.info(f"Memory size: {memory_size} bytes")

    def log_access(self, memory_id):
        self.logger.info(f"Accessed memory: {memory_id}")-e 
File: ./src/utils/logging_setup.py

import logging
import os
import socket
from datetime import datetime

def setup_logging():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    container_id = socket.gethostname() if is_running_in_docker() else 'local'
    log_directory = f'logs/{timestamp}_{container_id}'

    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    master_logger = logging.getLogger('master')
    master_logger.setLevel(logging.DEBUG)
    file_handler = logging.FileHandler(os.path.join(log_directory, "master.log"))
    file_handler.setFormatter(log_formatter)
    master_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(message)s'))
    master_logger.addHandler(console_handler)

    chat_logger = logging.getLogger('chat')
    chat_logger.setLevel(logging.DEBUG)
    chat_file_handler = logging.FileHandler(os.path.join(log_directory, 'chat.log'))
    chat_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
    chat_logger.addHandler(chat_file_handler)

    memory_logger = logging.getLogger('memory')
    memory_logger.setLevel(logging.DEBUG)
    memory_file_handler = logging.FileHandler(os.path.join(log_directory, 'memory.log'))
    memory_file_handler.setFormatter(log_formatter)
    memory_logger.addHandler(memory_file_handler)

    database_logger = logging.getLogger('database')
    database_logger.setLevel(logging.DEBUG)
    database_file_handler = logging.FileHandler(os.path.join(log_directory, 'database.log'))
    database_file_handler.setFormatter(log_formatter)
    database_logger.addHandler(database_file_handler)

    master_logger.info(f"Logging setup complete. Log directory: {log_directory}")
    return master_logger, chat_logger, memory_logger, database_logger

def is_running_in_docker() -> bool:
    return os.path.exists('/.dockerenv')-e 
File: ./src/utils/memory_analysis.py

from collections import Counter
import numpy as np
import aiosqlite

class MemoryAnalyzer:
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager

    async def analyze_distribution(self):
        async with aiosqlite.connect(self.memory_manager.master_db_path) as db:
            async with db.execute("SELECT author, COUNT(*) FROM memories GROUP BY author") as cursor:
                distribution = await cursor.fetchall()
        return dict(distribution)

    async def find_most_accessed(self, limit=10):
        # Assuming we've added an 'access_count' column to the memories table
        async with aiosqlite.connect(self.memory_manager.master_db_path) as db:
            async with db.execute("SELECT id, query, access_count FROM memories ORDER BY access_count DESC LIMIT ?", (limit,)) as cursor:
                most_accessed = await cursor.fetchall()
        return most_accessed

    async def find_most_similar(self, query, limit=10):
        query_embedding = await self.memory_manager.embeddings.aembed_query(query)
        all_memories = await self.memory_manager.get_all_memories()
        
        similarities = []
        for memory in all_memories:
            memory_embedding = np.frombuffer(memory[2])
            similarity = np.dot(query_embedding, memory_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(memory_embedding))
            similarities.append((memory[0], memory[1], similarity))
        
        similarities.sort(key=lambda x: x[2], reverse=True)
        return similarities[:limit]-e 
File: ./src/utils/reset_database.py

# src/utils/reset_database.py

import sys
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Add the base directory of the project to the PYTHONPATH
base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(base_path)

from src.memory.memory_manager import MemoryManager
from config import Config  # Import the config

if __name__ == "__main__":
    memory_manager = MemoryManager(Config.MEMORY_DB_PATH, Config.OPENAI_API_KEY)
    memory_manager.reset_database()
    print("Database has been reset.")
-e 
File: ./src/utils/similarity_analysis.py

import logging
from typing import Dict, List, Tuple

logger = logging.getLogger('memory')

class SimilarityAnalyzer:
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager

    async def analyze_retrieval_performance(self, query: str) -> Dict[str, List[Tuple[str, str, float]]]:
        try:
            results = await self.memory_manager.retrieve_relevant_memories(query, return_metadata=True)
            memories = results['memories']
            metadata = results['metadata']

            logger.info(f"Retrieved {len(memories)} memories for query: '{query}'")
            logger.info(f"Retrieval breakdown: {metadata}")

            return {
                "l2_norm": memories[:metadata['l2_count']],
                "cosine_similarity": memories[metadata['l2_count']:metadata['l2_count'] + metadata['cosine_count']],
                "bm25": memories[metadata['l2_count'] + metadata['cosine_count']:metadata['l2_count'] + metadata['cosine_count'] + metadata['bm25_count']],
                "jaccard_similarity": memories[metadata['l2_count'] + metadata['cosine_count'] + metadata['bm25_count']:]
            }
        except Exception as e:
            logger.error(f"Error during retrieval performance analysis: {str(e)}")
            return {}

    def print_analysis(self, analysis_results: Dict[str, List[Tuple[str, str, float]]]):
        if not analysis_results:
            print("No analysis results available.")
            return

        for method, memories in analysis_results.items():
            print(f"\n{method.upper()} Results:")
            for i, (query, result, _) in enumerate(memories, 1):
                print(f"{i}. Query: {query}")
                print(f"   Result: {result}")
                print()-e 
File: ./src/utils/api_utils.py

# src/utils/api_utils.py

import time
import asyncio
from functools import wraps
from cachetools import TTLCache

class RateLimiter:
    def __init__(self, max_calls, period):
        self.max_calls = max_calls
        self.period = period
        self.calls = []

    async def wait(self):
        now = time.time()
        self.calls = [call for call in self.calls if call > now - self.period]
        if len(self.calls) >= self.max_calls:
            sleep_time = self.calls[0] - (now - self.period)
            await asyncio.sleep(sleep_time)
        self.calls.append(time.time())

def rate_limited(max_calls, period):
    limiter = RateLimiter(max_calls, period)
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            await limiter.wait()
            return await func(*args, **kwargs)
        return wrapper
    return decorator

def exponential_backoff(max_retries=5, base_delay=1):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            retries = 0
            while retries < max_retries:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:  # Replace with specific exception if needed
                    if retries == max_retries - 1:
                        raise
                    delay = base_delay * (2 ** retries)
                    print(f"API call failed. Retrying in {delay} seconds...")
                    await asyncio.sleep(delay)
                    retries += 1
        return wrapper
    return decorator

# Simple cache with TTL
cache = TTLCache(maxsize=100, ttl=300)  # Cache up to 100 items for 5 minutes

def cached(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key in cache:
            return cache[key]
        result = await func(*args, **kwargs)
        cache[key] = result
        return result
    return wrapper-e 
File: ./src/utils/controller.py

import logging
from typing import List, Tuple
from src.agents.agent import Agent
from src.memory.enhanced_memory_manager import EnhancedMemoryManager
from config import config

class Controller:
    def __init__(self):
        self.logger = logging.getLogger('master')
        self.db_logger = logging.getLogger('database')
        self.memory_manager = None
        self.agent = None

    async def initialize(self):
        self.memory_manager = EnhancedMemoryManager(config.OPENAI_API_KEY)
        await self.memory_manager.initialize()
        self.agent = Agent(self.memory_manager)

    async def execute_query(self, query: str) -> str:
        try:
            response = await self.agent.process_query(query)
            self.logger.debug(f"Generated response: {response}")
            await self.memory_manager.save_memory(query, response)
            return response
        except Exception as e:
            self.logger.error(f"Error executing query '{query}': {str(e)}", exc_info=True)
            return f"An error occurred while processing your query: {str(e)}"

    async def get_recent_memories(self, limit: int) -> List[Tuple[str, str]]:
        return await self.memory_manager.get_memories(limit)-e 
File: ./src/api.py

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging
from config import config
from src.utils.memory_analysis import MemoryAnalyzer

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    response: str

# Initialize logging
master_logger, chat_logger, memory_logger, database_logger = setup_logging()

app = FastAPI()

controller = Controller()
memory_analyzer = None

@app.on_event("startup")
async def startup_event():
    global memory_analyzer
    master_logger.info("Starting up the API server")
    await controller.initialize()
    memory_analyzer = MemoryAnalyzer(controller.memory_manager)

@app.on_event("shutdown")
async def shutdown_event():
    master_logger.info("Shutting down the API server")

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    try:
        response = await controller.execute_query(request.query)
        return QueryResponse(response=response)
    except Exception as e:
        master_logger.error(f"Error processing query: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")

@app.get("/memory_distribution")
async def memory_distribution_endpoint():
    try:
        distribution = await memory_analyzer.analyze_distribution()
        return {"distribution": distribution}
    except Exception as e:
        master_logger.error(f"Error analyzing memory distribution: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error analyzing memory distribution: {str(e)}")
    
@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    master_logger.error(f"Unhandled exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": f"An unexpected error occurred: {str(exc)}"}
    )

@app.get("/consistency_check")
async def consistency_check_endpoint():
    try:
        await controller.memory_manager.run_consistency_check_and_fix()
        return {"message": "Consistency check and fix completed."}
    except Exception as e:
        master_logger.error(f"Error during consistency check: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error during consistency check: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    master_logger.info("Running the API server directly")
    uvicorn.run(app, host="0.0.0.0", port=8000)-e 
File: ./src/cli.py

import asyncio
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging
from src.utils.similarity_analysis import SimilarityAnalyzer

master_logger, chat_logger, memory_logger, database_logger = setup_logging()
controller = Controller()
similarity_analyzer = None

async def initialize():
    await controller.initialize()
    global similarity_analyzer
    similarity_analyzer = SimilarityAnalyzer(controller.memory_manager)

async def main():
    await initialize()
    while True:
        try:
            command = input("Enter a command (query/memories/consistency/analyze/quit): ").strip().lower()
            
            if command == 'quit':
                break
            elif command == 'query':
                query = input("Enter your query: ")
                response = await controller.execute_query(query)
                print(f"Response: {response}")
            elif command == 'memories':
                try:
                    limit = int(input("Enter the number of memories to retrieve: "))
                    memories = await controller.get_recent_memories(limit)
                    for i, (query, result) in enumerate(memories, 1):
                        print(f"{i}. Query: {query}\n   Result: {result}\n")
                except ValueError:
                    print("Invalid input. Please enter a number.")
            elif command == 'consistency':
                await controller.memory_manager.run_consistency_check_and_fix()
                print("Consistency check and fix completed.")
            elif command == 'analyze':
                query = input("Enter a query to analyze: ")
                analysis_results = await similarity_analyzer.analyze_retrieval_performance(query)
                similarity_analyzer.print_analysis(analysis_results)
            else:
                print("Invalid command. Please try again.")
        except Exception as e:
            master_logger.error(f"An error occurred: {str(e)}", exc_info=True)
            print(f"An error occurred: {str(e)}. Please try again.")

if __name__ == "__main__":
    asyncio.run(main())-e 
File: ./.pytest_cache/README.md

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.
-e 
File: ./.pytest_cache/.gitignore

# Created by pytest automatically.
*
-e 
File: ./.pytest_cache/CACHEDIR.TAG

Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html
-e 
File: ./.pytest_cache/v/cache/nodeids

[]-e 
File: ./.pytest_cache/v/cache/lastfailed

{
  "tests/test_memory_manager.py": true
}-e 
File: ./.pytest_cache/v/cache/stepwise

[]-e 
File: ./.dockerignore

__pycache__
*.pyc
*.pyo
*.pyd
*.pdb
*.egg
*.egg-info
dist
build
eggs
parts
var
develop-eggs
.installed.cfg
lib
lib64
*.log
logs
notebooks
data/raw/*
text_docs/*
.DS_Store
.vscode
-e 
File: ./docker-compose.yml

services:
  multi-agent-rag:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    ports:
      - "8080:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    command: ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]

  multi-agent-rag-cli:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    command: ["python", "-m", "src.cli"]
    stdin_open: true
    tty: true-e 
File: ./docs/README.md

# Multi-Agent RAG System with Continual Learning

## Project Overview

This project implements a vertical slice of a multi-agent RAG (Retrieval-Augmented Generation) system with continual learning capabilities. It uses LangChain, LangGraph, and LLM API calls to create a sophisticated question-answering system that learns from interactions.

## Key Features

- Multi-agent architecture for complex query processing
- RAG (Retrieval-Augmented Generation) for enhanced responses
- Continual learning to improve performance over time
- Distributed memory management with personal and master databases
- CLI and API interfaces for versatile interaction
- Comprehensive logging system for debugging and analysis
- Docker support for easy deployment and scaling

## System Architecture

The system consists of the following main components:

1. **Controller**: Orchestrates the overall flow of query processing.
2. **Agents**: Specialized modules for retrieval, processing, and response generation.
3. **Memory Manager**: Handles storage and retrieval of past interactions.
4. **Logging System**: Provides detailed logs for system operations, including database interactions.

## Directory Structure

```
.
├── Dockerfile
├── README.md
├── config.py
├── data/
├── docker-compose.yml
├── docs/
├── logs/
├── notebooks/
├── scripts/
├── src/
│   ├── agents/
│   ├── memory/
│   ├── utils/
│   ├── api.py
│   ├── app.py
│   └── cli.py
└── tests/
```

## Setup and Installation

### Prerequisites

- Python 3.12+
- Docker and Docker Compose

### Local Setup

1. Clone the repository:
   ```
   git clone https://github.com/your-username/multi-agent-rag.git
   cd multi-agent-rag
   ```

2. Create a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   Create a `.env` file in the root directory with the following content:
   ```
   OPENAI_API_KEY=your_openai_api_key
   TAVILY_API_KEY=your_tavily_api_key
   ```

### Docker Setup

1. Build the Docker image:
   ```
   docker-compose build
   ```

2. Run the container:
   ```
   docker-compose up
   ```

## Usage

### CLI Interface

Run the CLI interface with:

```
python src/cli.py
```

Available commands:
- `query`: Enter a query to process
- `memories`: Retrieve recent memories
- `consistency`: Run a consistency check on the databases
- `quit`: Exit the program

### API Interface

Start the API server with:

```
uvicorn src.api:app --host 0.0.0.0 --port 8000
```

API endpoints:
- POST `/query`: Submit a query for processing
- GET `/consistency_check`: Run a consistency check on the databases
- GET `/health`: Check the health status of the API
- GET `/memory_distribution`: Get the distribution of memories across containers

## Logging

Logs are stored in the `logs/` directory, organized by timestamp and container ID. There are separate log files for:

- Master log (`master.log`)
- Chat log (`chat.log`)
- Memory operations log (`memory.log`)
- Database operations log (`database.log`)

## Testing

Run the test suite with:

```
pytest tests/
```
-e 
File: ./Dockerfile

FROM python:3.12

# Create a non-root user
RUN useradd -m appuser

WORKDIR /app

COPY . /app

RUN mkdir -p /app/logs /app/data
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Create and activate virtual environment
RUN python -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Upgrade pip and install dependencies with increased timeout and using a mirror
RUN pip install --upgrade pip

# Install numpy and scipy separately with increased timeout
RUN pip install --no-cache-dir numpy==1.26.4 scipy==1.14.0 --timeout 600

# Install scikit-learn separately with an even longer timeout
RUN pip install --no-cache-dir scikit-learn==1.4.0 --timeout 900

# Install other dependencies
RUN pip install --no-cache-dir -r docker_requirements.txt --timeout 600 --index-url https://pypi.org/simple --trusted-host pypi.org

RUN pip install fastapi uvicorn

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

CMD ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]-e 
File: ./config.py

import os
from dotenv import load_dotenv

class Config:
    def __init__(self):
        load_dotenv()
        
        # General settings
        self.MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4-turbo")
        
        # Database settings
        self.MEMORY_DB_PATH = self._get_memory_db_path()
        
        # API Keys
        self.OPENAI_API_KEY = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
        self.TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
        
        # Other settings
        self.LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG")
        
        # Processing agent settings
        self.PROCESSING_AGENT_MEMORIES_INCLUDED = int(os.getenv("PROCESSING_AGENT_MEMORIES_INCLUDED", "5"))
        self.MEMORY_RETRIEVAL_THRESHOLD = float(os.getenv("MEMORY_RETRIEVAL_THRESHOLD", "0.75"))
        self.MEMORY_RETRIEVAL_LIMIT = int(os.getenv("MEMORY_RETRIEVAL_LIMIT", "10"))

    def _get_memory_db_path(self):
        return "/app/memory.db" if os.path.exists("/.dockerenv") else "memory.db"

    def validate_api_keys(self):
        return bool(self.OPENAI_API_KEY and self.TAVILY_API_KEY)

    L2_NORM_THRESHOLD = float(os.getenv("L2_NORM_THRESHOLD", "0.75"))
    COSINE_SIMILARITY_THRESHOLD = float(os.getenv("COSINE_SIMILARITY_THRESHOLD", "0.75"))
    BM25_THRESHOLD = float(os.getenv("BM25_THRESHOLD", "0.5"))
    JACCARD_SIMILARITY_THRESHOLD = float(os.getenv("JACCARD_SIMILARITY_THRESHOLD", "0.3"))

config = Config()-e 
File: ./docker_requirements.txt

langchain==0.2.3
langchain-openai==0.1.8
langchain-community==0.2.4
langchain-core==0.2.5
langchain-text-splitters==0.2.1
langsmith==0.1.77
openai==1.34.0
pydantic==2.7.4
pydantic-core==2.18.4
dataclasses-json==0.6.7
python-dotenv==1.0.1
aiosqlite
rank_bm25==0.2.2
cachetools==5.3.0