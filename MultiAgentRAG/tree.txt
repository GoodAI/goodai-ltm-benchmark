.
├── Dockerfile
├── config.py
├── data
│   ├── 0adeb4a1089f.db
│   ├── 825f9f7340d2.db
│   ├── 92a109e802cd.db
│   ├── aaafb8a77492.db
│   ├── aac2f4247439.db
│   └── master.db
├── docker-compose.yml
├── docker_requirements.txt
├── docs
│   ├── README.md
│   └── database_structure.md
├── get-docker.sh
├── logs
│   └── 20240627_114735_aaafb8a77492
│       ├── chat.log
│       ├── database.log
│       ├── master.log
│       └── memory.log
├── memory.db
├── notebooks
│   └── Multi_Agent_RAG_LangGraph.ipynb
├── scripts
│   ├── _docker_reset.sh
│   ├── generate_tree.sh
│   ├── logs_to_docs.sh
│   ├── manage_docker.sh
│   └── reset_logs.sh
├── src
│   ├── __init__.py
│   ├── __pycache__
│   │   └── __init__.cpython-310.pyc
│   ├── agents
│   │   ├── __init__.py
│   │   └── agent.py
│   ├── api.py
│   ├── app.py
│   ├── cli.py
│   ├── memory
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-310.pyc
│   │   │   └── memory_manager.cpython-310.pyc
│   │   └── memory_manager.py
│   └── utils
│       ├── __init__.py
│       ├── controller.py
│       ├── enhanced_logging.py
│       ├── logging_setup.py
│       ├── memory_analysis.py
│       └── reset_database.py
└── tree.txt

12 directories, 42 files

Note: The following list of files and directories are excluded only from the appended file contents section:

__pycache__, .git, .env, text_docs, scripts, logs, tree.txt, generate_tree.sh, README, *.ipynb, *.pdf, *.db

File Contents:

-e 
File: ./.dockerignore

__pycache__
*.pyc
*.pyo
*.pyd
*.pdb
*.egg
*.egg-info
dist
build
eggs
parts
var
develop-eggs
.installed.cfg
lib
lib64
*.log
logs
notebooks
data/raw/*
text_docs/*
.DS_Store
.vscode
-e 
File: ./Dockerfile

FROM python:3.12

# Create a non-root user
RUN useradd -m appuser

WORKDIR /app

COPY . /app

RUN mkdir -p /app/logs /app/data
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Create and activate virtual environment
RUN python -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Install dependencies in virtual environment
RUN pip install --no-cache-dir -r docker_requirements.txt
RUN pip install fastapi uvicorn

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

CMD ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]-e 
File: ./config.py

import os
from dotenv import load_dotenv

class Config:
    def __init__(self):
        load_dotenv()
        
        # General settings
        self.MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4-turbo")
        
        # Database settings
        self.MEMORY_DB_PATH = self._get_memory_db_path()
        
        # API Keys
        self.OPENAI_API_KEY = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
        self.TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
        
        # Other settings
        self.LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG")
        
        # Processing agent settings
        self.PROCESSING_AGENT_MEMORIES_INCLUDED = int(os.getenv("PROCESSING_AGENT_MEMORIES_INCLUDED", "5"))
        self.MEMORY_RETRIEVAL_THRESHOLD = float(os.getenv("MEMORY_RETRIEVAL_THRESHOLD", "0.75"))
        self.MEMORY_RETRIEVAL_LIMIT = int(os.getenv("MEMORY_RETRIEVAL_LIMIT", "10"))

    def _get_memory_db_path(self):
        return "/app/memory.db" if os.path.exists("/.dockerenv") else "memory.db"

    def validate_api_keys(self):
        return bool(self.OPENAI_API_KEY and self.TAVILY_API_KEY)

config = Config()-e 
File: ./docker-compose.yml

version: '3.8'

services:
  multi-agent-rag:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    ports:
      - "8080:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    command: ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]

  multi-agent-rag-cli:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    command: ["python", "-m", "src.cli"]
    stdin_open: true
    tty: true-e 
File: ./get-docker.sh

#!/bin/sh
set -e
# Docker Engine for Linux installation script.
#
# This script is intended as a convenient way to configure docker's package
# repositories and to install Docker Engine, This script is not recommended
# for production environments. Before running this script, make yourself familiar
# with potential risks and limitations, and refer to the installation manual
# at https://docs.docker.com/engine/install/ for alternative installation methods.
#
# The script:
#
# - Requires `root` or `sudo` privileges to run.
# - Attempts to detect your Linux distribution and version and configure your
#   package management system for you.
# - Doesn't allow you to customize most installation parameters.
# - Installs dependencies and recommendations without asking for confirmation.
# - Installs the latest stable release (by default) of Docker CLI, Docker Engine,
#   Docker Buildx, Docker Compose, containerd, and runc. When using this script
#   to provision a machine, this may result in unexpected major version upgrades
#   of these packages. Always test upgrades in a test environment before
#   deploying to your production systems.
# - Isn't designed to upgrade an existing Docker installation. When using the
#   script to update an existing installation, dependencies may not be updated
#   to the expected version, resulting in outdated versions.
#
# Source code is available at https://github.com/docker/docker-install/
#
# Usage
# ==============================================================================
#
# To install the latest stable versions of Docker CLI, Docker Engine, and their
# dependencies:
#
# 1. download the script
#
#   $ curl -fsSL https://get.docker.com -o install-docker.sh
#
# 2. verify the script's content
#
#   $ cat install-docker.sh
#
# 3. run the script with --dry-run to verify the steps it executes
#
#   $ sh install-docker.sh --dry-run
#
# 4. run the script either as root, or using sudo to perform the installation.
#
#   $ sudo sh install-docker.sh
#
# Command-line options
# ==============================================================================
#
# --version <VERSION>
# Use the --version option to install a specific version, for example:
#
#   $ sudo sh install-docker.sh --version 23.0
#
# --channel <stable|test>
#
# Use the --channel option to install from an alternative installation channel.
# The following example installs the latest versions from the "test" channel,
# which includes pre-releases (alpha, beta, rc):
#
#   $ sudo sh install-docker.sh --channel test
#
# Alternatively, use the script at https://test.docker.com, which uses the test
# channel as default.
#
# --mirror <Aliyun|AzureChinaCloud>
#
# Use the --mirror option to install from a mirror supported by this script.
# Available mirrors are "Aliyun" (https://mirrors.aliyun.com/docker-ce), and
# "AzureChinaCloud" (https://mirror.azure.cn/docker-ce), for example:
#
#   $ sudo sh install-docker.sh --mirror AzureChinaCloud
#
# ==============================================================================


# Git commit from https://github.com/docker/docker-install when
# the script was uploaded (Should only be modified by upload job):
SCRIPT_COMMIT_SHA="6d9743e9656cc56f699a64800b098d5ea5a60020"

# strip "v" prefix if present
VERSION="${VERSION#v}"

# The channel to install from:
#   * stable
#   * test
#   * edge (deprecated)
#   * nightly (deprecated)
DEFAULT_CHANNEL_VALUE="stable"
if [ -z "$CHANNEL" ]; then
	CHANNEL=$DEFAULT_CHANNEL_VALUE
fi

DEFAULT_DOWNLOAD_URL="https://download.docker.com"
if [ -z "$DOWNLOAD_URL" ]; then
	DOWNLOAD_URL=$DEFAULT_DOWNLOAD_URL
fi

DEFAULT_REPO_FILE="docker-ce.repo"
if [ -z "$REPO_FILE" ]; then
	REPO_FILE="$DEFAULT_REPO_FILE"
fi

mirror=''
DRY_RUN=${DRY_RUN:-}
while [ $# -gt 0 ]; do
	case "$1" in
		--channel)
			CHANNEL="$2"
			shift
			;;
		--dry-run)
			DRY_RUN=1
			;;
		--mirror)
			mirror="$2"
			shift
			;;
		--version)
			VERSION="${2#v}"
			shift
			;;
		--*)
			echo "Illegal option $1"
			;;
	esac
	shift $(( $# > 0 ? 1 : 0 ))
done

case "$mirror" in
	Aliyun)
		DOWNLOAD_URL="https://mirrors.aliyun.com/docker-ce"
		;;
	AzureChinaCloud)
		DOWNLOAD_URL="https://mirror.azure.cn/docker-ce"
		;;
	"")
		;;
	*)
		>&2 echo "unknown mirror '$mirror': use either 'Aliyun', or 'AzureChinaCloud'."
		exit 1
		;;
esac

case "$CHANNEL" in
	stable|test)
		;;
	edge|nightly)
		>&2 echo "DEPRECATED: the $CHANNEL channel has been deprecated and is no longer supported by this script."
		exit 1
		;;
	*)
		>&2 echo "unknown CHANNEL '$CHANNEL': use either stable or test."
		exit 1
		;;
esac

command_exists() {
	command -v "$@" > /dev/null 2>&1
}

# version_gte checks if the version specified in $VERSION is at least the given
# SemVer (Maj.Minor[.Patch]), or CalVer (YY.MM) version.It returns 0 (success)
# if $VERSION is either unset (=latest) or newer or equal than the specified
# version, or returns 1 (fail) otherwise.
#
# examples:
#
# VERSION=23.0
# version_gte 23.0  // 0 (success)
# version_gte 20.10 // 0 (success)
# version_gte 19.03 // 0 (success)
# version_gte 21.10 // 1 (fail)
version_gte() {
	if [ -z "$VERSION" ]; then
			return 0
	fi
	eval version_compare "$VERSION" "$1"
}

# version_compare compares two version strings (either SemVer (Major.Minor.Path),
# or CalVer (YY.MM) version strings. It returns 0 (success) if version A is newer
# or equal than version B, or 1 (fail) otherwise. Patch releases and pre-release
# (-alpha/-beta) are not taken into account
#
# examples:
#
# version_compare 23.0.0 20.10 // 0 (success)
# version_compare 23.0 20.10   // 0 (success)
# version_compare 20.10 19.03  // 0 (success)
# version_compare 20.10 20.10  // 0 (success)
# version_compare 19.03 20.10  // 1 (fail)
version_compare() (
	set +x

	yy_a="$(echo "$1" | cut -d'.' -f1)"
	yy_b="$(echo "$2" | cut -d'.' -f1)"
	if [ "$yy_a" -lt "$yy_b" ]; then
		return 1
	fi
	if [ "$yy_a" -gt "$yy_b" ]; then
		return 0
	fi
	mm_a="$(echo "$1" | cut -d'.' -f2)"
	mm_b="$(echo "$2" | cut -d'.' -f2)"

	# trim leading zeros to accommodate CalVer
	mm_a="${mm_a#0}"
	mm_b="${mm_b#0}"

	if [ "${mm_a:-0}" -lt "${mm_b:-0}" ]; then
		return 1
	fi

	return 0
)

is_dry_run() {
	if [ -z "$DRY_RUN" ]; then
		return 1
	else
		return 0
	fi
}

is_wsl() {
	case "$(uname -r)" in
	*microsoft* ) true ;; # WSL 2
	*Microsoft* ) true ;; # WSL 1
	* ) false;;
	esac
}

is_darwin() {
	case "$(uname -s)" in
	*darwin* ) true ;;
	*Darwin* ) true ;;
	* ) false;;
	esac
}

deprecation_notice() {
	distro=$1
	distro_version=$2
	echo
	printf "\033[91;1mDEPRECATION WARNING\033[0m\n"
	printf "    This Linux distribution (\033[1m%s %s\033[0m) reached end-of-life and is no longer supported by this script.\n" "$distro" "$distro_version"
	echo   "    No updates or security fixes will be released for this distribution, and users are recommended"
	echo   "    to upgrade to a currently maintained version of $distro."
	echo
	printf   "Press \033[1mCtrl+C\033[0m now to abort this script, or wait for the installation to continue."
	echo
	sleep 10
}

get_distribution() {
	lsb_dist=""
	# Every system that we officially support has /etc/os-release
	if [ -r /etc/os-release ]; then
		lsb_dist="$(. /etc/os-release && echo "$ID")"
	fi
	# Returning an empty string here should be alright since the
	# case statements don't act unless you provide an actual value
	echo "$lsb_dist"
}

echo_docker_as_nonroot() {
	if is_dry_run; then
		return
	fi
	if command_exists docker && [ -e /var/run/docker.sock ]; then
		(
			set -x
			$sh_c 'docker version'
		) || true
	fi

	# intentionally mixed spaces and tabs here -- tabs are stripped by "<<-EOF", spaces are kept in the output
	echo
	echo "================================================================================"
	echo
	if version_gte "20.10"; then
		echo "To run Docker as a non-privileged user, consider setting up the"
		echo "Docker daemon in rootless mode for your user:"
		echo
		echo "    dockerd-rootless-setuptool.sh install"
		echo
		echo "Visit https://docs.docker.com/go/rootless/ to learn about rootless mode."
		echo
	fi
	echo
	echo "To run the Docker daemon as a fully privileged service, but granting non-root"
	echo "users access, refer to https://docs.docker.com/go/daemon-access/"
	echo
	echo "WARNING: Access to the remote API on a privileged Docker daemon is equivalent"
	echo "         to root access on the host. Refer to the 'Docker daemon attack surface'"
	echo "         documentation for details: https://docs.docker.com/go/attack-surface/"
	echo
	echo "================================================================================"
	echo
}

# Check if this is a forked Linux distro
check_forked() {

	# Check for lsb_release command existence, it usually exists in forked distros
	if command_exists lsb_release; then
		# Check if the `-u` option is supported
		set +e
		lsb_release -a -u > /dev/null 2>&1
		lsb_release_exit_code=$?
		set -e

		# Check if the command has exited successfully, it means we're in a forked distro
		if [ "$lsb_release_exit_code" = "0" ]; then
			# Print info about current distro
			cat <<-EOF
			You're using '$lsb_dist' version '$dist_version'.
			EOF

			# Get the upstream release info
			lsb_dist=$(lsb_release -a -u 2>&1 | tr '[:upper:]' '[:lower:]' | grep -E 'id' | cut -d ':' -f 2 | tr -d '[:space:]')
			dist_version=$(lsb_release -a -u 2>&1 | tr '[:upper:]' '[:lower:]' | grep -E 'codename' | cut -d ':' -f 2 | tr -d '[:space:]')

			# Print info about upstream distro
			cat <<-EOF
			Upstream release is '$lsb_dist' version '$dist_version'.
			EOF
		else
			if [ -r /etc/debian_version ] && [ "$lsb_dist" != "ubuntu" ] && [ "$lsb_dist" != "raspbian" ]; then
				if [ "$lsb_dist" = "osmc" ]; then
					# OSMC runs Raspbian
					lsb_dist=raspbian
				else
					# We're Debian and don't even know it!
					lsb_dist=debian
				fi
				dist_version="$(sed 's/\/.*//' /etc/debian_version | sed 's/\..*//')"
				case "$dist_version" in
					12)
						dist_version="bookworm"
					;;
					11)
						dist_version="bullseye"
					;;
					10)
						dist_version="buster"
					;;
					9)
						dist_version="stretch"
					;;
					8)
						dist_version="jessie"
					;;
				esac
			fi
		fi
	fi
}

do_install() {
	echo "# Executing docker install script, commit: $SCRIPT_COMMIT_SHA"

	if command_exists docker; then
		cat >&2 <<-'EOF'
			Warning: the "docker" command appears to already exist on this system.

			If you already have Docker installed, this script can cause trouble, which is
			why we're displaying this warning and provide the opportunity to cancel the
			installation.

			If you installed the current Docker package using this script and are using it
			again to update Docker, you can safely ignore this message.

			You may press Ctrl+C now to abort this script.
		EOF
		( set -x; sleep 20 )
	fi

	user="$(id -un 2>/dev/null || true)"

	sh_c='sh -c'
	if [ "$user" != 'root' ]; then
		if command_exists sudo; then
			sh_c='sudo -E sh -c'
		elif command_exists su; then
			sh_c='su -c'
		else
			cat >&2 <<-'EOF'
			Error: this installer needs the ability to run commands as root.
			We are unable to find either "sudo" or "su" available to make this happen.
			EOF
			exit 1
		fi
	fi

	if is_dry_run; then
		sh_c="echo"
	fi

	# perform some very rudimentary platform detection
	lsb_dist=$( get_distribution )
	lsb_dist="$(echo "$lsb_dist" | tr '[:upper:]' '[:lower:]')"

	if is_wsl; then
		echo
		echo "WSL DETECTED: We recommend using Docker Desktop for Windows."
		echo "Please get Docker Desktop from https://www.docker.com/products/docker-desktop/"
		echo
		cat >&2 <<-'EOF'

			You may press Ctrl+C now to abort this script.
		EOF
		( set -x; sleep 20 )
	fi

	case "$lsb_dist" in

		ubuntu)
			if command_exists lsb_release; then
				dist_version="$(lsb_release --codename | cut -f2)"
			fi
			if [ -z "$dist_version" ] && [ -r /etc/lsb-release ]; then
				dist_version="$(. /etc/lsb-release && echo "$DISTRIB_CODENAME")"
			fi
		;;

		debian|raspbian)
			dist_version="$(sed 's/\/.*//' /etc/debian_version | sed 's/\..*//')"
			case "$dist_version" in
				12)
					dist_version="bookworm"
				;;
				11)
					dist_version="bullseye"
				;;
				10)
					dist_version="buster"
				;;
				9)
					dist_version="stretch"
				;;
				8)
					dist_version="jessie"
				;;
			esac
		;;

		centos|rhel)
			if [ -z "$dist_version" ] && [ -r /etc/os-release ]; then
				dist_version="$(. /etc/os-release && echo "$VERSION_ID")"
			fi
		;;

		*)
			if command_exists lsb_release; then
				dist_version="$(lsb_release --release | cut -f2)"
			fi
			if [ -z "$dist_version" ] && [ -r /etc/os-release ]; then
				dist_version="$(. /etc/os-release && echo "$VERSION_ID")"
			fi
		;;

	esac

	# Check if this is a forked Linux distro
	check_forked

	# Print deprecation warnings for distro versions that recently reached EOL,
	# but may still be commonly used (especially LTS versions).
	case "$lsb_dist.$dist_version" in
		debian.stretch|debian.jessie)
			deprecation_notice "$lsb_dist" "$dist_version"
			;;
		raspbian.stretch|raspbian.jessie)
			deprecation_notice "$lsb_dist" "$dist_version"
			;;
		ubuntu.xenial|ubuntu.trusty)
			deprecation_notice "$lsb_dist" "$dist_version"
			;;
		ubuntu.lunar|ubuntu.kinetic|ubuntu.impish|ubuntu.hirsute|ubuntu.groovy|ubuntu.eoan|ubuntu.disco|ubuntu.cosmic)
			deprecation_notice "$lsb_dist" "$dist_version"
			;;
		fedora.*)
			if [ "$dist_version" -lt 36 ]; then
				deprecation_notice "$lsb_dist" "$dist_version"
			fi
			;;
	esac

	# Run setup for each distro accordingly
	case "$lsb_dist" in
		ubuntu|debian|raspbian)
			pre_reqs="apt-transport-https ca-certificates curl"
			apt_repo="deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] $DOWNLOAD_URL/linux/$lsb_dist $dist_version $CHANNEL"
			(
				if ! is_dry_run; then
					set -x
				fi
				$sh_c 'apt-get update -qq >/dev/null'
				$sh_c "DEBIAN_FRONTEND=noninteractive apt-get install -y -qq $pre_reqs >/dev/null"
				$sh_c 'install -m 0755 -d /etc/apt/keyrings'
				$sh_c "curl -fsSL \"$DOWNLOAD_URL/linux/$lsb_dist/gpg\" -o /etc/apt/keyrings/docker.asc"
				$sh_c "chmod a+r /etc/apt/keyrings/docker.asc"
				$sh_c "echo \"$apt_repo\" > /etc/apt/sources.list.d/docker.list"
				$sh_c 'apt-get update -qq >/dev/null'
			)
			pkg_version=""
			if [ -n "$VERSION" ]; then
				if is_dry_run; then
					echo "# WARNING: VERSION pinning is not supported in DRY_RUN"
				else
					# Will work for incomplete versions IE (17.12), but may not actually grab the "latest" if in the test channel
					pkg_pattern="$(echo "$VERSION" | sed 's/-ce-/~ce~.*/g' | sed 's/-/.*/g')"
					search_command="apt-cache madison docker-ce | grep '$pkg_pattern' | head -1 | awk '{\$1=\$1};1' | cut -d' ' -f 3"
					pkg_version="$($sh_c "$search_command")"
					echo "INFO: Searching repository for VERSION '$VERSION'"
					echo "INFO: $search_command"
					if [ -z "$pkg_version" ]; then
						echo
						echo "ERROR: '$VERSION' not found amongst apt-cache madison results"
						echo
						exit 1
					fi
					if version_gte "18.09"; then
							search_command="apt-cache madison docker-ce-cli | grep '$pkg_pattern' | head -1 | awk '{\$1=\$1};1' | cut -d' ' -f 3"
							echo "INFO: $search_command"
							cli_pkg_version="=$($sh_c "$search_command")"
					fi
					pkg_version="=$pkg_version"
				fi
			fi
			(
				pkgs="docker-ce${pkg_version%=}"
				if version_gte "18.09"; then
						# older versions didn't ship the cli and containerd as separate packages
						pkgs="$pkgs docker-ce-cli${cli_pkg_version%=} containerd.io"
				fi
				if version_gte "20.10"; then
						pkgs="$pkgs docker-compose-plugin docker-ce-rootless-extras$pkg_version"
				fi
				if version_gte "23.0"; then
						pkgs="$pkgs docker-buildx-plugin"
				fi
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "DEBIAN_FRONTEND=noninteractive apt-get install -y -qq $pkgs >/dev/null"
			)
			echo_docker_as_nonroot
			exit 0
			;;
		centos|fedora|rhel)
			if [ "$(uname -m)" != "s390x" ] && [ "$lsb_dist" = "rhel" ]; then
				echo "Packages for RHEL are currently only available for s390x."
				exit 1
			fi

			if command_exists dnf; then
				pkg_manager="dnf"
				pkg_manager_flags="--best"
				config_manager="dnf config-manager"
				enable_channel_flag="--set-enabled"
				disable_channel_flag="--set-disabled"
				pre_reqs="dnf-plugins-core"
			else
				pkg_manager="yum"
				pkg_manager_flags=""
				config_manager="yum-config-manager"
				enable_channel_flag="--enable"
				disable_channel_flag="--disable"
				pre_reqs="yum-utils"
			fi

			if [ "$lsb_dist" = "fedora" ]; then
				pkg_suffix="fc$dist_version"
			else
				pkg_suffix="el"
			fi
			repo_file_url="$DOWNLOAD_URL/linux/$lsb_dist/$REPO_FILE"
			(
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "$pkg_manager $pkg_manager_flags install -y -q $pre_reqs"
				$sh_c "$config_manager --add-repo $repo_file_url"

				if [ "$CHANNEL" != "stable" ]; then
					$sh_c "$config_manager $disable_channel_flag 'docker-ce-*'"
					$sh_c "$config_manager $enable_channel_flag 'docker-ce-$CHANNEL'"
				fi
				$sh_c "$pkg_manager makecache"
			)
			pkg_version=""
			if [ -n "$VERSION" ]; then
				if is_dry_run; then
					echo "# WARNING: VERSION pinning is not supported in DRY_RUN"
				else
					pkg_pattern="$(echo "$VERSION" | sed 's/-ce-/\\\\.ce.*/g' | sed 's/-/.*/g').*$pkg_suffix"
					search_command="$pkg_manager list --showduplicates docker-ce | grep '$pkg_pattern' | tail -1 | awk '{print \$2}'"
					pkg_version="$($sh_c "$search_command")"
					echo "INFO: Searching repository for VERSION '$VERSION'"
					echo "INFO: $search_command"
					if [ -z "$pkg_version" ]; then
						echo
						echo "ERROR: '$VERSION' not found amongst $pkg_manager list results"
						echo
						exit 1
					fi
					if version_gte "18.09"; then
						# older versions don't support a cli package
						search_command="$pkg_manager list --showduplicates docker-ce-cli | grep '$pkg_pattern' | tail -1 | awk '{print \$2}'"
						cli_pkg_version="$($sh_c "$search_command" | cut -d':' -f 2)"
					fi
					# Cut out the epoch and prefix with a '-'
					pkg_version="-$(echo "$pkg_version" | cut -d':' -f 2)"
				fi
			fi
			(
				pkgs="docker-ce$pkg_version"
				if version_gte "18.09"; then
					# older versions didn't ship the cli and containerd as separate packages
					if [ -n "$cli_pkg_version" ]; then
						pkgs="$pkgs docker-ce-cli-$cli_pkg_version containerd.io"
					else
						pkgs="$pkgs docker-ce-cli containerd.io"
					fi
				fi
				if version_gte "20.10"; then
					pkgs="$pkgs docker-compose-plugin docker-ce-rootless-extras$pkg_version"
				fi
				if version_gte "23.0"; then
						pkgs="$pkgs docker-buildx-plugin"
				fi
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "$pkg_manager $pkg_manager_flags install -y -q $pkgs"
			)
			echo_docker_as_nonroot
			exit 0
			;;
		sles)
			if [ "$(uname -m)" != "s390x" ]; then
				echo "Packages for SLES are currently only available for s390x"
				exit 1
			fi
			repo_file_url="$DOWNLOAD_URL/linux/$lsb_dist/$REPO_FILE"
			pre_reqs="ca-certificates curl libseccomp2 awk"
			(
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "zypper install -y $pre_reqs"
				$sh_c "zypper addrepo $repo_file_url"
				if ! is_dry_run; then
						cat >&2 <<-'EOF'
						WARNING!!
						openSUSE repository (https://download.opensuse.org/repositories/security:/SELinux) will be enabled now.
						Do you wish to continue?
						You may press Ctrl+C now to abort this script.
						EOF
						( set -x; sleep 30 )
				fi
				opensuse_repo="https://download.opensuse.org/repositories/security:/SELinux/openSUSE_Factory/security:SELinux.repo"
				$sh_c "zypper addrepo $opensuse_repo"
				$sh_c "zypper --gpg-auto-import-keys refresh"
				$sh_c "zypper lr -d"
			)
			pkg_version=""
			if [ -n "$VERSION" ]; then
				if is_dry_run; then
					echo "# WARNING: VERSION pinning is not supported in DRY_RUN"
				else
					pkg_pattern="$(echo "$VERSION" | sed 's/-ce-/\\\\.ce.*/g' | sed 's/-/.*/g')"
					search_command="zypper search -s --match-exact 'docker-ce' | grep '$pkg_pattern' | tail -1 | awk '{print \$6}'"
					pkg_version="$($sh_c "$search_command")"
					echo "INFO: Searching repository for VERSION '$VERSION'"
					echo "INFO: $search_command"
					if [ -z "$pkg_version" ]; then
						echo
						echo "ERROR: '$VERSION' not found amongst zypper list results"
						echo
						exit 1
					fi
					search_command="zypper search -s --match-exact 'docker-ce-cli' | grep '$pkg_pattern' | tail -1 | awk '{print \$6}'"
					# It's okay for cli_pkg_version to be blank, since older versions don't support a cli package
					cli_pkg_version="$($sh_c "$search_command")"
					pkg_version="-$pkg_version"
				fi
			fi
			(
				pkgs="docker-ce$pkg_version"
				if version_gte "18.09"; then
					if [ -n "$cli_pkg_version" ]; then
						# older versions didn't ship the cli and containerd as separate packages
						pkgs="$pkgs docker-ce-cli-$cli_pkg_version containerd.io"
					else
						pkgs="$pkgs docker-ce-cli containerd.io"
					fi
				fi
				if version_gte "20.10"; then
					pkgs="$pkgs docker-compose-plugin docker-ce-rootless-extras$pkg_version"
				fi
				if version_gte "23.0"; then
						pkgs="$pkgs docker-buildx-plugin"
				fi
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "zypper -q install -y $pkgs"
			)
			echo_docker_as_nonroot
			exit 0
			;;
		*)
			if [ -z "$lsb_dist" ]; then
				if is_darwin; then
					echo
					echo "ERROR: Unsupported operating system 'macOS'"
					echo "Please get Docker Desktop from https://www.docker.com/products/docker-desktop"
					echo
					exit 1
				fi
			fi
			echo
			echo "ERROR: Unsupported distribution '$lsb_dist'"
			echo
			exit 1
			;;
	esac
	exit 1
}

# wrapped up in a function so that we have some protection against only getting
# half the file during "curl | sh"
do_install
-e 
File: ./docker_requirements.txt

langchain==0.2.3
langchain-openai==0.1.8
langchain-community==0.2.4
langchain-core==0.2.5
langchain-text-splitters==0.2.1
langsmith==0.1.77
openai==1.34.0
numpy==1.26.4
pydantic==2.7.4
pydantic-core==2.18.4
dataclasses-json==0.6.7
python-dotenv==1.0.1
aiosqlite
-e 
File: ./src/__init__.py

-e 
File: ./src/agents/__init__.py

-e 
File: ./src/agents/agent.py

import logging
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from typing import List, Tuple
from config import config
import json

logger = logging.getLogger('master')
chat_logger = logging.getLogger('chat')

class Agent:
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager
        self.chat_model = ChatOpenAI(model_name=config.MODEL_NAME)

    async def process_query(self, query: str) -> str:
        relevant_memories = await self.memory_manager.retrieve_relevant_memories(query)
        context = "\n\n".join([f"{doc[0]}\n{doc[1]} ({doc[2]})" for doc in relevant_memories])
        
        messages = [
            HumanMessage(content=f"REQUEST = \"{query}\". CONTEXT = \"\"\"{context}\"\"\" INSTRUCTIONS = Your 'CONTEXT' is formed of reverse chronologically ordered 'memories' in the format <REQUEST>,<RESPONSE> (<TIMESTAMP>). Use these to best respond to the request.")
        ]
        
        # Log the full message sent to the API
        chat_logger.info(f"Full ChatGPT API request: {json.dumps([m.dict() for m in messages], indent=2)}")
        
        response = await self.chat_model.ainvoke(messages)
        
        # Log the full API response
        chat_logger.info(f"Full ChatGPT API response: {json.dumps(response.dict(), indent=2)}")
        
        logger.debug(f"Processed query: {query} with context: {context}")
        
        return response.content-e 
File: ./src/app.py

import asyncio
from config import Config
from MultiAgentRAG.src.utils.controller import Controller
from MultiAgentRAG.src.utils.logging_setup import setup_logging
import logging

async def process_query(controller, query, chat_logger, memory_logger):
    chat_logger.info(f"Query: {query}")
    response = await controller.execute_query(query)
    chat_logger.info(f"Response: {response}")
    
    memories = await controller.get_recent_memories(5)
    memory_logger.info("Recent Memories:")
    for memory in memories:
        memory_logger.info(f"Query: {memory[0]}, Result: {memory[1]}")

async def main():
    # Initialize logging
    master_logger, chat_logger, memory_logger = setup_logging()

    # Set the log level for the memory logger
    memory_logger.setLevel(logging.DEBUG)

    try:
        master_logger.info("Starting the Multi-Agent RAG System")
        config = Config()
        
        if not config.validate_api_keys():
            raise EnvironmentError("API keys not found in environment variables")

        master_logger.info("API keys successfully loaded")
        master_logger.info(f"Using memory database path: {config.MEMORY_DB_PATH}")

        master_logger.debug("Initializing controller")
        controller = Controller(config)

        while True:
            try:
                query = input("Enter your query (or 'quit' to exit): ")
                if query.lower() == "quit":
                    master_logger.info("Exiting the program")
                    break

                master_logger.info(f"Executing query: {query}")
                await process_query(controller, query, chat_logger, memory_logger)

            except EOFError:
                master_logger.info("Received EOF, exiting the program")
                break
            except Exception as e:
                master_logger.error(f"An error occurred while processing the query: {e}", exc_info=True)
    except Exception as e:
        master_logger.error(f"An error occurred in the main program: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())-e 
File: ./src/memory/__init__.py

-e 
File: ./src/memory/memory_manager.py

import logging
import aiosqlite
from typing import List, Tuple, Union, Dict
from langchain_openai import OpenAIEmbeddings
import numpy as np
from config import config
import os
from src.utils.enhanced_logging import log_execution_time, DatabaseLogger
import json
import csv
import io

logger = logging.getLogger('memory')

class MemoryManager:
    def __init__(self, api_key: str):
        self.master_db_path = "/app/data/master.db"
        self.personal_db_path = self.get_personal_db_path()
        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        self.container_id = self.get_container_id()
        self.db_logger = DatabaseLogger(logging.getLogger('database'))

    async def initialize(self):
        await self.initialize_databases()

    async def initialize_databases(self):
        os.makedirs("/app/data", exist_ok=True)
        await self.create_tables(self.master_db_path)
        await self.create_tables(self.personal_db_path)
        await self.create_changelog_table(self.master_db_path)
        await self.create_changelog_table(self.personal_db_path)

    @log_execution_time(logger)
    async def inspect_database(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            async with db.execute("SELECT * FROM memories") as cursor:
                columns = [description[0] for description in cursor.description]
                rows = await cursor.fetchall()
        return {"columns": columns, "rows": rows}

    @log_execution_time(logger)
    async def compare_databases(self):
        personal_db = await self.inspect_database(self.personal_db_path)
        master_db = await self.inspect_database(self.master_db_path)
        
        personal_ids = set(row[0] for row in personal_db['rows'])
        master_ids = set(row[0] for row in master_db['rows'])
        
        only_in_personal = personal_ids - master_ids
        only_in_master = master_ids - personal_ids
        in_both = personal_ids.intersection(master_ids)
        
        return {
            "only_in_personal": list(only_in_personal),
            "only_in_master": list(only_in_master),
            "in_both": list(in_both)
        }

    def get_container_id(self):
        return os.environ.get('HOSTNAME', 'local')

    def get_personal_db_path(self):
        container_id = self.get_container_id()
        return f"/app/data/{container_id}.db"

    async def create_tables(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            if db_path == self.master_db_path:
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS memories (
                        id INTEGER PRIMARY KEY,
                        query TEXT,
                        result TEXT,
                        embedding BLOB,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        author TEXT
                    )
                """)
            else:
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS memories (
                        id INTEGER PRIMARY KEY,
                        query TEXT,
                        result TEXT,
                        embedding BLOB,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                """)
            await db.commit()
        logger.debug(f"Ensured memories table exists in {db_path}")

    async def create_changelog_table(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS changelog (
                    id INTEGER PRIMARY KEY,
                    operation TEXT,
                    memory_id INTEGER,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            await db.commit()

    async def log_change(self, db_path, operation, memory_id):
        async with aiosqlite.connect(db_path) as db:
            await db.execute("""
                INSERT INTO changelog (operation, memory_id) VALUES (?, ?)
            """, (operation, memory_id))
            await db.commit()

    async def save_memory(self, query: str, result: str):
        try:
            embedding = np.array(await self.embeddings.aembed_query(query)).tobytes()
            
            async with aiosqlite.connect(self.personal_db_path) as db:
                self.db_logger.log_query("INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)")
                cursor = await db.execute("""
                    INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)
                """, (query, result, embedding))
                memory_id = cursor.lastrowid
                await db.commit()
            self.db_logger.log_access(memory_id)
            await self.log_change(self.personal_db_path, "INSERT", memory_id)
            
            async with aiosqlite.connect(self.master_db_path) as db:
                self.db_logger.log_query("INSERT INTO memories (query, result, embedding, author) VALUES (?, ?, ?, ?)")
                cursor = await db.execute("""
                    INSERT INTO memories (query, result, embedding, author) VALUES (?, ?, ?, ?)
                """, (query, result, embedding, self.container_id))
                memory_id = cursor.lastrowid
                await db.commit()
            self.db_logger.log_access(memory_id)
            await self.log_change(self.master_db_path, "INSERT", memory_id)
            
            logger.debug(f"Saved memory for query: {query} with result: {result}")
        except Exception as e:
            logger.error(f"Error saving memory for query '{query}': {str(e)}", exc_info=True)
            raise
        
    async def retrieve_relevant_memories(self, query: str, threshold: float = config.MEMORY_RETRIEVAL_THRESHOLD, return_metadata: bool = False) -> Union[List[Tuple[str, str, str]], Dict]:
        try:
            query_embedding = np.array(await self.embeddings.aembed_query(query))
            
            personal_memories = await self._retrieve_from_db(self.personal_db_path, query_embedding, threshold)
            self.db_logger.log_query("SELECT query, result, embedding, timestamp FROM memories")
            
            metadata = {"queried_databases": ["personal"]}
            
            if len(personal_memories) < 3:
                master_memories = await self._retrieve_from_db(self.master_db_path, query_embedding, threshold, exclude_author=self.container_id)
                combined_memories = personal_memories + master_memories
                combined_memories.sort(key=lambda x: x[2], reverse=True)
                relevant_memories = combined_memories[:config.MEMORY_RETRIEVAL_LIMIT]
                metadata["queried_databases"].append("master")
            else:
                relevant_memories = personal_memories[:config.MEMORY_RETRIEVAL_LIMIT]
            
            if return_metadata:
                metadata["similarity_scores"] = [memory[2] for memory in relevant_memories]
                return {
                    "memories": [(memory[0], memory[1], memory[3]) for memory in relevant_memories],
                    "metadata": metadata
                }
            else:
                return [(memory[0], memory[1], memory[3]) for memory in relevant_memories]
        except Exception as e:
            logger.error(f"Error retrieving relevant memories for query '{query}': {str(e)}", exc_info=True)
            raise

    async def _retrieve_from_db(self, db_path: str, query_embedding: np.ndarray, threshold: float, exclude_author: str = None) -> List[Tuple[str, str, float, str]]:
        async with aiosqlite.connect(db_path) as db:
            if exclude_author:
                async with db.execute("SELECT query, result, embedding, timestamp FROM memories WHERE author != ?", (exclude_author,)) as cursor:
                    memories = await cursor.fetchall()
            else:
                async with db.execute("SELECT query, result, embedding, timestamp FROM memories") as cursor:
                    memories = await cursor.fetchall()

        relevant_memories = []
        for memory in memories:
            memory_query, memory_result, memory_embedding, timestamp = memory
            memory_embedding = np.frombuffer(memory_embedding)
            similarity = np.dot(query_embedding, memory_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(memory_embedding))
            if similarity >= threshold:
                relevant_memories.append((memory_query, memory_result, similarity, timestamp))

        relevant_memories.sort(key=lambda x: x[2], reverse=True)
        return relevant_memories

    async def get_memories(self, limit: int = config.MEMORY_RETRIEVAL_LIMIT) -> List[Tuple[str, str]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("""
                SELECT query, result FROM memories ORDER BY timestamp DESC LIMIT ?
            """, (limit,)) as cursor:
                memories = await cursor.fetchall()
        logger.debug(f"Retrieved {len(memories)} memories")
        return memories
    
    async def export_memories(self, db_path, format='json'):
        async with aiosqlite.connect(db_path) as db:
            async with db.execute("SELECT * FROM memories") as cursor:
                rows = await cursor.fetchall()
                columns = [description[0] for description in cursor.description]

        if format == 'json':
            data = [dict(zip(columns, row)) for row in rows]
            return json.dumps(data, default=str)
        elif format == 'csv':
            output = io.StringIO()
            writer = csv.writer(output)
            writer.writerow(columns)
            writer.writerows(rows)
            return output.getvalue()
        else:
            raise ValueError("Unsupported format. Use 'json' or 'csv'.")
        
    async def import_memories(self, db_path, data, format='json'):
        if format == 'json':
            memories = json.loads(data)
        elif format == 'csv':
            reader = csv.DictReader(io.StringIO(data))
            memories = list(reader)
        else:
            raise ValueError("Unsupported format. Use 'json' or 'csv'.")

        async with aiosqlite.connect(db_path) as db:
            for memory in memories:
                await db.execute("""
                    INSERT INTO memories (query, result, embedding, timestamp, author)
                    VALUES (?, ?, ?, ?, ?)
                """, (memory['query'], memory['result'], memory['embedding'], memory['timestamp'], memory.get('author')))
            await db.commit()

    async def explain_memory_retrieval(self, query: str, threshold: float = config.MEMORY_RETRIEVAL_THRESHOLD):
        explanation = []
        query_embedding = np.array(await self.embeddings.aembed_query(query))
        
        explanation.append(f"1. Converted query '{query}' to embedding.")
        
        personal_memories = await self._retrieve_from_db(self.personal_db_path, query_embedding, threshold)
        explanation.append(f"2. Retrieved {len(personal_memories)} memories from personal database.")
        
        if len(personal_memories) < 3:
            explanation.append("3. Less than 3 relevant memories found in personal database. Searching master database.")
            master_memories = await self._retrieve_from_db(self.master_db_path, query_embedding, threshold, exclude_author=self.container_id)
            explanation.append(f"4. Retrieved {len(master_memories)} additional memories from master database.")
            combined_memories = personal_memories + master_memories
            combined_memories.sort(key=lambda x: x[2], reverse=True)
            relevant_memories = combined_memories[:config.MEMORY_RETRIEVAL_LIMIT]
        else:
            explanation.append("3. Sufficient memories found in personal database. Not searching master database.")
            relevant_memories = personal_memories[:config.MEMORY_RETRIEVAL_LIMIT]
        
        explanation.append(f"5. Selected top {len(relevant_memories)} memories based on similarity.")
        for i, memory in enumerate(relevant_memories, 1):
            explanation.append(f"   {i}. Memory ID: {memory[0]}, Similarity: {memory[2]:.4f}")
        
        return "\n".join(explanation)
    
    async def perform_consistency_check(self):
        discrepancies = []
        
        async with aiosqlite.connect(self.personal_db_path) as personal_db, \
                   aiosqlite.connect(self.master_db_path) as master_db:
            async with personal_db.execute("SELECT id, query, result FROM memories") as personal_cursor:
                personal_memories = await personal_cursor.fetchall()

            for p_id, p_query, p_result in personal_memories:
                async with master_db.execute("SELECT id, query, result FROM memories WHERE author = ? AND query = ?", 
                                             (self.container_id, p_query)) as master_cursor:
                    master_memory = await master_cursor.fetchone()

                if master_memory is None:
                    discrepancies.append({
                        "type": "missing_in_master",
                        "personal_id": p_id,
                        "query": p_query
                    })
                elif master_memory[2] != p_result:
                    discrepancies.append({
                        "type": "content_mismatch",
                        "personal_id": p_id,
                        "master_id": master_memory[0],
                        "query": p_query,
                        "personal_result": p_result,
                        "master_result": master_memory[2]
                    })

            # Check for memories in master that are not in personal
            async with master_db.execute("SELECT id, query FROM memories WHERE author = ?", 
                                         (self.container_id,)) as master_cursor:
                master_memories = await master_cursor.fetchall()

            personal_queries = set(memory[1] for memory in personal_memories)
            for m_id, m_query in master_memories:
                if m_query not in personal_queries:
                    discrepancies.append({
                        "type": "missing_in_personal",
                        "master_id": m_id,
                        "query": m_query
                    })

        return discrepancies

    async def fix_discrepancies(self, discrepancies):
        for discrepancy in discrepancies:
            if discrepancy["type"] == "missing_in_master":
                await self._copy_memory_to_master(discrepancy["personal_id"])
            elif discrepancy["type"] == "missing_in_personal":
                await self._copy_memory_to_personal(discrepancy["master_id"])
            elif discrepancy["type"] == "content_mismatch":
                await self._resolve_content_mismatch(discrepancy)

    async def _copy_memory_to_master(self, personal_id):
        async with aiosqlite.connect(self.personal_db_path) as personal_db, \
                   aiosqlite.connect(self.master_db_path) as master_db:
            async with personal_db.execute("SELECT query, result, embedding FROM memories WHERE id = ?", 
                                           (personal_id,)) as cursor:
                memory = await cursor.fetchone()
            
            if memory:
                await master_db.execute("""
                    INSERT INTO memories (query, result, embedding, author)
                    VALUES (?, ?, ?, ?)
                """, (*memory, self.container_id))
                await master_db.commit()

    async def _copy_memory_to_personal(self, master_id):
        async with aiosqlite.connect(self.master_db_path) as master_db, \
                   aiosqlite.connect(self.personal_db_path) as personal_db:
            async with master_db.execute("SELECT query, result, embedding FROM memories WHERE id = ?", 
                                         (master_id,)) as cursor:
                memory = await cursor.fetchone()
            
            if memory:
                await personal_db.execute("""
                    INSERT INTO memories (query, result, embedding)
                    VALUES (?, ?, ?)
                """, memory)
                await personal_db.commit()

    async def _resolve_content_mismatch(self, discrepancy):
        # For this example, we'll always use the master version
        # In a real-world scenario, you might want a more sophisticated resolution strategy
        async with aiosqlite.connect(self.master_db_path) as master_db, \
                   aiosqlite.connect(self.personal_db_path) as personal_db:
            async with master_db.execute("SELECT result, embedding FROM memories WHERE id = ?", 
                                         (discrepancy["master_id"],)) as cursor:
                master_memory = await cursor.fetchone()
            
            if master_memory:
                await personal_db.execute("""
                    UPDATE memories
                    SET result = ?, embedding = ?
                    WHERE id = ?
                """, (*master_memory, discrepancy["personal_id"]))
                await personal_db.commit()

    async def run_consistency_check_and_fix(self):
        discrepancies = await self.perform_consistency_check()
        if discrepancies:
            logger.info(f"Found {len(discrepancies)} discrepancies. Fixing...")
            await self.fix_discrepancies(discrepancies)
            logger.info("Discrepancies fixed.")
        else:
            logger.info("No discrepancies found.")-e 
File: ./src/utils/__init__.py

-e 
File: ./src/utils/reset_database.py

# src/utils/reset_database.py

import sys
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Add the base directory of the project to the PYTHONPATH
base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(base_path)

from src.memory.memory_manager import MemoryManager
from config import Config  # Import the config

if __name__ == "__main__":
    memory_manager = MemoryManager(Config.MEMORY_DB_PATH, Config.OPENAI_API_KEY)
    memory_manager.reset_database()
    print("Database has been reset.")
-e 
File: ./src/utils/logging_setup.py

import logging
import os
import socket
from datetime import datetime

def setup_logging():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    container_id = socket.gethostname() if is_running_in_docker() else 'local'
    log_directory = f'logs/{timestamp}_{container_id}'

    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    master_logger = logging.getLogger('master')
    master_logger.setLevel(logging.DEBUG)
    file_handler = logging.FileHandler(os.path.join(log_directory, "master.log"))
    file_handler.setFormatter(log_formatter)
    master_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(message)s'))
    master_logger.addHandler(console_handler)

    chat_logger = logging.getLogger('chat')
    chat_logger.setLevel(logging.DEBUG)
    chat_file_handler = logging.FileHandler(os.path.join(log_directory, 'chat.log'))
    chat_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
    chat_logger.addHandler(chat_file_handler)

    memory_logger = logging.getLogger('memory')
    memory_logger.setLevel(logging.DEBUG)
    memory_file_handler = logging.FileHandler(os.path.join(log_directory, 'memory.log'))
    memory_file_handler.setFormatter(log_formatter)
    memory_logger.addHandler(memory_file_handler)

    database_logger = logging.getLogger('database')
    database_logger.setLevel(logging.DEBUG)
    database_file_handler = logging.FileHandler(os.path.join(log_directory, 'database.log'))
    database_file_handler.setFormatter(log_formatter)
    database_logger.addHandler(database_file_handler)

    master_logger.info(f"Logging setup complete. Log directory: {log_directory}")
    return master_logger, chat_logger, memory_logger, database_logger

def is_running_in_docker() -> bool:
    return os.path.exists('/.dockerenv')-e 
File: ./src/utils/controller.py

import logging
from typing import List, Tuple
from src.agents.agent import Agent
from src.memory.memory_manager import MemoryManager
from config import config

class Controller:
    def __init__(self):
        self.logger = logging.getLogger('master')
        self.db_logger = logging.getLogger('database')
        self.memory_manager = None
        self.agent = None

    async def initialize(self):
        self.memory_manager = MemoryManager(config.OPENAI_API_KEY)
        await self.memory_manager.initialize()
        self.agent = Agent(self.memory_manager)

    async def execute_query(self, query: str) -> str:
        try:
            response = await self.agent.process_query(query)
            self.logger.debug(f"Generated response: {response}")
            await self.memory_manager.save_memory(query, response)
            return response
        except Exception as e:
            self.logger.error(f"Error executing query '{query}': {str(e)}", exc_info=True)
            raise

    async def get_recent_memories(self, limit: int) -> List[Tuple[str, str]]:
        return await self.memory_manager.get_memories(limit)-e 
File: ./src/utils/enhanced_logging.py

import time
import logging
from functools import wraps

def log_execution_time(logger):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            result = await func(*args, **kwargs)
            end_time = time.time()
            execution_time = end_time - start_time
            logger.info(f"{func.__name__} executed in {execution_time:.4f} seconds")
            return result
        return wrapper
    return decorator

class DatabaseLogger:
    def __init__(self, logger):
        self.logger = logger
        file_handler = logging.FileHandler('database.log')
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        self.logger.addHandler(file_handler)

    def log_query(self, query, params=None):
        self.logger.debug(f"Executing query: {query}, Params: {params}")

    def log_memory_size(self, memory_size):
        self.logger.info(f"Memory size: {memory_size} bytes")

    def log_access(self, memory_id):
        self.logger.info(f"Accessed memory: {memory_id}")-e 
File: ./src/utils/memory_analysis.py

from collections import Counter
import numpy as np
import aiosqlite

class MemoryAnalyzer:
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager

    async def analyze_distribution(self):
        async with aiosqlite.connect(self.memory_manager.master_db_path) as db:
            async with db.execute("SELECT author, COUNT(*) FROM memories GROUP BY author") as cursor:
                distribution = await cursor.fetchall()
        return dict(distribution)

    async def find_most_accessed(self, limit=10):
        # Assuming we've added an 'access_count' column to the memories table
        async with aiosqlite.connect(self.memory_manager.master_db_path) as db:
            async with db.execute("SELECT id, query, access_count FROM memories ORDER BY access_count DESC LIMIT ?", (limit,)) as cursor:
                most_accessed = await cursor.fetchall()
        return most_accessed

    async def find_most_similar(self, query, limit=10):
        query_embedding = await self.memory_manager.embeddings.aembed_query(query)
        all_memories = await self.memory_manager.get_all_memories()
        
        similarities = []
        for memory in all_memories:
            memory_embedding = np.frombuffer(memory[2])
            similarity = np.dot(query_embedding, memory_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(memory_embedding))
            similarities.append((memory[0], memory[1], similarity))
        
        similarities.sort(key=lambda x: x[2], reverse=True)
        return similarities[:limit]-e 
File: ./src/api.py

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging
from config import config
from src.utils.memory_analysis import MemoryAnalyzer

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    response: str

# Initialize logging
master_logger, chat_logger, memory_logger, database_logger = setup_logging()

app = FastAPI()

controller = Controller()
memory_analyzer = None

@app.on_event("startup")
async def startup_event():
    global memory_analyzer
    master_logger.info("Starting up the API server")
    await controller.initialize()
    memory_analyzer = MemoryAnalyzer(controller.memory_manager)

@app.on_event("shutdown")
async def shutdown_event():
    master_logger.info("Shutting down the API server")

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    try:
        response = await controller.execute_query(request.query)
        return QueryResponse(response=response)
    except Exception as e:
        master_logger.error(f"Error processing query: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")

@app.get("/memory_distribution")
async def memory_distribution_endpoint():
    try:
        distribution = await memory_analyzer.analyze_distribution()
        return {"distribution": distribution}
    except Exception as e:
        master_logger.error(f"Error analyzing memory distribution: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error analyzing memory distribution: {str(e)}")
    
@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    master_logger.error(f"Unhandled exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": f"An unexpected error occurred: {str(exc)}"}
    )

@app.get("/consistency_check")
async def consistency_check_endpoint():
    try:
        await controller.memory_manager.run_consistency_check_and_fix()
        return {"message": "Consistency check and fix completed."}
    except Exception as e:
        master_logger.error(f"Error during consistency check: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error during consistency check: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    master_logger.info("Running the API server directly")
    uvicorn.run(app, host="0.0.0.0", port=8000)-e 
File: ./src/cli.py

import asyncio
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging

master_logger, chat_logger, memory_logger, database_logger = setup_logging()
controller = Controller()

async def initialize():
    await controller.initialize()

async def main():
    await initialize()
    while True:
        try:
            command = input("Enter a command (query/memories/consistency/quit): ").strip().lower()
            
            if command == 'quit':
                break
            elif command == 'query':
                query = input("Enter your query: ")
                response = await controller.execute_query(query)
                print(f"Response: {response}")
            elif command == 'memories':
                try:
                    limit = int(input("Enter the number of memories to retrieve: "))
                    memories = await controller.get_recent_memories(limit)
                    for i, (query, result) in enumerate(memories, 1):
                        print(f"{i}. Query: {query}\n   Result: {result}\n")
                except ValueError:
                    print("Invalid input. Please enter a number.")
            elif command == 'consistency':
                await controller.memory_manager.run_consistency_check_and_fix()
                print("Consistency check and fix completed.")
            else:
                print("Invalid command. Please try again.")
        except Exception as e:
            master_logger.error(f"An error occurred: {str(e)}", exc_info=True)
            print(f"An error occurred: {str(e)}. Please try again.")

if __name__ == "__main__":
    asyncio.run(main())-e 
File: ./docs/database_structure.md

# Database Structure and Operations

## Overview

The system uses two types of databases:
1. Personal Database: Stores memories specific to each container.
2. Master Database: Stores memories from all containers.

## Database Structure

Both personal and master databases have the following table structure:

### Memories Table

| Column    | Type    | Description                            |
|-----------|---------|----------------------------------------|
| id        | INTEGER | Primary key                            |
| query     | TEXT    | The original query                     |
| result    | TEXT    | The response or result                 |
| embedding | BLOB    | Vector representation of the query     |
| timestamp | DATETIME| When the memory was created/updated    |
| author    | TEXT    | (Master DB only) Container ID          |

## Querying Process

1. When a query is received, it's first converted to an embedding.
2. The system searches the personal database for similar memories.
3. If fewer than 3 relevant memories are found, it also searches the master database.
4. Memories are ranked by similarity and the top results are returned.

## Syncing Mechanism

- New memories are saved to both the personal and master databases.
- The master database includes an 'author' field to track which container created each memory.
- There's no automatic sync between personal and master databases after initial creation.

## Versioning and Change Tracking

(To be implemented)-e 
File: ./docs/README.md

# Multi-Agent RAG System with Continual Learning

## Project Overview

This project implements a vertical slice of a multi-agent RAG (Retrieval-Augmented Generation) system with continual learning capabilities. It uses LangChain, LangGraph, and LLM API calls to create a sophisticated question-answering system that learns from interactions.

## Key Features

- Multi-agent architecture for complex query processing
- RAG (Retrieval-Augmented Generation) for enhanced responses
- Continual learning to improve performance over time
- Distributed memory management with personal and master databases
- CLI and API interfaces for versatile interaction
- Comprehensive logging system for debugging and analysis
- Docker support for easy deployment and scaling

## System Architecture

The system consists of the following main components:

1. **Controller**: Orchestrates the overall flow of query processing.
2. **Agents**: Specialized modules for retrieval, processing, and response generation.
3. **Memory Manager**: Handles storage and retrieval of past interactions.
4. **Logging System**: Provides detailed logs for system operations, including database interactions.

## Directory Structure

```
.
├── Dockerfile
├── README.md
├── config.py
├── data/
├── docker-compose.yml
├── docs/
├── logs/
├── notebooks/
├── scripts/
├── src/
│   ├── agents/
│   ├── memory/
│   ├── utils/
│   ├── api.py
│   ├── app.py
│   └── cli.py
└── tests/
```

## Setup and Installation

### Prerequisites

- Python 3.12+
- Docker and Docker Compose

### Local Setup

1. Clone the repository:
   ```
   git clone https://github.com/your-username/multi-agent-rag.git
   cd multi-agent-rag
   ```

2. Create a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   Create a `.env` file in the root directory with the following content:
   ```
   OPENAI_API_KEY=your_openai_api_key
   TAVILY_API_KEY=your_tavily_api_key
   ```

### Docker Setup

1. Build the Docker image:
   ```
   docker-compose build
   ```

2. Run the container:
   ```
   docker-compose up
   ```

## Usage

### CLI Interface

Run the CLI interface with:

```
python src/cli.py
```

Available commands:
- `query`: Enter a query to process
- `memories`: Retrieve recent memories
- `consistency`: Run a consistency check on the databases
- `quit`: Exit the program

### API Interface

Start the API server with:

```
uvicorn src.api:app --host 0.0.0.0 --port 8000
```

API endpoints:
- POST `/query`: Submit a query for processing
- GET `/consistency_check`: Run a consistency check on the databases
- GET `/health`: Check the health status of the API
- GET `/memory_distribution`: Get the distribution of memories across containers

## Logging

Logs are stored in the `logs/` directory, organized by timestamp and container ID. There are separate log files for:

- Master log (`master.log`)
- Chat log (`chat.log`)
- Memory operations log (`memory.log`)
- Database operations log (`database.log`)

## Testing

Run the test suite with:

```
pytest tests/
```
-e 
File: ./.pytest_cache/README.md

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.
-e 
File: ./.pytest_cache/.gitignore

# Created by pytest automatically.
*
-e 
File: ./.pytest_cache/CACHEDIR.TAG

Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html
-e 
File: ./.pytest_cache/v/cache/nodeids

[]-e 
File: ./.pytest_cache/v/cache/lastfailed

{
  "tests/test_memory_manager.py": true
}-e 
File: ./.pytest_cache/v/cache/stepwise

[]