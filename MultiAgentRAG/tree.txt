.
├── Dockerfile
├── __pycache__
│   └── config.cpython-312.pyc
├── config.py
├── data
│   ├── be2117638dc8.db
│   ├── d09f2973073d.db
│   ├── d561de7f7631.db
│   ├── e49b6a1e67d7.db
│   └── fda22b21d614.db
├── docker-compose.yml
├── docker_requirements.txt
├── docs
│   └── README.md
├── logs
│   ├── 20240715_122843_27e301997f58
│   │   ├── all.log
│   │   ├── chat.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240715_124939_e49b6a1e67d7
│   │   ├── all.log
│   │   ├── chat.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240715_132323_fda22b21d614
│   │   ├── all.log
│   │   ├── chat.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240715_133049_d09f2973073d
│   │   ├── all.log
│   │   ├── chat.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240715_133643_be2117638dc8
│   │   ├── all.log
│   │   ├── chat.log
│   │   ├── master.log
│   │   └── memory.log
│   └── 20240715_134114_d561de7f7631
│       ├── all.log
│       ├── chat.log
│       ├── master.log
│       └── memory.log
├── scripts
│   ├── _docker_reset.sh
│   ├── generate_tree.sh
│   ├── logs_to_docs.sh
│   ├── manage_docker.sh
│   └── reset_logs.sh
├── src
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── __init__.cpython-312.pyc
│   │   ├── controller.cpython-310.pyc
│   │   └── controller.cpython-312.pyc
│   ├── agents
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-310.pyc
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   ├── processing_agent.cpython-312.pyc
│   │   │   ├── response_agent.cpython-312.pyc
│   │   │   ├── retrieval_agent.cpython-310.pyc
│   │   │   └── retrieval_agent.cpython-312.pyc
│   │   └── agent.py
│   ├── api.py
│   ├── cli.py
│   ├── memory
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   └── memory_manager.cpython-312.pyc
│   │   └── memory_manager.py
│   └── utils
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-312.pyc
│       │   ├── data_utils.cpython-312.pyc
│       │   └── json_utils.cpython-312.pyc
│       ├── controller.py
│       ├── enhanced_logging.py
│       ├── error_handling.py
│       ├── logging_setup.py
│       ├── structured_logging.py
│       ├── tracing.py
│       └── visualizer.py
└── tree.txt

19 directories, 70 files

Note: The following list of files and directories are excluded only from the appended file contents section:

__pycache__, .git, .env, text_docs, scripts, logs, tree.txt, generate_tree.sh, README, *.ipynb, *.pdf, *.db

File Contents:

-e 
File: ./.dockerignore

__pycache__
*.pyc
*.pyo
*.pyd
*.pdb
*.egg
*.egg-info
dist
build
eggs
parts
var
develop-eggs
.installed.cfg
lib
lib64
*.log
logs
notebooks
data/raw/*
text_docs/*
.DS_Store
.vscode
-e 
File: ./config.py

import os
from dotenv import load_dotenv

class Config:
    def __init__(self):
        load_dotenv()
        
        # General settings
        # self.MODEL_NAME = os.getenv("MODEL_NAME", "llama3-70b-8192") #! groq
        self.MODEL_NAME = os.getenv("MODEL_NAME", "meta-llama/Llama-3-70b-chat-hf") #? Together AI
        
        # Database settings
        self.personal_db_path = self._get_personal_db_path()
        
        # API Keys
        self.OPENAI_API_KEY = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
        self.TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
        self.GROQ_API_KEY = os.getenv("GROQ_API_KEY")
        self.TOGETHER_API_KEY = os.getenv('TOGETHER_API_KEY')
        
        # Other settings
        self.LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG")
        
        # Processing agent settings
        self.PROCESSING_AGENT_MEMORIES_INCLUDED = int(os.getenv("PROCESSING_AGENT_MEMORIES_INCLUDED", "5"))
        self.MEMORY_RETRIEVAL_THRESHOLD = float(os.getenv("MEMORY_RETRIEVAL_THRESHOLD", "0.75"))
        self.MEMORY_RETRIEVAL_LIMIT = int(os.getenv("MEMORY_RETRIEVAL_LIMIT", "10"))

    def _get_memory_db_path(self):
        return "/app/memory.db" if os.path.exists("/.dockerenv") else "memory.db"
    
    def _get_personal_db_path(self):
        container_id = os.environ.get('HOSTNAME', 'local')
        return f"/app/data/{container_id}.db" if os.path.exists("/.dockerenv") else f"{container_id}.db"

    def validate_api_keys(self):
        return bool(self.GROQ_API_KEY and self.TAVILY_API_KEY)

    L2_NORM_THRESHOLD = float(os.getenv("L2_NORM_THRESHOLD", "0.75"))
    COSINE_SIMILARITY_THRESHOLD = float(os.getenv("COSINE_SIMILARITY_THRESHOLD", "0.75"))
    BM25_THRESHOLD = float(os.getenv("BM25_THRESHOLD", "0.5"))
    JACCARD_SIMILARITY_THRESHOLD = float(os.getenv("JACCARD_SIMILARITY_THRESHOLD", "0.3"))

config = Config()-e 
File: ./docker-compose.yml

services:

  loki:
    image: grafana/loki:2.8.0
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./loki-config.yaml:/etc/loki/local-config.yaml

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    volumes:
      - ./grafana-provisioning:/etc/grafana/provisioning

  promtail:
    image: grafana/promtail:2.8.0
    volumes:
      - ./logs:/var/log
      - ./promtail-config.yaml:/etc/promtail/config.yaml
    command: -config.file=/etc/promtail/config.yaml
    
  multi-agent-rag:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    ports:
      - "8080:8000"
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GOODAI_OPENAI_API_KEY_LTM01=${GOODAI_OPENAI_API_KEY_LTM01}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    command: ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]

  multi-agent-rag-cli:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GOODAI_OPENAI_API_KEY_LTM01=${GOODAI_OPENAI_API_KEY_LTM01}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./local_logs:/app/local_logs
    command: ["python", "-m", "src.cli"]
    stdin_open: true
    tty: true-e 
File: ./Dockerfile

FROM python:3.12

# Create a non-root user
RUN useradd -m appuser

WORKDIR /app

COPY . /app

RUN mkdir -p /app/logs /app/data
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Create and activate virtual environment
RUN python -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Upgrade pip and install dependencies with increased timeout and using a mirror
RUN pip install --upgrade pip

# Install numpy and scipy separately with increased timeout
RUN pip install --no-cache-dir numpy==1.26.4 scipy==1.14.0 --timeout 600

# Install scikit-learn separately with an even longer timeout
RUN pip install --no-cache-dir scikit-learn==1.4.0 --timeout 900

# Install other dependencies
RUN pip install --no-cache-dir -r docker_requirements.txt --timeout 600 --index-url https://pypi.org/simple --trusted-host pypi.org

RUN pip install fastapi uvicorn

RUN pip install networkx matplotlib

RUN pip install structlog==24.1.0

RUN pip install structlog aiologger

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "-u", "src/utils/log_sync.py", "uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]-e 
File: ./docker_requirements.txt

langchain==0.2.3
langchain-openai==0.1.8
langchain-community==0.2.4
langchain-core==0.2.5
langchain-text-splitters==0.2.1
langsmith==0.1.77
openai==1.34.0
pydantic==2.7.4
pydantic-core==2.18.4
dataclasses-json==0.6.7
python-dotenv==1.0.1
aiosqlite
rank_bm25==0.2.2
cachetools==5.3.0
fastapi
uvicorn
opentelemetry-api==1.22.0
opentelemetry-sdk==1.22.0
opentelemetry-instrumentation-fastapi==0.43b0
aiologger==0.7.0
aiofiles
together-e 
File: ./docs/README.md

# Multi-Agent RAG System with Continual Learning

## Project Overview

This project implements a vertical slice of a multi-agent RAG (Retrieval-Augmented Generation) system with continual learning capabilities. It uses LangChain, LangGraph, and LLM API calls to create a sophisticated question-answering system that learns from interactions.

## Key Features

- Multi-agent architecture for complex query processing
- RAG (Retrieval-Augmented Generation) for enhanced responses
- Continual learning to improve performance over time
- Distributed memory management with personal and master databases
- CLI and API interfaces for versatile interaction
- Comprehensive logging system for debugging and analysis
- Docker support for easy deployment and scaling

## System Architecture

The system consists of the following main components:

1. **Controller**: Orchestrates the overall flow of query processing.
2. **Agents**: Specialized modules for retrieval, processing, and response generation.
3. **Memory Manager**: Handles storage and retrieval of past interactions.
4. **Logging System**: Provides detailed logs for system operations, including database interactions.

## Directory Structure

```
.
├── Dockerfile
├── README.md
├── config.py
├── data/
├── docker-compose.yml
├── docs/
├── logs/
├── notebooks/
├── scripts/
├── src/
│   ├── agents/
│   ├── memory/
│   ├── utils/
│   ├── api.py
│   ├── app.py
│   └── cli.py
└── tests/
```

## Setup and Installation

### Prerequisites

- Python 3.12+
- Docker and Docker Compose

### Local Setup

1. Clone the repository:
   ```
   git clone https://github.com/your-username/multi-agent-rag.git
   cd multi-agent-rag
   ```

2. Create a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   Create a `.env` file in the root directory with the following content:
   ```
   OPENAI_API_KEY=your_openai_api_key
   TAVILY_API_KEY=your_tavily_api_key
   ```

### Docker Setup

1. Build the Docker image:
   ```
   docker-compose build
   ```

2. Run the container:
   ```
   docker-compose up
   ```

## Usage

### CLI Interface

Run the CLI interface with:

```
python src/cli.py
```

Available commands:
- `query`: Enter a query to process
- `memories`: Retrieve recent memories
- `consistency`: Run a consistency check on the databases
- `quit`: Exit the program

### API Interface

Start the API server with:

```
uvicorn src.api:app --host 0.0.0.0 --port 8000
```

API endpoints:
- POST `/query`: Submit a query for processing
- GET `/consistency_check`: Run a consistency check on the databases
- GET `/health`: Check the health status of the API
- GET `/memory_distribution`: Get the distribution of memories across containers

## Logging

Logs are stored in the `logs/` directory, organized by timestamp and container ID. There are separate log files for:

- Master log (`master.log`)
- Chat log (`chat.log`)
- Memory operations log (`memory.log`)
- Database operations log (`database.log`)

## Testing

Run the test suite with:

```
pytest tests/
```
-e 
File: ./src/agents/agent.py

import json
# from groq import Groq # type: ignore #! groq version
from together import Together  # Import the Together SDK
from config import config
import asyncio

from src.utils.structured_logging import get_logger
from src.utils.enhanced_logging import log_execution_time
from src.utils.error_handling import log_error_with_traceback

class Agent:
    # def __init__(self, memory_manager, groq_api_key: str): #! groq version
    def __init__(self, memory_manager, together_api_key: str):
        self.logger = get_logger('chat')
        self.memory_manager = memory_manager
        # self.groq_client = Groq(api_key=groq_api_key) #! groq version
        self.together_client = Together(api_key=together_api_key)
        
    @log_execution_time
    async def process_query(self, query: str) -> str:
        try:
            self.logger.info("Processing query", extra={"query": query})
            relevant_memories = await self.memory_manager.retrieve_relevant_memories(query)
            
            messages = [
                {
                    "role": "system",
                    "content": """You are a highly efficient, no-nonsense administrator at the core of a sophisticated memory system. Your primary role is to analyze retrieved memories and provide concise, relevant summaries or indicate their lack of relevance. You value brevity and directness above all else.

        The retrieved memories you will be processing are structured as follows:

        {
        "Relevant Injected Content": {
            "UID": {
            "Time": "timestamp",
            "Query": "previous query",
            "Response": "previous response",
            "Similarity": float value,
            "linked_UIDs": ["list of linked memory UIDs"]
            },
            ...
        }
        }

        Analyze these memories in relation to the current query efficiently, paying special attention to linked memories."""
                },
                {
                    "role": "user",
                    "content": f"""QUERY: {query}

        RETRIEVED MEMORIES:
        {relevant_memories}

        Your task:

        1. Assess if the retrieved memories contain ANY relevant information to the query.
        2. If NO relevant information exists, respond ONLY with: "NO RELEVANT INFORMATION"
        3. If relevant information exists, assess, making an internal summary of ONLY the most pertinent points.
        4. Pay special attention to linked memories and their relevance to the current query.
        5. After summarizing, provide a concise response to the original query based on the relevant information.

        Imagine your internal summary in the following format:
        SUMMARY:
        • Point 1
        • Point 2
        ...

        RESPONSE BASED ON INTERNAL SUMMARY:
        [Your concise response to the original query]
    Note - only respond with the terse response appropriate to the query, provide none of the summary information that is for your eyes only."""
                }
            ]

            # Log the full request
            self.logger.info("LLM Request", extra={"messages": json.dumps(messages, indent=2)})

            # Use asyncio.to_thread to run the synchronous API call in a separate thread
            response = await asyncio.to_thread(
                self.together_client.chat.completions.create,  # Use Together API method
                messages=messages,
                model=config.MODEL_NAME,
            )

            # Log the full response
            self.logger.info("LLM Response", extra={"response": json.dumps(response.model_dump(), indent=2)})

            self.logger.info("Query processed", extra={"query": query})
            return response.choices[0].message.content
        except Exception as e:
            log_error_with_traceback(self.logger, "Error processing query", e)
            raise
-e 
File: ./src/agents/__init__.py

-e 
File: ./src/api.py

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging
from src.utils.tracing import setup_tracing, instrument_fastapi, tracer
from config import config
from src.utils.error_handling import global_exception_handler
from src.utils.structured_logging import get_logger
from src.utils.tracing import setup_tracing, instrument_fastapi, tracer

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    response: str

# Initialize logging
loggers = setup_logging()
logger = get_logger('master')

app = FastAPI()
app.add_exception_handler(Exception, global_exception_handler)
setup_tracing()
instrument_fastapi(app)
memory_analyzer = None

controller = None

@app.on_event("startup")
async def startup_event():
    global controller
    logger.info("Starting up the API server")
    controller = Controller()
    await controller.initialize()
    logger.info("API server initialization complete")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("Shutting down the API server")

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    with tracer.start_as_current_span("query_processing"):
        try:
            logger.info("Received query", extra={"query": request.query})
            if controller is None or controller.agent is None:
                raise ValueError("Controller or Agent not initialized")
            response = await controller.execute_query(request.query)
            logger.info("Query processed successfully", extra={"query": request.query})
            return QueryResponse(response=response)
        except Exception as e:
            logger.error("Error processing query", extra={"query": request.query, "error": str(e)})
            raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")                     

@app.get("/memory_stats")
async def memory_stats_endpoint():
    try:
        stats = await controller.memory_manager.get_memory_stats()
        return {"stats": stats}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/link_distribution")
async def link_distribution_endpoint():
    try:
        distribution = await controller.memory_manager.analyze_link_distribution()
        return {"distribution": distribution}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/visualize_network")
async def visualize_network_endpoint():
    try:
        await controller.memory_manager.visualize_network()
        return {"message": "Network visualization generated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": f"An unexpected error occurred: {str(exc)}"}
    )

if __name__ == "__main__":
    import uvicorn
    logger.info("Running the API server directly")
    uvicorn.run(app, host="0.0.0.0", port=8000)-e 
File: ./src/cli.py

import asyncio
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging
from src.utils.structured_logging import get_logger

loggers = setup_logging()
logger = get_logger('master')

controller = Controller()
similarity_analyzer = None

async def initialize():
    await controller.initialize()
    global similarity_analyzer

async def main():
    await initialize()
    while True:
        try:
            command = input("Enter a command (query/stats/distribution/visualize/quit): ").strip().lower()
            logger.info("Received command", extra={"command": command})
            
            if command == 'quit':
                logger.info("Exiting CLI")
                break
            elif command == 'query':
                query = input("Enter your query: ")
                logger.info("Processing query", extra={"query": query})
                response = await controller.execute_query(query)
                print(f"Response: {response}")
            elif command == 'stats':
                stats = await controller.memory_manager.get_memory_stats()
                print("Memory Statistics:")
                for key, value in stats.items():
                    print(f"{key}: {value}")
            elif command == 'distribution':
                distribution = await controller.memory_manager.analyze_link_distribution()
                print("Link Type Distribution:")
                for link_type, count in distribution.items():
                    print(f"{link_type}: {count}")
            elif command == 'visualize':
                await controller.memory_manager.visualize_network()
                print("Network visualization generated and saved as 'memory_network.png'")
            else:
                print("Invalid command. Please try again.")
        except Exception as e:
            logger.error("Error processing command", extra={"command": command, "error": str(e)})
            print(f"An error occurred: {str(e)}. Please try again.")

if __name__ == "__main__":
    asyncio.run(main())-e 
File: ./src/memory/memory_manager.py

import aiosqlite # type: ignore
from typing import List, Tuple, Union, Dict, Optional
from langchain_openai import OpenAIEmbeddings
import numpy as np
from config import Config
import os
from src.utils.enhanced_logging import log_execution_time
from src.utils.visualizer import visualize_memory_network
from sklearn.feature_extraction.text import TfidfVectorizer
from rank_bm25 import BM25Okapi # type: ignore
from datetime import datetime
import time
from scipy.spatial.distance import cosine
from src.utils.structured_logging import get_logger
from src.utils.error_handling import log_error_with_traceback
import json

class MemoryManager:
    def __init__(self, openai_api_key: str):
        self.logger = get_logger('memory')
        self.config = Config()
        self.personal_db_path = self.get_personal_db_path()
        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        self.container_id = self.get_container_id()
        self.tfidf_vectorizer: Optional[TfidfVectorizer] = None
        self.bm25: Optional[BM25Okapi] = None
        self.corpus: List[str] = []

    async def initialize(self):
        os.makedirs("/app/data", exist_ok=True)
        await self.create_tables(self.personal_db_path)
        await self._load_corpus()
        await self._update_indexing()

    def get_container_id(self):
        return os.environ.get('HOSTNAME', 'local')

    def get_personal_db_path(self):
        container_id = self.get_container_id()
        return f"/app/data/{container_id}.db"

    async def create_tables(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS memories (
                    id INTEGER PRIMARY KEY,
                    query TEXT NOT NULL,
                    result TEXT NOT NULL,
                    embedding BLOB NOT NULL,
                    embedding_shape TEXT NOT NULL,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Create memory_links table
            await db.execute("""
                CREATE TABLE IF NOT EXISTS memory_links (
                    id INTEGER PRIMARY KEY,
                    source_memory_id INTEGER,
                    target_memory_id INTEGER,
                    link_type TEXT,
                    FOREIGN KEY (source_memory_id) REFERENCES memories (id),
                    FOREIGN KEY (target_memory_id) REFERENCES memories (id)
                )
            """)
            await db.commit()
        self.logger.debug(f"Ensured memories and memory_links tables exist in {db_path}")

    @log_execution_time
    async def create_memory_link(self, source_id: int, target_id: int, link_type: str):
        async with aiosqlite.connect(self.personal_db_path) as db:
            await db.execute("""
                INSERT INTO memory_links (source_memory_id, target_memory_id, link_type)
                VALUES (?, ?, ?)
            """, (source_id, target_id, link_type))
            await db.commit()

    @log_execution_time
    async def get_linked_memories(self, memory_id: int) -> List[Tuple[int, str]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("""
                SELECT target_memory_id, link_type FROM memory_links
                WHERE source_memory_id = ?
            """, (memory_id,)) as cursor:
                return await cursor.fetchall()


    @log_execution_time
    async def save_memory(self, query: str, result: str):
        try:
            embedding = await self.embeddings.aembed_query(query)
            
            # Convert the list to a numpy array
            embedding_array = np.array(embedding, dtype=np.float32)
            embedding_shape = embedding_array.shape
            embedding_bytes = embedding_array.tobytes()
            
            async with aiosqlite.connect(self.personal_db_path) as db:
                cursor = await db.execute("""
                    INSERT INTO memories (query, result, embedding, embedding_shape) 
                    VALUES (?, ?, ?, ?)
                """, (query, result, embedding_bytes, str(embedding_shape)))
                memory_id = cursor.lastrowid
                await db.commit()
            
            # Remove this line: self.db_logger.log_access(memory_id)
            
            self.logger.debug(f"Saved memory for query: {query} with result: {result}")
            
            # Update corpus and indexing
            await self._load_corpus()
            await self._update_indexing()

            # Call auto_link_memories with the new memory_id
            await self.auto_link_memories(memory_id)
            
            self.logger.info("Memory saved", extra={"query": query, "memory_id": memory_id})
        except Exception as e:
            log_error_with_traceback(self.logger, "Error retrieving relevant memories", e)
            raise

    async def _load_corpus(self):
        start_time = time.time()
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT query, result FROM memories") as cursor:
                memories = await cursor.fetchall()
        self.corpus = [f"{query} {result}" for query, result in memories]
        self.logger.info(f"Loaded corpus with {len(self.corpus)} entries in {time.time() - start_time:.2f} seconds")
    
    @log_execution_time
    async def _update_indexing(self):
        if self.corpus:
            start_time = time.time()
            self.tfidf_vectorizer = TfidfVectorizer()
            self.tfidf_vectorizer.fit(self.corpus)
            tokenized_corpus = [doc.split() for doc in self.corpus]
            self.bm25 = BM25Okapi(tokenized_corpus)
            self.logger.info(f"Updated indexing with {len(self.corpus)} documents in {time.time() - start_time:.2f} seconds")
        else:
            self.logger.warning("Corpus is empty. Skipping indexing update.")


    @log_execution_time
    async def retrieve_relevant_memories(self, query: str, threshold: float = 0.75) -> str:
        query_embedding = await self.embeddings.aembed_query(query)
        
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT id, query, result, embedding, timestamp FROM memories") as cursor:
                all_memories = await cursor.fetchall()
        
        relevant_memories = self._calculate_cosine_similarity(all_memories, query_embedding, threshold)
        
        structured_memories = {}
        for memory in relevant_memories:
            memory_id, query, result, similarity, timestamp = memory
            linked_memories = await self.get_linked_memories(int(memory_id))
            structured_memories[str(memory_id)] = {
                "Time": timestamp.isoformat() if isinstance(timestamp, datetime) else timestamp,
                "Query": query,
                "Response": result,
                "Similarity": similarity,
                "linked_UIDs": [str(linked_id) for linked_id, _ in linked_memories]
            }
        
        # Fetch details of linked memories
        for memory_id, memory_data in structured_memories.items():
            for linked_id in memory_data["linked_UIDs"]:
                if linked_id not in structured_memories:
                    linked_memory = await self.get_memory(int(linked_id))
                    structured_memories[linked_id] = {
                        "Time": linked_memory['timestamp'].isoformat() if isinstance(linked_memory['timestamp'], datetime) else linked_memory['timestamp'],
                        "Query": linked_memory['query'],
                        "Response": linked_memory['result'],
                        "Similarity": 1.0,  # Linked memories are considered fully relevant
                        "linked_UIDs": []  # We don't fetch nested links to avoid potential infinite recursion
                    }
        
        self.logger.info("Retrieved relevant memories", 
                         extra={
                             "query": query, 
                             "total_memories": len(structured_memories),
                             "linked_memories": sum(len(m["linked_UIDs"]) for m in structured_memories.values())
                         })
        
        return json.dumps({"Relevant Injected Content": structured_memories}, indent=2)


    def _calculate_l2_norm(self, memories: List[Tuple], query_embedding: np.ndarray, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        return [
            (str(memory[0]), memory[1], memory[2], np.linalg.norm(query_embedding - np.frombuffer(memory[3])), memory[4])
            for memory in memories
            if np.linalg.norm(query_embedding - np.frombuffer(memory[3])) <= threshold
        ]

    def _calculate_cosine_similarity(self, memories: List[Tuple], query_embedding: Union[List[float], np.ndarray], threshold: float) -> List[Tuple[int, str, str, float, datetime]]:
        try:
            results = []
        
            # Ensure query_embedding is a numpy array
            if isinstance(query_embedding, list):
                query_embedding = np.array(query_embedding, dtype=np.float32)
            
            for memory in memories:
                memory_id, query, result, embedding_bytes, timestamp = memory
                memory_embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
                
                # Reshape memory_embedding if necessary
                if memory_embedding.shape != query_embedding.shape:
                    self.logger.warning(f"Reshaping embedding for memory {memory_id}: {memory_embedding.shape} to {query_embedding.shape}")
                    try:
                        memory_embedding = memory_embedding.reshape(query_embedding.shape)
                    except ValueError as v:
                        log_error_with_traceback(self.logger, "Cannot reshape embedding for memory {memory_id}. Skipping.", v)
                        continue
                
                # Compute cosine similarity
                similarity = np.dot(query_embedding, memory_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(memory_embedding))
                
                if similarity >= threshold:
                    results.append((memory_id, query, result, float(similarity), timestamp))
 
            return results
        except Exception as e:
            log_error_with_traceback(self.logger, "Error calculating cosine similarity", e)
            raise

    def _calculate_bm25(self, memories: List[Tuple], query: str, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)
        max_score = np.max(scores) if scores.size > 0 else 1
        normalized_scores = scores / max_score

        return [
            (str(memory[0]), memory[1], memory[2], score, memory[4])
            for memory, score in zip(memories, normalized_scores)
            if score >= threshold
        ]
    
    def _calculate_jaccard_similarity(self, memories: List[Tuple], query: str, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        query_set = set(query.lower().split())
        return [
            (str(memory[0]), memory[1], memory[2],
             len(set(memory[1].lower().split()) & query_set) / len(set(memory[1].lower().split()) | query_set),
             memory[4])
            for memory in memories
            if len(set(memory[1].lower().split()) & query_set) / len(set(memory[1].lower().split()) | query_set) >= threshold
        ]

    def _format_memories(self, all_memories: Dict[str, List[Tuple[str, str, str, float, str]]]) -> str:
        formatted_output = []
        seen_memories = set()

        for metric, memories in all_memories.items():
            if not memories:
                continue

            formatted_output.append(f"Similar by {metric} (ordered by timestamp - ascending):")
            sorted_memories = sorted(memories, key=lambda x: datetime.fromisoformat(x[4]))

            for memory in sorted_memories:
                memory_id, query, result, score, timestamp = memory
                if memory_id not in seen_memories:
                    formatted_output.append(f"<{memory_id}>, <{query}>, <{result}>, <{timestamp}>, <{score:.2f}>")
                    seen_memories.add(memory_id)
                else:
                    formatted_output.append(f"<{memory_id}>, <{score:.2f}>")

        return " ".join(formatted_output)

    async def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("""
                SELECT query, result FROM memories ORDER BY timestamp DESC LIMIT ?
            """, (limit,)) as cursor:
                memories = await cursor.fetchall()
        self.logger.debug(f"Retrieved {len(memories)} memories")
        return memories

    async def get_all_memories(self) -> List[Dict]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT id, query, result, embedding FROM memories") as cursor:
                rows = await cursor.fetchall()
                return [{
                    'id': row[0],
                    'query': row[1],
                    'result': row[2],
                    'embedding': np.frombuffer(row[3], dtype=np.float32)
                } for row in rows]

    async def _add_to_personal_db(self, memory: Tuple):
            query, result, embedding = memory
            async with aiosqlite.connect(self.personal_db_path) as db:
                await db.execute("""
                    INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)
                """, (query, result, embedding))
                await db.commit()
            self.logger.info(f"Added missing memory to personal DB: {query}")

    async def analyze_memory_distribution(self) -> Dict[str, int]:
        """
        Analyze the distribution of memories across different similarity metrics.
        """
        distribution = {
            "L2 norm": 0,
            "Cosine Similarity": 0,
            "BM25": 0,
            "Jaccard Similarity": 0
        }

        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT query, result FROM memories") as cursor:
                memories = await cursor.fetchall()

        for memory in memories:
            query, result = memory
            _, metadata = await self.retrieve_relevant_memories(query, return_metadata=True)
            distribution["L2 norm"] += metadata["l2_count"]
            distribution["Cosine Similarity"] += metadata["cosine_count"]
            distribution["BM25"] += metadata["bm25_count"]
            distribution["Jaccard Similarity"] += metadata["jaccard_count"]

        total = sum(distribution.values())
        for key in distribution:
            distribution[key] = round((distribution[key] / total) * 100, 2) if total > 0 else 0

        return distribution

    async def get_memory_stats(self) -> Dict[str, Union[int, float]]:
        """
        Get statistics about the current state of memories.
        """
        try:
            async with aiosqlite.connect(self.personal_db_path) as db:
                async with db.execute("SELECT COUNT(*) FROM memories") as cursor:
                    (total_memories,) = await cursor.fetchone()
                
                async with db.execute("SELECT MIN(timestamp), MAX(timestamp) FROM memories") as cursor:
                    (oldest_memory, newest_memory) = await cursor.fetchone()

            distribution = await self.analyze_memory_distribution()

            return {
                "total_memories": total_memories,
                "oldest_memory": oldest_memory,
                "newest_memory": newest_memory,
                "distribution": distribution
            }
        except Exception as e:
            log_error_with_traceback(self.logger, "Error getting memory stats: {str(e)}", e)
            raise
        
    @log_execution_time
    async def auto_link_memories(self, new_memory_id: int, threshold: float = 0.8):
        new_memory = await self.get_memory(new_memory_id)
        all_memories = await self.get_all_memories()
        
        for memory in all_memories:
            if memory['id'] != new_memory_id:
                embedding_similarity = self._calculate_embedding_similarity(new_memory['embedding'], memory['embedding'])
                keyword_similarity = self._calculate_keyword_similarity(new_memory['query'], memory['query'])
                
                combined_similarity = (embedding_similarity + keyword_similarity) / 2
                
                if combined_similarity >= threshold:
                    await self.create_memory_link(new_memory_id, memory['id'], "auto_similar")
                    await self.create_memory_link(memory['id'], new_memory_id, "auto_similar")

    async def get_memory(self, memory_id: int) -> Dict:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT id, query, result, embedding, timestamp FROM memories WHERE id = ?", (memory_id,)) as cursor:
                row = await cursor.fetchone()
                return {
                    'id': row[0],
                    'query': row[1],
                    'result': row[2],
                    'embedding': np.frombuffer(row[3], dtype=np.float32),
                    'timestamp': row[4]
                }

    async def get_all_memories(self) -> List[Dict]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT id, query, result, embedding FROM memories") as cursor:
                rows = await cursor.fetchall()
                return [{
                    'id': row[0],
                    'query': row[1],
                    'result': row[2],
                    'embedding': np.frombuffer(row[3], dtype=np.float32)
                } for row in rows]

    def _calculate_embedding_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        return 1 - cosine(emb1, emb2)

    def _calculate_keyword_similarity(self, query1: str, query2: str) -> float:
        tokens1 = set(query1.lower().split())
        tokens2 = set(query2.lower().split())
        return len(tokens1 & tokens2) / len(tokens1 | tokens2)
    
    async def get_memory_stats(self) -> Dict[str, Union[int, float]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT COUNT(*) FROM memories") as cursor:
                total_memories = (await cursor.fetchone())[0]
            async with db.execute("SELECT COUNT(*) FROM memory_links") as cursor:
                total_links = (await cursor.fetchone())[0]
            async with db.execute("SELECT AVG(link_count) FROM (SELECT COUNT(*) as link_count FROM memory_links GROUP BY source_memory_id)") as cursor:
                avg_links_per_memory = (await cursor.fetchone())[0]
        
        return {
            "total_memories": total_memories,
            "total_links": total_links,
            "avg_links_per_memory": avg_links_per_memory
        }
    
    async def visualize_network(self):
        memories = await self.get_all_memories()
        links = await self.get_all_links()
        
        memory_tuples = [(m['id'], m['query'], m['result']) for m in memories]
        visualize_memory_network(memory_tuples, links)

    async def get_all_links(self) -> List[Tuple[int, int, str]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT source_memory_id, target_memory_id, link_type FROM memory_links") as cursor:
                return await cursor.fetchall()-e 
File: ./src/memory/__init__.py

-e 
File: ./src/utils/controller.py

from typing import List, Tuple
from src.agents.agent import Agent
from src.memory.memory_manager import MemoryManager
from config import config
from src.utils.structured_logging import get_logger
from src.utils.error_handling import log_error_with_traceback
import os

class Controller:
    def __init__(self):
        self.logger = get_logger('master')
        self.memory_manager = None
        self.agent = None

    async def initialize(self):
        container_id = os.environ.get('HOSTNAME', 'local')
        self.memory_manager = MemoryManager(config.OPENAI_API_KEY)  # Use OpenAI API key for embeddings
        await self.memory_manager.initialize()
        # self.agent = Agent(self.memory_manager, config.GROQ_API_KEY)  # Pass Groq API key to Agent
        self.agent = Agent(self.memory_manager, config.TOGETHER_API_KEY)  #? Together version

    async def execute_query(self, query: str) -> str:
        try:
            self.logger.info("Executing query", extra={"query": query})
            response = await self.agent.process_query(query)
            self.logger.info("Query executed successfully", extra={"query": query})
            await self.memory_manager.save_memory(query, response)
            return response
        except Exception as e:
            log_error_with_traceback(self.logger, "Error executing query", e)
            return f"An error occurred while processing your query: {str(e)}"

    async def get_recent_memories(self, limit: int) -> List[Tuple[str, str]]:
        return await self.memory_manager.get_memories(limit)-e 
File: ./src/utils/enhanced_logging.py

from functools import wraps
import time
import structlog

def log_execution_time(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        logger = structlog.get_logger(func.__module__)
        start_time = time.time()
        result = await func(*args, **kwargs)
        execution_time = time.time() - start_time
        logger.info(f"{func.__name__} executed",
                    execution_time=execution_time,
                    function=func.__name__)
        return result
    return wrapper-e 
File: ./src/utils/error_handling.py

import traceback
from fastapi import Request
from fastapi.responses import JSONResponse
from src.utils.structured_logging import get_logger

error_logger = get_logger("error")

async def global_exception_handler(request: Request, exc: Exception):
    error_logger.error("Unhandled exception", 
                       url=str(request.url),
                       method=request.method,
                       error=str(exc),
                       traceback=traceback.format_exc())
    return JSONResponse(
        status_code=500,
        content={"detail": f"An unexpected error occurred: {str(exc)}"}
    )

def log_error_with_traceback(logger, message, exc):
    logger.error(f"{message}: {str(exc)}", 
                 exc_info=True, 
                 extra={"traceback": traceback.format_exc()})-e 
File: ./src/utils/logging_setup.py

import logging
import os
import sys
from datetime import datetime
import structlog
from structlog.stdlib import LoggerFactory
from structlog.processors import JSONRenderer

def setup_logging():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    container_id = os.environ.get('HOSTNAME', 'local')
    log_directory = f'logs/{timestamp}_{container_id}'

    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    # Define a custom processor to handle 'extra' fields
    def add_extra_fields(logger, method_name, event_dict):
        extra = event_dict.pop('extra', {})
        event_dict.update(extra)
        return event_dict

    # Configure structlog
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            add_extra_fields,
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

    # Set up root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)

    # File handler for all logs
    all_file_handler = logging.FileHandler(os.path.join(log_directory, "all.log"))
    all_file_handler.setFormatter(logging.Formatter('%(message)s'))
    root_logger.addHandler(all_file_handler)

    # Console handler for all logs
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(logging.Formatter('%(message)s'))
    root_logger.addHandler(console_handler)

    # Specific loggers
    logger_names = ['master', 'chat', 'memory']
    loggers = {}
    for name in logger_names:
        logger = structlog.get_logger(name)
        file_handler = logging.FileHandler(os.path.join(log_directory, f"{name}.log"))
        file_handler.setFormatter(logging.Formatter('%(message)s'))
        logger.addHandler(file_handler)
        loggers[name] = logger

    logging.info(f"Logging setup complete. Log directory: {log_directory}")
    return loggers

def get_logger(name):
    return structlog.get_logger(name)-e 
File: ./src/utils/structured_logging.py

import structlog

def setup_structured_logging():
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
    
def get_logger(name):
    return structlog.get_logger(name)-e 
File: ./src/utils/tracing.py

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

def setup_tracing():
    trace.set_tracer_provider(TracerProvider())
    trace.get_tracer_provider().add_span_processor(
        BatchSpanProcessor(ConsoleSpanExporter())
    )

def instrument_fastapi(app):
    FastAPIInstrumentor.instrument_app(app)

tracer = trace.get_tracer(__name__)-e 
File: ./src/utils/visualizer.py

import networkx as nx
import matplotlib.pyplot as plt
from typing import List, Tuple

def visualize_memory_network(memories: List[Tuple[int, str, str]], links: List[Tuple[int, int, str]]):
    G = nx.Graph()
    
    # Add nodes
    for memory_id, query, _ in memories:
        G.add_node(memory_id, label=query[:20])  # Use first 20 characters of query as label
    
    # Add edges
    for source_id, target_id, link_type in links:
        G.add_edge(source_id, target_id, label=link_type)
    
    # Set up the plot
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G)
    
    # Draw nodes
    nx.draw_networkx_nodes(G, pos, node_size=700, node_color='lightblue')
    nx.draw_networkx_labels(G, pos, {node: data['label'] for node, data in G.nodes(data=True)})
    
    # Draw edges
    nx.draw_networkx_edges(G, pos)
    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
    
    plt.title("Memory Network Visualization")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig('memory_network.png')
    plt.close()

# Usage in MemoryManager:
# async def visualize_network(self):
#     memories = await self.get_all_memories()
#     links = await self.get_all_links()
#     visualize_memory_network([(m['id'], m['query'], m['result']) for m in memories], links)-e 
File: ./src/utils/__init__.py

-e 
File: ./src/__init__.py

