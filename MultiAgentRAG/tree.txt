.
├── Dockerfile
├── config.py
├── data
│   ├── 838622856d00.db
│   ├── 8aae7173af37.db
│   ├── e69ce7b49fea.db
│   └── master.db
├── docker-compose.yml
├── docker_requirements.txt
├── docs
│   └── README.md
├── logs
│   ├── 20240704_101957_9b54d4275e48
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240704_104434_b2d089a0ff0c
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240708_105442_4d43a30ff783
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240708_111940_92044deca7b2
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240708_113157_92929d0ea2fc
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240708_125536_838622856d00
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   ├── 20240709_092326_8aae7173af37
│   │   ├── chat.log
│   │   ├── database.log
│   │   ├── master.log
│   │   └── memory.log
│   └── 20240709_093333_e69ce7b49fea
│       ├── chat.log
│       ├── database.log
│       ├── master.log
│       └── memory.log
├── notebooks
│   └── Multi_Agent_RAG_LangGraph.ipynb
├── scripts
│   ├── _docker_reset.sh
│   ├── generate_tree.sh
│   ├── logs_to_docs.sh
│   ├── manage_docker.sh
│   └── reset_logs.sh
├── src
│   ├── __init__.py
│   ├── __pycache__
│   │   └── __init__.cpython-310.pyc
│   ├── agents
│   │   ├── __init__.py
│   │   └── agent.py
│   ├── api.py
│   ├── app.py
│   ├── cli.py
│   ├── memory
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-310.pyc
│   │   │   └── memory_manager.cpython-310.pyc
│   │   └── memory_manager.py
│   └── utils
│       ├── __init__.py
│       ├── controller.py
│       ├── enhanced_logging.py
│       └── logging_setup.py
└── tree.txt

19 directories, 63 files

Note: The following list of files and directories are excluded only from the appended file contents section:

__pycache__, .git, .env, text_docs, scripts, logs, tree.txt, generate_tree.sh, README, *.ipynb, *.pdf, *.db

File Contents:

-e 
File: ./src/memory/__init__.py

-e 
File: ./src/memory/memory_manager.py

import logging
import aiosqlite # type: ignore
from typing import List, Tuple, Union, Dict, Optional
from langchain_openai import OpenAIEmbeddings
import numpy as np
from config import Config
import os
from src.utils.enhanced_logging import log_execution_time, DatabaseLogger
from sklearn.feature_extraction.text import TfidfVectorizer
from rank_bm25 import BM25Okapi # type: ignore
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time
import json

logger = logging.getLogger('memory')

class MemoryManager:
    def __init__(self, api_key: str):
        self.config = Config()
        self.master_db_path = "/app/data/master.db"
        self.personal_db_path = self.get_personal_db_path()
        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        self.container_id = self.get_container_id()
        self.db_logger = DatabaseLogger(logging.getLogger('database'))
        self.tfidf_vectorizer: Optional[TfidfVectorizer] = None
        self.bm25: Optional[BM25Okapi] = None
        self.corpus: List[str] = []

    def get_container_id(self):
        return os.environ.get('HOSTNAME', 'local')

    def get_personal_db_path(self):
        container_id = self.get_container_id()
        return f"/app/data/{container_id}.db"

    async def initialize(self):
        await self.initialize_databases()
        await self._load_corpus()
        self._update_indexing()

    async def initialize_databases(self):
        os.makedirs("/app/data", exist_ok=True)
        await self.create_tables(self.master_db_path)
        await self.create_tables(self.personal_db_path)
        await self.create_changelog_table(self.master_db_path)
        await self.create_changelog_table(self.personal_db_path)

    async def create_tables(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            if db_path == self.master_db_path:
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS memories (
                        id INTEGER PRIMARY KEY,
                        query TEXT,
                        result TEXT,
                        embedding BLOB,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        author TEXT
                    )
                """)
            else:
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS memories (
                        id INTEGER PRIMARY KEY,
                        query TEXT,
                        result TEXT,
                        embedding BLOB,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                """)
            await db.commit()
        logger.debug(f"Ensured memories table exists in {db_path}")

    async def create_changelog_table(self, db_path):
        async with aiosqlite.connect(db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS changelog (
                    id INTEGER PRIMARY KEY,
                    operation TEXT,
                    memory_id INTEGER,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            await db.commit()

    async def log_change(self, db_path, operation, memory_id):
        async with aiosqlite.connect(db_path) as db:
            await db.execute("""
                INSERT INTO changelog (operation, memory_id) VALUES (?, ?)
            """, (operation, memory_id))
            await db.commit()

    @log_execution_time(logger)
    async def save_memory(self, query: str, result: str):
        try:
            embedding = np.array(await self.embeddings.aembed_query(query)).tobytes()
            
            async with aiosqlite.connect(self.personal_db_path) as db:
                self.db_logger.log_query("INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)")
                cursor = await db.execute("""
                    INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)
                """, (query, result, embedding))
                memory_id = cursor.lastrowid
                await db.commit()
            self.db_logger.log_access(memory_id)
            await self.log_change(self.personal_db_path, "INSERT", memory_id)
            
            async with aiosqlite.connect(self.master_db_path) as db:
                self.db_logger.log_query("INSERT INTO memories (query, result, embedding, author) VALUES (?, ?, ?, ?)")
                cursor = await db.execute("""
                    INSERT INTO memories (query, result, embedding, author) VALUES (?, ?, ?, ?)
                """, (query, result, embedding, self.container_id))
                memory_id = cursor.lastrowid
                await db.commit()
            self.db_logger.log_access(memory_id)
            await self.log_change(self.master_db_path, "INSERT", memory_id)
            
            logger.debug(f"Saved memory for query: {query} with result: {result}")
            
            # Update corpus and indexing
            await self._load_corpus()
            self._update_indexing()
        except Exception as e:
            logger.error(f"Error saving memory for query '{query}': {str(e)}", exc_info=True)
            raise

    async def _load_corpus(self):
        start_time = time.time()
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT query, result FROM memories") as cursor:
                memories = await cursor.fetchall()
        self.corpus = [f"{query} {result}" for query, result in memories]
        logger.info(f"Loaded corpus with {len(self.corpus)} entries in {time.time() - start_time:.2f} seconds")

    def _update_indexing(self):
        if self.corpus:
            start_time = time.time()
            self.tfidf_vectorizer = TfidfVectorizer()
            self.tfidf_vectorizer.fit(self.corpus)
            tokenized_corpus = [doc.split() for doc in self.corpus]
            self.bm25 = BM25Okapi(tokenized_corpus)
            logger.info(f"Updated indexing with {len(self.corpus)} documents in {time.time() - start_time:.2f} seconds")
        else:
            logger.warning("Corpus is empty. Skipping indexing update.")

    @log_execution_time(logger)
    async def retrieve_relevant_memories(self, query: str, threshold: float = 0.75, return_metadata: bool = False) -> Union[str, Dict]:
        if not self.corpus:
            logger.warning("Corpus is empty. No memories to retrieve.")
            return ""

        start_time = time.time()
        query_embedding = np.array(await self.embeddings.aembed_query(query))
        
        try:
            async with aiosqlite.connect(self.personal_db_path) as db:
                async with db.execute("SELECT rowid, query, result, embedding, timestamp FROM memories") as cursor:
                    all_memories = await cursor.fetchall()
        except aiosqlite.Error as e:
            logger.error(f"Database error: {e}")
            return ""

        logger.debug(f"Retrieved {len(all_memories)} memories from database")

        with ThreadPoolExecutor(max_workers=4) as executor:
            l2_future = executor.submit(self._calculate_l2_norm, all_memories, query_embedding, threshold)
            cosine_future = executor.submit(self._calculate_cosine_similarity, all_memories, query_embedding, threshold)
            bm25_future = executor.submit(self._calculate_bm25, all_memories, query, threshold)
            jaccard_future = executor.submit(self._calculate_jaccard_similarity, all_memories, query, threshold)

            l2_memories = l2_future.result()
            cosine_memories = cosine_future.result()
            bm25_memories = bm25_future.result()
            jaccard_memories = jaccard_future.result()

        logger.debug(f"L2 memories: {len(l2_memories)}, Cosine memories: {len(cosine_memories)}, BM25 memories: {len(bm25_memories)}, Jaccard memories: {len(jaccard_memories)}")

        all_memories = {
            'L2 norm': l2_memories,
            'Cosine Similarity': cosine_memories,
            'BM25': bm25_memories,
            'Jaccard Similarity': jaccard_memories
        }

        formatted_output = self._format_memories(all_memories)

        logger.info(f"Retrieved and processed memories in {time.time() - start_time:.2f} seconds")

        if return_metadata:
            metadata = {
                "l2_count": len(l2_memories),
                "cosine_count": len(cosine_memories),
                "bm25_count": len(bm25_memories),
                "jaccard_count": len(jaccard_memories),
                "total_memories": len(all_memories),
                "processing_time": time.time() - start_time
            }
            return formatted_output, metadata
        else:
            return formatted_output

    def _calculate_l2_norm(self, memories: List[Tuple], query_embedding: np.ndarray, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        return [
            (str(memory[0]), memory[1], memory[2], np.linalg.norm(query_embedding - np.frombuffer(memory[3])), memory[4])
            for memory in memories
            if np.linalg.norm(query_embedding - np.frombuffer(memory[3])) <= threshold
        ]

    def _calculate_cosine_similarity(self, memories: List[Tuple], query_embedding: np.ndarray, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        return [
            (str(memory[0]), memory[1], memory[2], 
             np.dot(query_embedding, np.frombuffer(memory[3])) / (np.linalg.norm(query_embedding) * np.linalg.norm(np.frombuffer(memory[3]))),
             memory[4])
            for memory in memories
            if np.dot(query_embedding, np.frombuffer(memory[3])) / (np.linalg.norm(query_embedding) * np.linalg.norm(np.frombuffer(memory[3]))) >= threshold
        ]

    def _calculate_bm25(self, memories: List[Tuple], query: str, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)
        max_score = np.max(scores) if scores.size > 0 else 1
        normalized_scores = scores / max_score

        return [
            (str(memory[0]), memory[1], memory[2], score, memory[4])
            for memory, score in zip(memories, normalized_scores)
            if score >= threshold
        ]
    
    def _calculate_jaccard_similarity(self, memories: List[Tuple], query: str, threshold: float) -> List[Tuple[str, str, str, float, str]]:
        query_set = set(query.lower().split())
        return [
            (str(memory[0]), memory[1], memory[2],
             len(set(memory[1].lower().split()) & query_set) / len(set(memory[1].lower().split()) | query_set),
             memory[4])
            for memory in memories
            if len(set(memory[1].lower().split()) & query_set) / len(set(memory[1].lower().split()) | query_set) >= threshold
        ]

    def _format_memories(self, all_memories: Dict[str, List[Tuple[str, str, str, float, str]]]) -> str:
        formatted_output = []
        seen_memories = set()

        for metric, memories in all_memories.items():
            if not memories:
                continue

            formatted_output.append(f"Similar by {metric} (ordered by timestamp - ascending):")
            sorted_memories = sorted(memories, key=lambda x: datetime.fromisoformat(x[4]))

            for memory in sorted_memories:
                memory_id, query, result, score, timestamp = memory
                if memory_id not in seen_memories:
                    formatted_output.append(f"<{memory_id}>, <{query}>, <{result}>, <{timestamp}>, <{score:.2f}>")
                    seen_memories.add(memory_id)
                else:
                    formatted_output.append(f"<{memory_id}>, <{score:.2f}>")

        return " ".join(formatted_output)

    async def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("""
                SELECT query, result FROM memories ORDER BY timestamp DESC LIMIT ?
            """, (limit,)) as cursor:
                memories = await cursor.fetchall()
        logger.debug(f"Retrieved {len(memories)} memories")
        return memories

    async def run_consistency_check_and_fix(self):
        logger.info("Starting consistency check and fix")
        try:
            personal_memories = await self._get_all_memories(self.personal_db_path)
            master_memories = await self._get_all_memories(self.master_db_path, self.container_id)

            # Check for memories in personal DB that are not in master DB
            for memory in personal_memories:
                if memory not in master_memories:
                    await self._add_to_master_db(memory)

            # Check for memories in master DB that are not in personal DB
            for memory in master_memories:
                if memory not in personal_memories:
                    await self._add_to_personal_db(memory)

            logger.info("Consistency check and fix completed successfully")
        except Exception as e:
            logger.error(f"Error during consistency check and fix: {str(e)}", exc_info=True)
            raise

    async def _get_all_memories(self, db_path: str, author: str = None) -> List[Tuple]:
        async with aiosqlite.connect(db_path) as db:
            if author:
                async with db.execute("SELECT query, result, embedding FROM memories WHERE author = ?", (author,)) as cursor:
                    return await cursor.fetchall()
            else:
                async with db.execute("SELECT query, result, embedding FROM memories") as cursor:
                    return await cursor.fetchall()

    async def _add_to_master_db(self, memory: Tuple):
        query, result, embedding = memory
        async with aiosqlite.connect(self.master_db_path) as db:
            await db.execute("""
                INSERT INTO memories (query, result, embedding, author) VALUES (?, ?, ?, ?)
            """, (query, result, embedding, self.container_id))
            await db.commit()
        logger.info(f"Added missing memory to master DB: {query}")

    async def _add_to_personal_db(self, memory: Tuple):
            query, result, embedding = memory
            async with aiosqlite.connect(self.personal_db_path) as db:
                await db.execute("""
                    INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)
                """, (query, result, embedding))
                await db.commit()
            logger.info(f"Added missing memory to personal DB: {query}")

    async def analyze_memory_distribution(self) -> Dict[str, int]:
        """
        Analyze the distribution of memories across different similarity metrics.
        """
        distribution = {
            "L2 norm": 0,
            "Cosine Similarity": 0,
            "BM25": 0,
            "Jaccard Similarity": 0
        }

        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT query, result FROM memories") as cursor:
                memories = await cursor.fetchall()

        for memory in memories:
            query, result = memory
            _, metadata = await self.retrieve_relevant_memories(query, return_metadata=True)
            distribution["L2 norm"] += metadata["l2_count"]
            distribution["Cosine Similarity"] += metadata["cosine_count"]
            distribution["BM25"] += metadata["bm25_count"]
            distribution["Jaccard Similarity"] += metadata["jaccard_count"]

        total = sum(distribution.values())
        for key in distribution:
            distribution[key] = round((distribution[key] / total) * 100, 2) if total > 0 else 0

        return distribution

    async def prune_memories(self, threshold: float = 0.5, max_memories: int = 1000):
        """
        Prune less relevant memories to maintain system performance.
        """
        logger.info(f"Starting memory pruning process. Threshold: {threshold}, Max memories: {max_memories}")
        
        async with aiosqlite.connect(self.personal_db_path) as db:
            async with db.execute("SELECT COUNT(*) FROM memories") as cursor:
                (total_memories,) = await cursor.fetchone()

            if total_memories <= max_memories:
                logger.info(f"Total memories ({total_memories}) do not exceed the maximum limit. No pruning needed.")
                return

            # Retrieve all memories
            async with db.execute("SELECT id, query, result, embedding FROM memories") as cursor:
                all_memories = await cursor.fetchall()

            # Calculate relevance scores
            relevance_scores = []
            for memory in all_memories:
                memory_id, query, result, embedding = memory
                embedding_array = np.frombuffer(embedding)
                
                # Use a combination of metrics to determine relevance
                l2_norm = np.linalg.norm(embedding_array)
                cosine_sim = np.dot(embedding_array, embedding_array) / (np.linalg.norm(embedding_array) ** 2)
                
                # You might want to adjust these weights based on your specific use case
                relevance_score = 0.5 * (1 / (1 + l2_norm)) + 0.5 * cosine_sim
                
                relevance_scores.append((memory_id, relevance_score))

            # Sort memories by relevance score
            relevance_scores.sort(key=lambda x: x[1], reverse=True)

            # Determine which memories to keep
            memories_to_keep = set(score[0] for score in relevance_scores[:max_memories])

            # Delete memories that don't meet the threshold or exceed the maximum limit
            deleted_count = 0
            async with db.cursor() as cursor:
                for memory_id, relevance_score in relevance_scores[max_memories:]:
                    if relevance_score < threshold:
                        await cursor.execute("DELETE FROM memories WHERE id = ?", (memory_id,))
                        deleted_count += 1

            await db.commit()

        logger.info(f"Memory pruning completed. Deleted {deleted_count} memories.")
        
        # Update corpus and indexing after pruning
        await self._load_corpus()
        self._update_indexing()

    async def export_memories(self, file_path: str):
        """
        Export all memories to a JSON file.
        """
        try:
            async with aiosqlite.connect(self.personal_db_path) as db:
                async with db.execute("SELECT query, result, timestamp FROM memories") as cursor:
                    memories = await cursor.fetchall()

            export_data = [
                {
                    "query": memory[0],
                    "result": memory[1],
                    "timestamp": memory[2]
                } for memory in memories
            ]

            with open(file_path, 'w') as f:
                json.dump(export_data, f, indent=2)

            logger.info(f"Successfully exported {len(memories)} memories to {file_path}")
        except Exception as e:
            logger.error(f"Error exporting memories: {str(e)}", exc_info=True)
            raise

    async def import_memories(self, file_path: str):
        """
        Import memories from a JSON file.
        """
        try:
            with open(file_path, 'r') as f:
                import_data = json.load(f)

            async with aiosqlite.connect(self.personal_db_path) as db:
                for memory in import_data:
                    query = memory['query']
                    result = memory['result']
                    timestamp = memory['timestamp']
                    embedding = await self.embeddings.aembed_query(query)
                    
                    await db.execute("""
                        INSERT INTO memories (query, result, embedding, timestamp)
                        VALUES (?, ?, ?, ?)
                    """, (query, result, np.array(embedding).tobytes(), timestamp))
                
                await db.commit()

            logger.info(f"Successfully imported {len(import_data)} memories from {file_path}")
            
            # Update corpus and indexing after import
            await self._load_corpus()
            self._update_indexing()
        except Exception as e:
            logger.error(f"Error importing memories: {str(e)}", exc_info=True)
            raise

    async def get_memory_stats(self) -> Dict[str, Union[int, float]]:
        """
        Get statistics about the current state of memories.
        """
        try:
            async with aiosqlite.connect(self.personal_db_path) as db:
                async with db.execute("SELECT COUNT(*) FROM memories") as cursor:
                    (total_memories,) = await cursor.fetchone()
                
                async with db.execute("SELECT MIN(timestamp), MAX(timestamp) FROM memories") as cursor:
                    (oldest_memory, newest_memory) = await cursor.fetchone()

            distribution = await self.analyze_memory_distribution()

            return {
                "total_memories": total_memories,
                "oldest_memory": oldest_memory,
                "newest_memory": newest_memory,
                "distribution": distribution
            }
        except Exception as e:
            logger.error(f"Error getting memory stats: {str(e)}", exc_info=True)
            raise

# Add any additional methods or error handling as needed-e 
File: ./src/__init__.py

-e 
File: ./src/agents/__init__.py

-e 
File: ./src/agents/agent.py

import logging
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from config import config
import json

logger = logging.getLogger("master")
chat_logger = logging.getLogger("chat")


class Agent:
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager
        self.chat_model = ChatOpenAI(model_name=config.MODEL_NAME)

    async def process_query(self, query: str) -> str:
        relevant_memories = await self.memory_manager.retrieve_relevant_memories(query)
        messages = [
            HumanMessage(
                content=f"""REQUEST = "{query}"

CONTEXT = "{relevant_memories}"

INSTRUCTIONS = You are a memory summarization agent. Your task is to analyze the provided CONTEXT, which contains relevant memories, and create a concise summary that will help in responding to the REQUEST. Follow these guidelines:

1. Memory format: The CONTEXT contains memories in the following format:
   Similar by [Similarity Metric] (ordered by timestamp - ascending):
   <memory_id>, <previous_query>, <previous_response>, <timestamp>, <similarity_score>
   
   For previously mentioned memories:
   <memory_id>, <similarity_score>

2. Analyze relevance: Consider the similarity metrics and scores to determine each memory's relevance to the current REQUEST.

3. Summarize key information: Focus on details that directly relate to or could inform a response to the REQUEST.

4. Chronological perspective: If relevant, note how information or responses have evolved over time.

5. Highlight conflicts: If memories contain conflicting information, briefly mention these discrepancies.

6. Omit redundancy: If multiple memories contain the same information, mention it only once, noting its recurrence.

Provide your summary in the following format:

SUMMARY:
1. Most relevant information: [Concise bullet points of the most pertinent details]
2. Chronological developments: [If applicable, brief timeline of how information or responses have changed]
3. Conflicting data: [If present, short description of any contradictions in the memories]
4. Recurring themes: [Common elements or responses that appear multiple times]
5. Potential gaps: [Mention any apparent missing information that might be useful for addressing the REQUEST]

RELEVANCE SCORE: [Provide a score from 1-10 indicating how relevant and useful the summarized memories are to the REQUEST, with 10 being highly relevant and 1 being minimally relevant]

This summary aims to provide a comprehensive yet concise overview of the relevant memories to assist in formulating an optimal response to the REQUEST."""
            )
        ]

        # Log the full message sent to the API
        chat_logger.info(
            f"Full ChatGPT API request: {json.dumps([m.dict() for m in messages], indent=2)}"
        )

        response = await self.chat_model.ainvoke(messages)

        # Log the full API response
        chat_logger.info(
            f"Full ChatGPT API response: {json.dumps(response.dict(), indent=2)}"
        )

        logger.debug(f"Processed query: {query} with context: {relevant_memories}")

        return response.content
-e 
File: ./src/app.py

import asyncio
from config import Config
from MultiAgentRAG.src.utils.controller import Controller
from MultiAgentRAG.src.utils.logging_setup import setup_logging
import logging

async def process_query(controller, query, chat_logger, memory_logger):
    chat_logger.info(f"Query: {query}")
    response = await controller.execute_query(query)
    chat_logger.info(f"Response: {response}")
    
    memories = await controller.get_recent_memories(5)
    memory_logger.info("Recent Memories:")
    for memory in memories:
        memory_logger.info(f"Query: {memory[0]}, Result: {memory[1]}")

async def main():
    # Initialize logging
    master_logger, chat_logger, memory_logger = setup_logging()

    # Set the log level for the memory logger
    memory_logger.setLevel(logging.DEBUG)

    try:
        master_logger.info("Starting the Multi-Agent RAG System")
        config = Config()
        
        if not config.validate_api_keys():
            raise EnvironmentError("API keys not found in environment variables")

        master_logger.info("API keys successfully loaded")
        master_logger.info(f"Using memory database path: {config.MEMORY_DB_PATH}")

        master_logger.debug("Initializing controller")
        controller = Controller(config)

        while True:
            try:
                query = input("Enter your query (or 'quit' to exit): ")
                if query.lower() == "quit":
                    master_logger.info("Exiting the program")
                    break

                master_logger.info(f"Executing query: {query}")
                await process_query(controller, query, chat_logger, memory_logger)

            except EOFError:
                master_logger.info("Received EOF, exiting the program")
                break
            except Exception as e:
                master_logger.error(f"An error occurred while processing the query: {e}", exc_info=True)
    except Exception as e:
        master_logger.error(f"An error occurred in the main program: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())-e 
File: ./src/utils/__init__.py

-e 
File: ./src/utils/enhanced_logging.py

import time
import logging
from functools import wraps

def log_execution_time(logger):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            result = await func(*args, **kwargs)
            end_time = time.time()
            execution_time = end_time - start_time
            logger.info(f"{func.__name__} executed in {execution_time:.4f} seconds")
            return result
        return wrapper
    return decorator

class DatabaseLogger:
    def __init__(self, logger):
        self.logger = logger
        file_handler = logging.FileHandler('database.log')
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        self.logger.addHandler(file_handler)

    def log_query(self, query, params=None):
        self.logger.debug(f"Executing query: {query}, Params: {params}")

    def log_memory_size(self, memory_size):
        self.logger.info(f"Memory size: {memory_size} bytes")

    def log_access(self, memory_id):
        self.logger.info(f"Accessed memory: {memory_id}")-e 
File: ./src/utils/logging_setup.py

import logging
import os
import socket
from datetime import datetime

def setup_logging():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    container_id = socket.gethostname() if is_running_in_docker() else 'local'
    log_directory = f'logs/{timestamp}_{container_id}'

    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    master_logger = logging.getLogger('master')
    master_logger.setLevel(logging.DEBUG)
    file_handler = logging.FileHandler(os.path.join(log_directory, "master.log"))
    file_handler.setFormatter(log_formatter)
    master_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(message)s'))
    master_logger.addHandler(console_handler)

    chat_logger = logging.getLogger('chat')
    chat_logger.setLevel(logging.DEBUG)
    chat_file_handler = logging.FileHandler(os.path.join(log_directory, 'chat.log'))
    chat_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
    chat_logger.addHandler(chat_file_handler)

    memory_logger = logging.getLogger('memory')
    memory_logger.setLevel(logging.DEBUG)
    memory_file_handler = logging.FileHandler(os.path.join(log_directory, 'memory.log'))
    memory_file_handler.setFormatter(log_formatter)
    memory_logger.addHandler(memory_file_handler)

    database_logger = logging.getLogger('database')
    database_logger.setLevel(logging.DEBUG)
    database_file_handler = logging.FileHandler(os.path.join(log_directory, 'database.log'))
    database_file_handler.setFormatter(log_formatter)
    database_logger.addHandler(database_file_handler)

    master_logger.info(f"Logging setup complete. Log directory: {log_directory}")
    return master_logger, chat_logger, memory_logger, database_logger

def is_running_in_docker() -> bool:
    return os.path.exists('/.dockerenv')-e 
File: ./src/utils/controller.py

import logging
from typing import List, Tuple
from src.agents.agent import Agent
from src.memory.memory_manager import MemoryManager
from config import config

class Controller:
    def __init__(self):
        self.logger = logging.getLogger('master')
        self.db_logger = logging.getLogger('database')
        self.memory_manager = None
        self.agent = None

    async def initialize(self):
        self.memory_manager = MemoryManager(config.OPENAI_API_KEY)
        await self.memory_manager.initialize()
        self.agent = Agent(self.memory_manager)

    async def execute_query(self, query: str) -> str:
        try:
            response = await self.agent.process_query(query)
            self.logger.debug(f"Generated response: {response}")
            await self.memory_manager.save_memory(query, response)
            return response
        except Exception as e:
            self.logger.error(f"Error executing query '{query}': {str(e)}", exc_info=True)
            return f"An error occurred while processing your query: {str(e)}"

    async def get_recent_memories(self, limit: int) -> List[Tuple[str, str]]:
        return await self.memory_manager.get_memories(limit)-e 
File: ./src/api.py

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging
from config import config

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    response: str

# Initialize logging
master_logger, chat_logger, memory_logger, database_logger = setup_logging()

app = FastAPI()

controller = Controller()
memory_analyzer = None

@app.on_event("startup")
async def startup_event():
    global memory_analyzer
    master_logger.info("Starting up the API server")
    await controller.initialize()

@app.on_event("shutdown")
async def shutdown_event():
    master_logger.info("Shutting down the API server")

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    try:
        response = await controller.execute_query(request.query)
        return QueryResponse(response=response)
    except Exception as e:
        master_logger.error(f"Error processing query: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")

@app.get("/memory_distribution")
async def memory_distribution_endpoint():
    try:
        distribution = await memory_analyzer.analyze_distribution()
        return {"distribution": distribution}
    except Exception as e:
        master_logger.error(f"Error analyzing memory distribution: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error analyzing memory distribution: {str(e)}")
    
@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    master_logger.error(f"Unhandled exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": f"An unexpected error occurred: {str(exc)}"}
    )

@app.get("/consistency_check")
async def consistency_check_endpoint():
    try:
        await controller.memory_manager.run_consistency_check_and_fix()
        return {"message": "Consistency check and fix completed."}
    except Exception as e:
        master_logger.error(f"Error during consistency check: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error during consistency check: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    master_logger.info("Running the API server directly")
    uvicorn.run(app, host="0.0.0.0", port=8000)-e 
File: ./src/cli.py

import asyncio
from src.utils.controller import Controller
from src.utils.logging_setup import setup_logging

master_logger, chat_logger, memory_logger, database_logger = setup_logging()
controller = Controller()
similarity_analyzer = None

async def initialize():
    await controller.initialize()
    global similarity_analyzer

async def main():
    await initialize()
    while True:
        try:
            command = input("Enter a command (query/quit): ").strip().lower()
            
            if command == 'quit':
                break
            elif command == 'query':
                query = input("Enter your query: ")
                response = await controller.execute_query(query)
                print(f"Response: {response}")
            else:
                print("Invalid command. Please try again.")
        except Exception as e:
            master_logger.error(f"An error occurred: {str(e)}", exc_info=True)
            print(f"An error occurred: {str(e)}. Please try again.")

if __name__ == "__main__":
    asyncio.run(main())-e 
File: ./.pytest_cache/README.md

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.
-e 
File: ./.pytest_cache/.gitignore

# Created by pytest automatically.
*
-e 
File: ./.pytest_cache/CACHEDIR.TAG

Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html
-e 
File: ./.pytest_cache/v/cache/nodeids

[]-e 
File: ./.pytest_cache/v/cache/lastfailed

{
  "tests/test_memory_manager.py": true
}-e 
File: ./.pytest_cache/v/cache/stepwise

[]-e 
File: ./.dockerignore

__pycache__
*.pyc
*.pyo
*.pyd
*.pdb
*.egg
*.egg-info
dist
build
eggs
parts
var
develop-eggs
.installed.cfg
lib
lib64
*.log
logs
notebooks
data/raw/*
text_docs/*
.DS_Store
.vscode
-e 
File: ./docker-compose.yml

services:
  multi-agent-rag:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    ports:
      - "8080:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    command: ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]

  multi-agent-rag-cli:
    build:
      context: .
      dockerfile: Dockerfile
    image: multi-agent-rag
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    command: ["python", "-m", "src.cli"]
    stdin_open: true
    tty: true-e 
File: ./docs/README.md

# Multi-Agent RAG System with Continual Learning

## Project Overview

This project implements a vertical slice of a multi-agent RAG (Retrieval-Augmented Generation) system with continual learning capabilities. It uses LangChain, LangGraph, and LLM API calls to create a sophisticated question-answering system that learns from interactions.

## Key Features

- Multi-agent architecture for complex query processing
- RAG (Retrieval-Augmented Generation) for enhanced responses
- Continual learning to improve performance over time
- Distributed memory management with personal and master databases
- CLI and API interfaces for versatile interaction
- Comprehensive logging system for debugging and analysis
- Docker support for easy deployment and scaling

## System Architecture

The system consists of the following main components:

1. **Controller**: Orchestrates the overall flow of query processing.
2. **Agents**: Specialized modules for retrieval, processing, and response generation.
3. **Memory Manager**: Handles storage and retrieval of past interactions.
4. **Logging System**: Provides detailed logs for system operations, including database interactions.

## Directory Structure

```
.
├── Dockerfile
├── README.md
├── config.py
├── data/
├── docker-compose.yml
├── docs/
├── logs/
├── notebooks/
├── scripts/
├── src/
│   ├── agents/
│   ├── memory/
│   ├── utils/
│   ├── api.py
│   ├── app.py
│   └── cli.py
└── tests/
```

## Setup and Installation

### Prerequisites

- Python 3.12+
- Docker and Docker Compose

### Local Setup

1. Clone the repository:
   ```
   git clone https://github.com/your-username/multi-agent-rag.git
   cd multi-agent-rag
   ```

2. Create a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   Create a `.env` file in the root directory with the following content:
   ```
   OPENAI_API_KEY=your_openai_api_key
   TAVILY_API_KEY=your_tavily_api_key
   ```

### Docker Setup

1. Build the Docker image:
   ```
   docker-compose build
   ```

2. Run the container:
   ```
   docker-compose up
   ```

## Usage

### CLI Interface

Run the CLI interface with:

```
python src/cli.py
```

Available commands:
- `query`: Enter a query to process
- `memories`: Retrieve recent memories
- `consistency`: Run a consistency check on the databases
- `quit`: Exit the program

### API Interface

Start the API server with:

```
uvicorn src.api:app --host 0.0.0.0 --port 8000
```

API endpoints:
- POST `/query`: Submit a query for processing
- GET `/consistency_check`: Run a consistency check on the databases
- GET `/health`: Check the health status of the API
- GET `/memory_distribution`: Get the distribution of memories across containers

## Logging

Logs are stored in the `logs/` directory, organized by timestamp and container ID. There are separate log files for:

- Master log (`master.log`)
- Chat log (`chat.log`)
- Memory operations log (`memory.log`)
- Database operations log (`database.log`)

## Testing

Run the test suite with:

```
pytest tests/
```
-e 
File: ./Dockerfile

FROM python:3.12

# Create a non-root user
RUN useradd -m appuser

WORKDIR /app

COPY . /app

RUN mkdir -p /app/logs /app/data
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Create and activate virtual environment
RUN python -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Upgrade pip and install dependencies with increased timeout and using a mirror
RUN pip install --upgrade pip

# Install numpy and scipy separately with increased timeout
RUN pip install --no-cache-dir numpy==1.26.4 scipy==1.14.0 --timeout 600

# Install scikit-learn separately with an even longer timeout
RUN pip install --no-cache-dir scikit-learn==1.4.0 --timeout 900

# Install other dependencies
RUN pip install --no-cache-dir -r docker_requirements.txt --timeout 600 --index-url https://pypi.org/simple --trusted-host pypi.org

RUN pip install fastapi uvicorn

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

CMD ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]-e 
File: ./config.py

import os
from dotenv import load_dotenv

class Config:
    def __init__(self):
        load_dotenv()
        
        # General settings
        self.MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4-turbo")
        
        # Database settings
        self.MEMORY_DB_PATH = self._get_memory_db_path()
        
        # API Keys
        self.OPENAI_API_KEY = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
        self.TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
        
        # Other settings
        self.LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG")
        
        # Processing agent settings
        self.PROCESSING_AGENT_MEMORIES_INCLUDED = int(os.getenv("PROCESSING_AGENT_MEMORIES_INCLUDED", "5"))
        self.MEMORY_RETRIEVAL_THRESHOLD = float(os.getenv("MEMORY_RETRIEVAL_THRESHOLD", "0.75"))
        self.MEMORY_RETRIEVAL_LIMIT = int(os.getenv("MEMORY_RETRIEVAL_LIMIT", "10"))

    def _get_memory_db_path(self):
        return "/app/memory.db" if os.path.exists("/.dockerenv") else "memory.db"

    def validate_api_keys(self):
        return bool(self.OPENAI_API_KEY and self.TAVILY_API_KEY)

    L2_NORM_THRESHOLD = float(os.getenv("L2_NORM_THRESHOLD", "0.75"))
    COSINE_SIMILARITY_THRESHOLD = float(os.getenv("COSINE_SIMILARITY_THRESHOLD", "0.75"))
    BM25_THRESHOLD = float(os.getenv("BM25_THRESHOLD", "0.5"))
    JACCARD_SIMILARITY_THRESHOLD = float(os.getenv("JACCARD_SIMILARITY_THRESHOLD", "0.3"))

config = Config()-e 
File: ./docker_requirements.txt

langchain==0.2.3
langchain-openai==0.1.8
langchain-community==0.2.4
langchain-core==0.2.5
langchain-text-splitters==0.2.1
langsmith==0.1.77
openai==1.34.0
pydantic==2.7.4
pydantic-core==2.18.4
dataclasses-json==0.6.7
python-dotenv==1.0.1
aiosqlite
rank_bm25==0.2.2
cachetools==5.3.0