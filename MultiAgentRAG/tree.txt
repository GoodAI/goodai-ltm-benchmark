      fantasy_info.pdf
     app.log
    memory.db
    README.md
    requirements.txt
     reset_logs.sh
      processing_agent.py
      response_agent.py
      retrieval_agent.py
      __init__.py
     app.py
     controller.py
      memory_manager.py
      __init__.py
      data_utils.py
      __init__.py
     __init__.py
     __init__.py

File Contents:

-e 
File: ./memory.db

./README.md

# Useful commands:
python3.12 ./src/app.py
./generate_tree.sh
source rag-venv/bin/activate

# Multi-Agent RAG System with Continual Learning

This project implements a vertical slice of a multi-agent RAG (Retrieval-Augmented Generation) system with continual learning capabilities using LangChain, LangGraph, and LLM API calls.

## Setup

1. Install the required dependencies: "pip install -r .\MultiAgentRAG\requirements.txt"
2. Set up your OpenAI API key: "export OPENAI_API_KEY=your_api_key"
3. Prepare your data:
- Place your raw data files in the `data/raw` directory.
- The system will process the data and store the embeddings in the `data/embeddings` directory.

## Usage

Run the `app.py` file to start the interactive multi-agent RAG system: "python .\MultiAgentRAG\src\app.py"

Enter your queries and the system will retrieve relevant documents, process the query, generate a response, and store the query-response pair in memory for continual learning.

Type 'quit' to exit the program.

## Project Structure

- `data/`: Contains the raw, processed, and embeddings data.
- `notebooks/`: Jupyter notebooks for experimentation and analysis.
- `src/`: Source code for the multi-agent RAG system.
  - `agents/`: Implementations of individual agents (retrieval, processing, response).
  - `memory/`: Memory management for continual learning.
  - `utils/`: Utility functions for data processing.
  - `controller.py`: Central controller for orchestrating the agents and memory.
  - `app.py`: Main application entry point.
- `tests/`: Unit tests for the system (not implemented in this vertical slice).
- `requirements.txt`: Lists the required Python dependencies.
- `README.md`: Project documentation.-e 
File: ./requirements.txt

aiohttp==3.9.5
aiosignal==1.3.1
annotated-types==0.7.0
anyio==4.4.0
attrs==23.2.0
certifi==2024.6.2
charset-normalizer==3.3.2
dataclasses-json==0.6.7
distro==1.9.0
faiss-cpu==1.8.0
frozenlist==1.4.1
greenlet==3.0.3
grpcio==1.64.1
grpcio-tools==1.64.1
h11==0.14.0
h2==4.1.0
hpack==4.0.0
httpcore==1.0.5
httpx==0.27.0
hyperframe==6.0.1
idna==3.7
jsonpatch==1.33
jsonpointer==3.0.0
langchain==0.2.3
langchain-community==0.2.4
langchain-core==0.2.5
langchain-openai==0.1.8
langchain-text-splitters==0.2.1
langsmith==0.1.77
marshmallow==3.21.3
multidict==6.0.5
mypy-extensions==1.0.0
numpy==1.26.4
openai==1.34.0
orjson==3.10.4
packaging==23.2
portalocker==2.8.2
protobuf==5.27.1
pydantic==2.7.4
pydantic_core==2.18.4
PyMuPDF==1.24.5
PyMuPDFb==1.24.3
PyYAML==6.0.1
qdrant-client==1.9.1
regex==2024.5.15
requests==2.32.3
setuptools==70.0.0
sniffio==1.3.1
SQLAlchemy==2.0.30
tenacity==8.3.0
tiktoken==0.7.0
tqdm==4.66.4
typing-inspect==0.9.0
typing_extensions==4.12.2
urllib3==2.2.1
yarl==1.9.4
python-dotenv==1.0.1
-e 
File: ./scripts/reset_logs.sh

#!/bin/bash
# reset_logs.sh
# This script resets all log files in the specified directory.

# Directory containing the log files
LOG_DIR="./logs"

# Check if the directory exists
if [ ! -d "$LOG_DIR" ]; then
    echo "Directory $LOG_DIR does not exist."
    exit 1
fi

# Iterate over all log files in the directory
for log_file in "$LOG_DIR"/*.log; 
do
    # Check if the file exists
    if [ -f "$log_file" ]; then
        # Truncate the file to zero length
        > "$log_file"
        echo "Reset $log_file"
    else
        echo "No log files found in $LOG_DIR"
    fi
done

echo "All log files have been reset."
-e 
File: ./src/agents/processing_agent.py

from langchain_community.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage, Document
from typing import List

class ProcessingAgent:
    def __init__(self, model_name: str):
        self.chat_model = ChatOpenAI(model_name=model_name)

    def process(self, query: str, context_documents: List[Document]) -> str:
        context = "\n\n".join([doc.page_content for doc in context_documents])
        messages = [
            HumanMessage(content=f"Given the following context:\n{context}\n\nAnswer the question: {query}")
        ]
        response = self.chat_model(messages)
        return response.content
-e 
File: ./src/agents/response_agent.py

# src/agents/response_agent.py

from langchain_community.chat_models import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage

class ResponseAgent:
    def __init__(self, model_name: str):
        self.chat_model = ChatOpenAI(model_name=model_name)

    def generate_response(self, query: str, result: str) -> str:
        messages = [
            HumanMessage(content=f"User Query:\n{query}"),
            AIMessage(content=f"Assistant Response:\n{result}"),
            HumanMessage(content="Generate a final response based on the above interaction.")
        ]
        response = self.chat_model(messages)
        return response.content
-e 
File: ./src/agents/retrieval_agent.py

from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from typing import List

class RetrievalAgent:
    def __init__(self, vectorstore: FAISS):
        self.vectorstore = vectorstore

    def retrieve(self, query: str) -> List[Document]:
        return self.vectorstore.similarity_search(query)-e 
File: ./src/agents/__init__.py

-e 
File: ./src/app.py

# src/app.py
import os
import logging
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from controller import Controller
from utils.data_utils import load_and_process_data

# Setup logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s %(levelname)s %(message)s',
    handlers=[
        logging.FileHandler("logs/app.log"),
        logging.StreamHandler()
    ]
)

def main():
    try:
        logging.info("Starting the Multi-Agent RAG System")

        # Load environment variables from .env file
        logging.debug("Loading environment variables from .env file")
        load_dotenv()

        openai_api_key = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
        tavily_api_key = os.getenv("TAVILY_API_KEY")

        if not openai_api_key or not tavily_api_key:
            logging.error("API keys not found in environment variables")
            raise EnvironmentError("API keys not found in environment variables")

        logging.info("API keys successfully loaded")

        # Set the environment variable for OpenAI API key
        os.environ["OPENAI_API_KEY"] = openai_api_key

        # Load and process data
        logging.debug("Loading and processing data from 'data/raw'")
        raw_documents = load_and_process_data("data/raw")
        logging.info(f"Total documents processed: {len(raw_documents)}")

        # Create vectorstore
        logging.debug("Creating vectorstore with OpenAIEmbeddings")
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        vectorstore = FAISS.from_documents(raw_documents, embeddings)
        logging.info("Vectorstore created successfully")

        # Initialize controller
        logging.debug("Initializing controller")
        controller = Controller(vectorstore, "gpt-3.5-turbo", "memory.db")

        while True:
            query = input("Enter your query (or 'quit' to exit): ")
            if query.lower() == "quit":
                logging.info("Exiting the program")
                break

            try:
                logging.info(f"Executing query: {query}")
                response = controller.execute_query(query)
                logging.info(f"Response: {response}")

                memories = controller.get_memories(5)
                logging.info("Recent Memories:")
                for memory in memories:
                    logging.info(f"Query: {memory[0]}, Result: {memory[1]}")
            except Exception as e:
                logging.error(f"An error occurred while processing the query: {e}", exc_info=True)
    except Exception as e:
        logging.error(f"An error occurred in the main program: {e}", exc_info=True)

if __name__ == "__main__":
    main()
-e 
File: ./src/controller.py

# src/controller.py

from typing import List, Tuple
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from agents.retrieval_agent import RetrievalAgent
from agents.processing_agent import ProcessingAgent
from agents.response_agent import ResponseAgent
from memory.memory_manager import MemoryManager

class Controller:
    def __init__(self, vectorstore: FAISS, model_name: str, memory_db_path: str):
        self.retrieval_agent = RetrievalAgent(vectorstore)
        self.processing_agent = ProcessingAgent(model_name)
        self.response_agent = ResponseAgent(model_name)
        self.memory_manager = MemoryManager(memory_db_path)

    def execute_query(self, query: str) -> str:
        # Retrieve relevant documents
        context_documents = self.retrieval_agent.retrieve(query)
        print(f"Retrieved documents: {context_documents}")  # Debugging statement
        print(f"Number of documents retrieved: {len(context_documents)}")  # Debugging statement
        
        if not context_documents:
            raise ValueError("No documents retrieved")

        # Process query with context
        result = self.processing_agent.process(query, context_documents)
        print(f"Processing result: {result}")  # Debugging statement

        # Generate final response
        response = self.response_agent.generate_response(query, result)
        print(f"Generated response: {response}")  # Debugging statement

        # Save memory
        self.memory_manager.save_memory(query, response)

        return response

    def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        memories = self.memory_manager.get_memories(limit)
        print(f"Memories retrieved: {memories}")  # Debugging statement
        return memories
-e 
File: ./src/memory/memory_manager.py

import sqlite3
from typing import List, Tuple

class MemoryManager:
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
        self.create_tables()

    def create_tables(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS memories (
                id INTEGER PRIMARY KEY,
                query TEXT,
                result TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)

    def save_memory(self, query: str, result: str):
        self.conn.execute("""
            INSERT INTO memories (query, result) VALUES (?, ?)
        """, (query, result))
        self.conn.commit()

    def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        cursor = self.conn.execute("""
            SELECT query, result FROM memories ORDER BY timestamp DESC LIMIT ?
        """, (limit,))
        return cursor.fetchall()-e 
File: ./src/memory/__init__.py

-e 
File: ./src/utils/data_utils.py

# src/utils/data_utils.py

import os
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import tiktoken
import logging

def tiktoken_len(text):
    tokens = tiktoken.encoding_for_model("gpt-3.5-turbo").encode(text)
    return len(tokens)

def load_and_process_data(directory_path: str):
    all_docs = []
    if not os.path.exists(directory_path):
        logging.error(f"Directory {directory_path} does not exist.")
        return all_docs

    if not os.listdir(directory_path):
        logging.warning(f"Directory {directory_path} is empty. Using default document.")
        default_content = "This is a default document. Add PDF files to the data/raw directory for processing."
        default_doc = Document(page_content=default_content)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=0,
            length_function=tiktoken_len,
        )
        split_chunks = text_splitter.split_documents([default_doc])
        all_docs.extend(split_chunks)
        return all_docs

    for filename in os.listdir(directory_path):
        file_path = os.path.join(directory_path, filename)
        logging.debug(f"Found file: {file_path}")
        if filename.endswith(".pdf"):
            logging.debug(f"Processing file: {filename}")
            try:
                loader = PyMuPDFLoader(file_path)
                docs = loader.load()
                logging.debug(f"Loaded documents from {filename}: {docs}")

                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=300,
                    chunk_overlap=0,
                    length_function=tiktoken_len,
                )
                split_chunks = text_splitter.split_documents(docs)
                logging.debug(f"Split documents from {filename}: {split_chunks}")

                all_docs.extend(split_chunks)
            except Exception as e:
                logging.error(f"Error processing file {filename}: {e}", exc_info=True)
        else:
            logging.debug(f"Skipping non-PDF file: {filename}")

    logging.info(f"Total documents processed: {len(all_docs)}")
    return all_docs
-e 
File: ./src/utils/__init__.py

-e 
File: ./src/__init__.py

-e 
File: ./tests/__init__.py

import unittest
from src.utils.data_utils import load_and_process_data
from src.memory.memory_manager import MemoryManager

class TestDataUtils(unittest.TestCase):
    def test_load_and_process_data(self):
        # Assuming there's a test PDF in data/raw for testing
        docs = load_and_process_data("data/raw")
        self.assertTrue(len(docs) > 0, "Should load and split documents")

class TestMemoryManager(unittest.TestCase):
    def setUp(self):
        self.memory_manager = MemoryManager(":memory:")  # Use in-memory database for testing

    def test_save_and_get_memories(self):
        self.memory_manager.save_memory("test_query", "test_result")
        memories = self.memory_manager.get_memories()
        self.assertEqual(len(memories), 1)
        self.assertEqual(memories[0][0], "test_query")
        self.assertEqual(memories[0][1], "test_result")

if __name__ == "__main__":
    unittest.main()
