(rag-venv) goose@LAPTOP-PUFQ2IJ7:/mnt/c/Users/fkgde/Desktop/GoodAI/__FULLTIME/LTM-Benchmark/goodai-ltm-benchmark/MultiAgentRAG$ tree
.
├── README.md
├── data
│   ├── embeddings
│   ├── processed
│   └── raw
├── json_output
│   ├── Memory_1.json
│   ├── Memory_2.json
│   ├── Memory_3.json
│   ├── Memory_4.json
│   └── Memory_5.json
├── logs
│   ├── app.log
│   ├── chat.log
│   ├── master.log
│   └── memory.log
├── memory.db
├── notebooks
│   └── Multi_Agent_RAG_LangGraph.ipynb
├── requirements.txt
├── scripts
│   ├── generate_tree.sh
│   ├── logs_to_docs.sh
│   └── reset_logs.sh
├── src
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── controller.cpython-310.pyc
│   │   └── controller.cpython-312.pyc
│   ├── agents
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-310.pyc
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   ├── processing_agent.cpython-312.pyc
│   │   │   ├── response_agent.cpython-312.pyc
│   │   │   ├── retrieval_agent.cpython-310.pyc
│   │   │   └── retrieval_agent.cpython-312.pyc
│   │   ├── processing_agent.py
│   │   ├── response_agent.py
│   │   └── retrieval_agent.py
│   ├── app.py
│   ├── controller.py
│   ├── memory
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   └── memory_manager.cpython-312.pyc
│   │   └── memory_manager.py
│   └── utils
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-312.pyc
│       │   ├── data_utils.cpython-312.pyc
│       │   └── json_utils.cpython-312.pyc
│       ├── data_utils.py
│       ├── file_manager.py
│       ├── json_utils.py
│       ├── pdf_generator.py
│       └── pdf_to_json.py
├── tests
│   └── __init__.py
├── text_docs
│   ├── app.txt
│   ├── chat.txt
│   ├── master.txt
│   └── memory.txt
└── tree.txt

File Contents:

-e 
File: ./json_output/Memory_1.json

{
    "title": "Memory_1",
    "query": "What is my name? ",
    "result": "I apologize for any confusion, but without additional information, I am unable to determine your name. If you would like to share your name with me, I would be happy to address you by it in our future interactions. Thank you.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_2.json

{
    "title": "Memory_2",
    "query": "My name is Fin, I like to ride bikes. I run a company called \"HEE9\", I am 6'5\"",
    "result": "It seems there was a misunderstanding in my previous response. As an AI language model, I don't have the ability to count or identify repeated numbers in text. How can I assist you further with your query about your interests and company?",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_3.json

{
    "title": "Memory_3",
    "query": "What is my occupation? ",
    "result": "Of course! I apologize for not being able to recall your occupation. If you would like to share it with me again, I would be happy to assist you further. Please let me know how I can help you today.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_4.json

{
    "title": "Memory_4",
    "query": "My occupation is BUS DRIVER",
    "result": "Thank you for sharing your occupation as a bus driver. It's clear that you play a crucial role in ensuring the safe transportation of passengers. If you have any questions or need assistance related to your job as a bus driver, feel free to ask. Your dedication to your profession is commendable!",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_5.json

{
    "title": "Memory_5",
    "query": "My name is Fin",
    "result": "Hello Fin! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know how I can help you.",
    "tags": "retrieved, processed"
}-e 
File: ./README.md

# Useful commands:
python3.12 ./src/app.py
./generate_tree.sh
source rag-venv/bin/activate

# Multi-Agent RAG System with Continual Learning

This project implements a vertical slice of a multi-agent RAG (Retrieval-Augmented Generation) system with continual learning capabilities using LangChain, LangGraph, and LLM API calls.

## Setup

1. Install the required dependencies: "pip install -r .\MultiAgentRAG\requirements.txt"
2. Set up your OpenAI API key: "export OPENAI_API_KEY=your_api_key"
3. Prepare your data:
- Place your raw data files in the `data/raw` directory.
- The system will process the data and store the embeddings in the `data/embeddings` directory.

## Usage

Run the `app.py` file to start the interactive multi-agent RAG system: "python .\MultiAgentRAG\src\app.py"

Enter your queries and the system will retrieve relevant documents, process the query, generate a response, and store the query-response pair in memory for continual learning.

Type 'quit' to exit the program.

## Project Structure

- `data/`: Contains the raw, processed, and embeddings data.
- `notebooks/`: Jupyter notebooks for experimentation and analysis.
- `src/`: Source code for the multi-agent RAG system.
  - `agents/`: Implementations of individual agents (retrieval, processing, response).
  - `memory/`: Memory management for continual learning.
  - `utils/`: Utility functions for data processing.
  - `controller.py`: Central controller for orchestrating the agents and memory.
  - `app.py`: Main application entry point.
- `tests/`: Unit tests for the system (not implemented in this vertical slice).
- `requirements.txt`: Lists the required Python dependencies.
- `README.md`: Project documentation.-e 
File: ./requirements.txt

aiohttp==3.9.5
aiosignal==1.3.1
annotated-types==0.7.0
anyio==4.4.0
attrs==23.2.0
certifi==2024.6.2
charset-normalizer==3.3.2
dataclasses-json==0.6.7
distro==1.9.0
faiss-cpu==1.8.0
frozenlist==1.4.1
greenlet==3.0.3
grpcio==1.64.1
grpcio-tools==1.64.1
h11==0.14.0
h2==4.1.0
hpack==4.0.0
httpcore==1.0.5
httpx==0.27.0
hyperframe==6.0.1
idna==3.7
jsonpatch==1.33
jsonpointer==3.0.0
langchain==0.2.3
langchain-community==0.2.4
langchain-core==0.2.5
langchain-openai==0.1.8
langchain-text-splitters==0.2.1
langsmith==0.1.77
marshmallow==3.21.3
multidict==6.0.5
mypy-extensions==1.0.0
numpy==1.26.4
openai==1.34.0
orjson==3.10.4
packaging==23.2
portalocker==2.8.2
protobuf==5.27.1
pydantic==2.7.4
pydantic_core==2.18.4
PyMuPDF==1.24.5
PyMuPDFb==1.24.3
PyYAML==6.0.1
qdrant-client==1.9.1
regex==2024.5.15
requests==2.32.3
setuptools==70.0.0
sniffio==1.3.1
SQLAlchemy==2.0.30
tenacity==8.3.0
tiktoken==0.7.0
tqdm==4.66.4
typing-inspect==0.9.0
typing_extensions==4.12.2
urllib3==2.2.1
yarl==1.9.4
python-dotenv==1.0.1
-e 
File: ./src/agents/processing_agent.py

# src/agents/processing_agent.py

import logging
# from langchain_community.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, Document
from typing import List

logger = logging.getLogger('master')

class ProcessingAgent:
    def __init__(self, model_name: str):
        self.chat_model = ChatOpenAI(model_name=model_name)

    def process(self, query: str, context_documents: List[Document]) -> str:
        context = "\n\n".join([doc.page_content for doc in context_documents[:3]])  # Use only top 3 documents for context
        messages = [
            HumanMessage(content=f"Given the following context:\n{context}\n\nAnswer the question: {query}")
        ]
        response = self.chat_model(messages)
        logger.debug(f"Processed query: {query} with context: {context}")
        return response.content
-e 
File: ./src/agents/response_agent.py

# src/agents/response_agent.py

import logging
# from langchain_community.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage

logger = logging.getLogger('master')

class ResponseAgent:
    def __init__(self, model_name: str):
        self.chat_model = ChatOpenAI(model_name=model_name)

    def generate_response(self, query: str, result: str) -> str:
        messages = [
            HumanMessage(content=f"User Query:\n{query}"),
            AIMessage(content=f"Assistant Response:\n{result}"),
            HumanMessage(content="Generate a final response based on the above interaction.")
        ]
        response = self.chat_model(messages)
        logger.debug(f"Generated final response for query: {query} with result: {result}")
        return response.content
-e 
File: ./src/agents/retrieval_agent.py

# src/agents/retrieval_agent.py

import logging
# from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from typing import List

logger = logging.getLogger('master')

class RetrievalAgent:
    def __init__(self, vectorstore: FAISS):
        self.vectorstore = vectorstore

    def retrieve(self, query: str) -> List[Document]:
        results = self.vectorstore.similarity_search(query)
        # Rank documents based on relevance score
        ranked_results = sorted(results, key=lambda doc: doc.metadata.get('relevance_score', 0), reverse=True)
        logger.debug(f"Retrieved and ranked documents for query: {query}")
        return ranked_results
-e 
File: ./src/agents/__init__.py

-e 
File: ./src/app.py

import os
import logging
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

from utils.data_utils import load_and_process_data, structure_memories
# from src.utils.pdf_generator import generate_pdf
from utils.json_utils import save_memory_to_json
from controller import Controller

# Setup logging
master_logger = logging.getLogger('master')
master_logger.setLevel(logging.DEBUG)
master_file_handler = logging.FileHandler("logs/master.log")
master_file_handler.setLevel(logging.DEBUG)
master_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
master_logger.addHandler(master_file_handler)

# Create console handler for query and response logs
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(message)s'))
master_logger.addHandler(console_handler)

# Chat Logger
chat_logger = logging.getLogger('chat')
chat_logger.setLevel(logging.DEBUG)
chat_file_handler = logging.FileHandler('logs/chat.log')
chat_file_handler.setLevel(logging.DEBUG)
chat_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
chat_logger.addHandler(chat_file_handler)

# Memory Logger
memory_logger = logging.getLogger('memory')
memory_logger.setLevel(logging.DEBUG)
memory_file_handler = logging.FileHandler('logs/memory.log')
memory_file_handler.setLevel(logging.DEBUG)
memory_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
memory_logger.addHandler(memory_file_handler)

def main():
    try:
        master_logger.info("Starting the Multi-Agent RAG System")

        # Load environment variables from .env file
        master_logger.debug("Loading environment variables from .env file")
        load_dotenv()

        openai_api_key = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
        tavily_api_key = os.getenv("TAVILY_API_KEY")

        if not openai_api_key or not tavily_api_key:
            master_logger.error("API keys not found in environment variables")
            raise EnvironmentError("API keys not found in environment variables")

        master_logger.info("API keys successfully loaded")

        # Set the environment variable for OpenAI API key
        os.environ["OPENAI_API_KEY"] = openai_api_key

        # Load and process data
        master_logger.debug("Loading and processing data from 'data/raw'")
        raw_documents = load_and_process_data("data/raw")
        master_logger.info(f"Total documents processed: {len(raw_documents)}")

        # Create vectorstore
        master_logger.debug("Creating vectorstore with OpenAIEmbeddings")
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        vectorstore = FAISS.from_documents(raw_documents, embeddings)
        master_logger.info("Vectorstore created successfully")

        # Initialize controller
        master_logger.debug("Initializing controller")
        controller = Controller(vectorstore, "gpt-3.5-turbo", "memory.db")

        while True:
            query = input("Enter your query (or 'quit' to exit): ")
            if query.lower() == "quit":
                master_logger.info("Exiting the program")
                break

            try:
                master_logger.info(f"Executing query: {query}")
                chat_logger.info(f"Query: {query}")
                response = controller.execute_query(query)
                chat_logger.info(f"Response: {response}")
                console_handler.setLevel(logging.INFO)
                master_logger.info(f"Response: {response}")

                memories = controller.get_memories(5)
                memory_logger.info("Recent Memories:")
                for memory in memories:
                    memory_logger.info(f"Query: {memory[0]}, Result: {memory[1]}")

                # Structure memories and save as JSON
                structured_memories = structure_memories(memories)
                for memory in structured_memories:
                    save_memory_to_json(memory, output_dir='json_output')

            except Exception as e:
                master_logger.error(f"An error occurred while processing the query: {e}", exc_info=True)
    except Exception as e:
        master_logger.error(f"An error occurred in the main program: {e}", exc_info=True)

if __name__ == "__main__":
    main()
-e 
File: ./src/controller.py

# src/controller.py

import logging
from typing import List, Tuple
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from agents.retrieval_agent import RetrievalAgent
from agents.processing_agent import ProcessingAgent
from agents.response_agent import ResponseAgent
from memory.memory_manager import MemoryManager

class Controller:
    def __init__(self, vectorstore: FAISS, model_name: str, memory_db_path: str):
        self.retrieval_agent = RetrievalAgent(vectorstore)
        self.processing_agent = ProcessingAgent(model_name)
        self.response_agent = ResponseAgent(model_name)
        self.memory_manager = MemoryManager(memory_db_path)
        self.logger = logging.getLogger('master')

    def execute_query(self, query: str) -> str:
        # Retrieve relevant documents
        context_documents = self.retrieval_agent.retrieve(query)
        self.logger.debug(f"Retrieved documents: {context_documents}")
        self.logger.debug(f"Number of documents retrieved: {len(context_documents)}")
        
        if not context_documents:
            raise ValueError("No documents retrieved")
        
        # Retrieve relevant memories
        relevant_memories = self.memory_manager.retrieve_relevant_memories(query)
        self.logger.debug(f"Retrieved relevant memories: {relevant_memories}")
        
        # Combine context documents and relevant memories
        context = context_documents + [Document(page_content=memory[0] + "\n" + memory[1]) for memory in relevant_memories]
        
        # Process query with context
        result = self.processing_agent.process(query, context)
        self.logger.debug(f"Processing result: {result}")

        # Generate final response
        response = self.response_agent.generate_response(query, result)
        self.logger.debug(f"Generated response: {response}")

        # Save memory
        self.memory_manager.save_memory(query, response)

        return response

    def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        memories = self.memory_manager.get_memories(limit)
        self.logger.debug(f"Memories retrieved: {memories}")
        return memories
-e 
File: ./src/memory/memory_manager.py

# src/memory/memory_manager.py

import logging
import sqlite3
from typing import List, Tuple
# from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings
import numpy as np
import glob
import json

logger = logging.getLogger('memory')

class MemoryManager:
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
        self.create_tables()
        self.embeddings = OpenAIEmbeddings()
        self.load_memories()

    def create_tables(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS memories (
                id INTEGER PRIMARY KEY,
                query TEXT,
                result TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        logger.debug("Ensured memories table exists")

    def save_memory(self, query: str, result: str):
        self.conn.execute("""
            INSERT INTO memories (query, result) VALUES (?, ?)
        """, (query, result))
        self.conn.commit()
        logger.debug(f"Saved memory for query: {query} with result: {result}")

    def load_memories(self):
        json_files = glob.glob("json_output/*.json")
        
        for file_path in json_files:
            with open(file_path, 'r') as json_file:
                memory_data = json.load(json_file)
                query = memory_data['query']
                result = memory_data['result']
                self.save_memory(query, result)
        
        logger.debug(f"Loaded {len(json_files)} memories from JSON files")

    def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        cursor = self.conn.execute("""
            SELECT query, result FROM memories ORDER BY timestamp DESC LIMIT ?
        """, (limit,))
        memories = cursor.fetchall()
        logger.debug(f"Retrieved {len(memories)} memories")
        return memories

    def retrieve_relevant_memories(self, query: str, threshold: float = 0.75) -> List[Tuple[str, str]]:
        cursor = self.conn.execute("SELECT query, result FROM memories")
        all_memories = cursor.fetchall()
        
        query_embedding = self.embeddings.embed_query(query)
        relevant_memories = []
        
        for memory in all_memories:
            memory_query = memory[0]
            memory_embedding = self.embeddings.embed_query(memory_query)
            similarity = np.dot(query_embedding, memory_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(memory_embedding))
            
            if similarity >= threshold:
                relevant_memories.append((memory, similarity))
        
        relevant_memories.sort(key=lambda x: x[1], reverse=True)  # Sort by similarity score in descending order
        return [memory[0] for memory in relevant_memories]  # Return the memory content instead of similarity scores



-e 
File: ./src/memory/__init__.py

-e 
File: ./src/utils/data_utils.py

# src/utils/data_utils.py

import os
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import tiktoken
import logging
from typing import List, Dict, Tuple

logger = logging.getLogger('master')

def tiktoken_len(text):
    tokens = tiktoken.encoding_for_model("gpt-3.5-turbo").encode(text)
    return len(tokens)

def load_and_process_data(directory_path: str):
    all_docs = []
    if not os.path.exists(directory_path):
        logger.error(f"Directory {directory_path} does not exist.")
        return all_docs

    if not os.listdir(directory_path):
        logger.warning(f"Directory {directory_path} is empty. Using default document.")
        default_content = "This is a default document. Add PDF files to the data/raw directory for processing."
        default_doc = Document(page_content=default_content)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=0,
            length_function=tiktoken_len,
        )
        split_chunks = text_splitter.split_documents([default_doc])
        all_docs.extend(split_chunks)
        return all_docs

    for filename in os.listdir(directory_path):
        file_path = os.path.join(directory_path, filename)
        logger.debug(f"Found file: {file_path}")
        if filename.endswith(".pdf"):
            logger.debug(f"Processing file: {filename}")
            try:
                loader = PyMuPDFLoader(file_path)
                docs = loader.load()
                logger.debug(f"Loaded documents from {filename}")

                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=300,
                    chunk_overlap=0,
                    length_function=tiktoken_len,
                )
                split_chunks = text_splitter.split_documents(docs)
                logger.debug(f"Split documents from {filename}")

                all_docs.extend(split_chunks)
            except Exception as e:
                logger.error(f"Error processing file {filename}: {e}", exc_info=True)
        else:
            logger.debug(f"Skipping non-PDF file: {filename}")

    logger.info(f"Total documents processed: {len(all_docs)}")
    return all_docs

def structure_memories(memories: List[Tuple[str, str]]) -> List[Dict[str, str]]:
    """
    Transforms a list of memory tuples into a structured list of dictionaries.

    Args:
        memories (List[Tuple[str, str]]): List of memories where each memory is a tuple (query, result).

    Returns:
        List[Dict[str, str]]: List of structured memories with titles, queries, results, and tags.
    """
    structured_memories = []
    for idx, (query, result) in enumerate(memories):
        memory_data = {
            "title": f"Memory_{idx + 1}",
            "query": query,
            "result": result,
            "tags": "retrieved, processed"
        }
        structured_memories.append(memory_data)
    return structured_memories
-e 
File: ./src/utils/file_manager.py

# src/utils/file_manager.py

import os
import json
from typing import Dict
import logging

logger = logging.getLogger('file_manager')

def save_json(data: Dict, filename: str, output_dir: str = 'json_output'):
    """
    Save a dictionary as a JSON file.
    
    Args:
        data (Dict): The data to be saved as JSON.
        filename (str): The name of the JSON file.
        output_dir (str): The directory where the JSON file will be saved.
    
    Returns:
        None
    """
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        output_path = os.path.join(output_dir, filename)
        
        with open(output_path, 'w') as json_file:
            json.dump(data, json_file, indent=4)
        
        logger.info(f"Saved JSON: {output_path}")
    except Exception as e:
        logger.error(f"Failed to save JSON {filename}: {e}")

def load_json(filename: str, output_dir: str = 'json_output') -> Dict:
    """
    Load a JSON file and return its content as a dictionary.
    
    Args:
        filename (str): The name of the JSON file.
        output_dir (str): The directory where the JSON file is located.
    
    Returns:
        Dict: The content of the JSON file.
    """
    try:
        output_path = os.path.join(output_dir, filename)
        
        with open(output_path, 'r') as json_file:
            data = json.load(json_file)
        
        logger.info(f"Loaded JSON: {output_path}")
        return data
    except Exception as e:
        logger.error(f"Failed to load JSON {filename}: {e}")
        return {}
-e 
File: ./src/utils/json_utils.py

# src/utils/json_utils.py

import os
import json
from typing import Dict
import logging

logger = logging.getLogger('json_utils')

def save_memory_to_json(memory_data: Dict[str, str], output_dir: str = 'json_output'):
    """
    Saves the structured memory data to a JSON file.
    
    Args:
        memory_data (Dict[str, str]): Dictionary containing the structured memory data.
        output_dir (str): Directory to save the JSON files.
    
    Returns:
        None
    """
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        output_path = os.path.join(output_dir, f"{memory_data['title']}.json")
        
        with open(output_path, 'w') as json_file:
            json.dump(memory_data, json_file, indent=4)
        
        logger.info(f"Generated JSON: {output_path}")
    except Exception as e:
        logger.error(f"Error saving JSON file {memory_data['title']}: {e}")
-e 
File: ./src/utils/pdf_generator.py

# src/utils/pdf_generator.py

from fpdf import FPDF
from typing import Dict
import os

class PDF(FPDF):
    def header(self):
        self.set_font('Arial', 'B', 12)
        self.cell(0, 10, 'Memory Document', 0, 1, 'C')

    def footer(self):
        self.set_y(-15)
        self.set_font('Arial', 'I', 8)
        self.cell(0, 10, f'Page {self.page_no()}', 0, 0, 'C')

def generate_pdf(memory_data: Dict[str, str], output_dir: str = 'output'):
    """
    Generates a PDF document from the provided memory data.
    
    Args:
        memory_data (Dict[str, str]): Dictionary containing the structured memory data.
        output_dir (str): Directory to save the generated PDF documents.
    
    Returns:
        None
    """
    pdf = PDF()
    pdf.add_page()
    
    pdf.set_font("Arial", size=12)
    pdf.cell(200, 10, txt=memory_data['title'], ln=True, align='C')

    pdf.set_font("Arial", size=10)
    pdf.multi_cell(0, 10, txt=f"Query: {memory_data['query']}")
    pdf.multi_cell(0, 10, txt=f"Result: {memory_data['result']}")
    pdf.multi_cell(0, 10, txt=f"Tags: {memory_data['tags']}")

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    output_path = os.path.join(output_dir, f"{memory_data['title']}.pdf")
    pdf.output(output_path)
    print(f"Generated PDF: {output_path}")
-e 
File: ./src/utils/pdf_to_json.py

# src/utils/pdf_to_json.py

import os
import PyPDF2
import json

def pdf_to_json(pdf_path: str, output_dir: str = 'json_output'):
    """
    Converts a PDF file to a JSON file.
    
    Args:
        pdf_path (str): Path to the PDF file.
        output_dir (str): Directory to save the JSON files.
    
    Returns:
        None
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(pdf_path, 'rb') as pdf_file:
        reader = PyPDF2.PdfFileReader(pdf_file)
        text = ""
        for page_num in range(reader.numPages):
            page = reader.getPage(page_num)
            text += page.extractText()
    
    # Assume the PDF filename follows the format 'Memory_X.pdf'
    base_name = os.path.basename(pdf_path)
    title = os.path.splitext(base_name)[0]
    memory_data = {
        "title": title,
        "content": text,
        "tags": "converted"
    }
    
    output_path = os.path.join(output_dir, f"{title}.json")
    
    with open(output_path, 'w') as json_file:
        json.dump(memory_data, json_file, indent=4)
    
    print(f"Converted PDF to JSON: {output_path}")-e 
File: ./src/utils/__init__.py

-e 
File: ./src/__init__.py

-e 
File: ./tests/__init__.py

import unittest
from src.utils.data_utils import load_and_process_data
from src.memory.memory_manager import MemoryManager

class TestDataUtils(unittest.TestCase):
    def test_load_and_process_data(self):
        # Assuming there's a test PDF in data/raw for testing
        docs = load_and_process_data("data/raw")
        self.assertTrue(len(docs) > 0, "Should load and split documents")

class TestMemoryManager(unittest.TestCase):
    def setUp(self):
        self.memory_manager = MemoryManager(":memory:")  # Use in-memory database for testing

    def test_save_and_get_memories(self):
        self.memory_manager.save_memory("test_query", "test_result")
        memories = self.memory_manager.get_memories()
        self.assertEqual(len(memories), 1)
        self.assertEqual(memories[0][0], "test_query")
        self.assertEqual(memories[0][1], "test_result")

if __name__ == "__main__":
    unittest.main()
