.
├── Dockerfile
├── README.md
├── config.py
├── docker-compose.yml
├── get-docker.sh
├── json_output
│   ├── Memory_1.json
│   ├── Memory_2.json
│   ├── Memory_3.json
│   ├── Memory_4.json
│   └── Memory_5.json
├── logging_setup.py
├── logs
│   └── docker_agents
│       ├── 20240621_100315_cda0f10422bb
│       │   ├── chat.log
│       │   ├── master.log
│       │   └── memory.log
│       └── 20240621_104419_449de6b53046
│           ├── chat.log
│           ├── master.log
│           └── memory.log
├── memory.db
├── notebooks
│   └── Multi_Agent_RAG_LangGraph.ipynb
├── requirements.txt
├── requirements_lite.txt
├── scripts
│   ├── _docker_reset.sh
│   ├── generate_tree.sh
│   ├── logs_to_docs.sh
│   └── reset_logs.sh
├── src
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── controller.cpython-310.pyc
│   │   └── controller.cpython-312.pyc
│   ├── agents
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-310.pyc
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   ├── processing_agent.cpython-312.pyc
│   │   │   ├── response_agent.cpython-312.pyc
│   │   │   ├── retrieval_agent.cpython-310.pyc
│   │   │   └── retrieval_agent.cpython-312.pyc
│   │   ├── processing_agent.py
│   │   ├── response_agent.py
│   │   └── retrieval_agent.py
│   ├── api.py
│   ├── app.py
│   ├── controller.py
│   ├── memory
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   └── memory_manager.cpython-312.pyc
│   │   └── memory_manager.py
│   └── utils
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-312.pyc
│       │   ├── data_utils.cpython-312.pyc
│       │   └── json_utils.cpython-312.pyc
│       ├── data_utils.py
│       ├── file_manager.py
│       ├── json_utils.py
│       └── reset_database.py
├── text_docs
└── tree.txt

16 directories, 54 files

Note: The following list of files and directories are excluded only from the appended file contents section:

__pycache__, .git, .env, text_docs, scripts, logs, tree.txt, generate_tree.sh, README, *.ipynb, *.pdf, *.db

File Contents:

-e 
File: ./.dockerignore

__pycache__
*.pyc
*.pyo
*.pyd
*.pdb
*.egg
*.egg-info
dist
build
eggs
parts
var
develop-eggs
.installed.cfg
lib
lib64
*.log
logs
notebooks
data/raw/*
text_docs/*
.DS_Store
.vscode
-e 
File: ./config.py

# config.py

import os

class Config:
    # General settings
    MODEL_NAME = "gpt-3.5-turbo"
    
    # Database settings
    MEMORY_DB_PATH = "/app/memory.db" if os.path.exists("/.dockerenv") else "memory.db"
    
    # API Keys
    OPENAI_API_KEY = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
    TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
    
    # Other settings
    LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG")
    
# You can add more configurations as needed
-e 
File: ./docker-compose.yml

version: '3.1'

services:
  multi-agent-rag:
    build: .
    ports:
      - "8080:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - LOG_LEVEL=${LOG_LEVEL}
    env_file:
      - .env
    volumes:
      - ./memory.db:/app/memory.db
      - ./logs/docker_agents:/app/logs
    stdin_open: true
    tty: true
-e 
File: ./Dockerfile

# Dockerfile

FROM python:3.12

WORKDIR /app

COPY . /app

RUN mkdir -p /app/logs

RUN apt-get update && apt-get install -y dos2unix

RUN dos2unix /app/.env

RUN pip install --no-cache-dir -r requirements_lite.txt
RUN pip install fastapi uvicorn

EXPOSE 8000

COPY .env /app/.env

CMD ["sh", "-c", "printenv && uvicorn src.api:app --host 0.0.0.0 --port 8000"]
-e 
File: ./get-docker.sh

#!/bin/sh
set -e
# Docker Engine for Linux installation script.
#
# This script is intended as a convenient way to configure docker's package
# repositories and to install Docker Engine, This script is not recommended
# for production environments. Before running this script, make yourself familiar
# with potential risks and limitations, and refer to the installation manual
# at https://docs.docker.com/engine/install/ for alternative installation methods.
#
# The script:
#
# - Requires `root` or `sudo` privileges to run.
# - Attempts to detect your Linux distribution and version and configure your
#   package management system for you.
# - Doesn't allow you to customize most installation parameters.
# - Installs dependencies and recommendations without asking for confirmation.
# - Installs the latest stable release (by default) of Docker CLI, Docker Engine,
#   Docker Buildx, Docker Compose, containerd, and runc. When using this script
#   to provision a machine, this may result in unexpected major version upgrades
#   of these packages. Always test upgrades in a test environment before
#   deploying to your production systems.
# - Isn't designed to upgrade an existing Docker installation. When using the
#   script to update an existing installation, dependencies may not be updated
#   to the expected version, resulting in outdated versions.
#
# Source code is available at https://github.com/docker/docker-install/
#
# Usage
# ==============================================================================
#
# To install the latest stable versions of Docker CLI, Docker Engine, and their
# dependencies:
#
# 1. download the script
#
#   $ curl -fsSL https://get.docker.com -o install-docker.sh
#
# 2. verify the script's content
#
#   $ cat install-docker.sh
#
# 3. run the script with --dry-run to verify the steps it executes
#
#   $ sh install-docker.sh --dry-run
#
# 4. run the script either as root, or using sudo to perform the installation.
#
#   $ sudo sh install-docker.sh
#
# Command-line options
# ==============================================================================
#
# --version <VERSION>
# Use the --version option to install a specific version, for example:
#
#   $ sudo sh install-docker.sh --version 23.0
#
# --channel <stable|test>
#
# Use the --channel option to install from an alternative installation channel.
# The following example installs the latest versions from the "test" channel,
# which includes pre-releases (alpha, beta, rc):
#
#   $ sudo sh install-docker.sh --channel test
#
# Alternatively, use the script at https://test.docker.com, which uses the test
# channel as default.
#
# --mirror <Aliyun|AzureChinaCloud>
#
# Use the --mirror option to install from a mirror supported by this script.
# Available mirrors are "Aliyun" (https://mirrors.aliyun.com/docker-ce), and
# "AzureChinaCloud" (https://mirror.azure.cn/docker-ce), for example:
#
#   $ sudo sh install-docker.sh --mirror AzureChinaCloud
#
# ==============================================================================


# Git commit from https://github.com/docker/docker-install when
# the script was uploaded (Should only be modified by upload job):
SCRIPT_COMMIT_SHA="6d9743e9656cc56f699a64800b098d5ea5a60020"

# strip "v" prefix if present
VERSION="${VERSION#v}"

# The channel to install from:
#   * stable
#   * test
#   * edge (deprecated)
#   * nightly (deprecated)
DEFAULT_CHANNEL_VALUE="stable"
if [ -z "$CHANNEL" ]; then
	CHANNEL=$DEFAULT_CHANNEL_VALUE
fi

DEFAULT_DOWNLOAD_URL="https://download.docker.com"
if [ -z "$DOWNLOAD_URL" ]; then
	DOWNLOAD_URL=$DEFAULT_DOWNLOAD_URL
fi

DEFAULT_REPO_FILE="docker-ce.repo"
if [ -z "$REPO_FILE" ]; then
	REPO_FILE="$DEFAULT_REPO_FILE"
fi

mirror=''
DRY_RUN=${DRY_RUN:-}
while [ $# -gt 0 ]; do
	case "$1" in
		--channel)
			CHANNEL="$2"
			shift
			;;
		--dry-run)
			DRY_RUN=1
			;;
		--mirror)
			mirror="$2"
			shift
			;;
		--version)
			VERSION="${2#v}"
			shift
			;;
		--*)
			echo "Illegal option $1"
			;;
	esac
	shift $(( $# > 0 ? 1 : 0 ))
done

case "$mirror" in
	Aliyun)
		DOWNLOAD_URL="https://mirrors.aliyun.com/docker-ce"
		;;
	AzureChinaCloud)
		DOWNLOAD_URL="https://mirror.azure.cn/docker-ce"
		;;
	"")
		;;
	*)
		>&2 echo "unknown mirror '$mirror': use either 'Aliyun', or 'AzureChinaCloud'."
		exit 1
		;;
esac

case "$CHANNEL" in
	stable|test)
		;;
	edge|nightly)
		>&2 echo "DEPRECATED: the $CHANNEL channel has been deprecated and is no longer supported by this script."
		exit 1
		;;
	*)
		>&2 echo "unknown CHANNEL '$CHANNEL': use either stable or test."
		exit 1
		;;
esac

command_exists() {
	command -v "$@" > /dev/null 2>&1
}

# version_gte checks if the version specified in $VERSION is at least the given
# SemVer (Maj.Minor[.Patch]), or CalVer (YY.MM) version.It returns 0 (success)
# if $VERSION is either unset (=latest) or newer or equal than the specified
# version, or returns 1 (fail) otherwise.
#
# examples:
#
# VERSION=23.0
# version_gte 23.0  // 0 (success)
# version_gte 20.10 // 0 (success)
# version_gte 19.03 // 0 (success)
# version_gte 21.10 // 1 (fail)
version_gte() {
	if [ -z "$VERSION" ]; then
			return 0
	fi
	eval version_compare "$VERSION" "$1"
}

# version_compare compares two version strings (either SemVer (Major.Minor.Path),
# or CalVer (YY.MM) version strings. It returns 0 (success) if version A is newer
# or equal than version B, or 1 (fail) otherwise. Patch releases and pre-release
# (-alpha/-beta) are not taken into account
#
# examples:
#
# version_compare 23.0.0 20.10 // 0 (success)
# version_compare 23.0 20.10   // 0 (success)
# version_compare 20.10 19.03  // 0 (success)
# version_compare 20.10 20.10  // 0 (success)
# version_compare 19.03 20.10  // 1 (fail)
version_compare() (
	set +x

	yy_a="$(echo "$1" | cut -d'.' -f1)"
	yy_b="$(echo "$2" | cut -d'.' -f1)"
	if [ "$yy_a" -lt "$yy_b" ]; then
		return 1
	fi
	if [ "$yy_a" -gt "$yy_b" ]; then
		return 0
	fi
	mm_a="$(echo "$1" | cut -d'.' -f2)"
	mm_b="$(echo "$2" | cut -d'.' -f2)"

	# trim leading zeros to accommodate CalVer
	mm_a="${mm_a#0}"
	mm_b="${mm_b#0}"

	if [ "${mm_a:-0}" -lt "${mm_b:-0}" ]; then
		return 1
	fi

	return 0
)

is_dry_run() {
	if [ -z "$DRY_RUN" ]; then
		return 1
	else
		return 0
	fi
}

is_wsl() {
	case "$(uname -r)" in
	*microsoft* ) true ;; # WSL 2
	*Microsoft* ) true ;; # WSL 1
	* ) false;;
	esac
}

is_darwin() {
	case "$(uname -s)" in
	*darwin* ) true ;;
	*Darwin* ) true ;;
	* ) false;;
	esac
}

deprecation_notice() {
	distro=$1
	distro_version=$2
	echo
	printf "\033[91;1mDEPRECATION WARNING\033[0m\n"
	printf "    This Linux distribution (\033[1m%s %s\033[0m) reached end-of-life and is no longer supported by this script.\n" "$distro" "$distro_version"
	echo   "    No updates or security fixes will be released for this distribution, and users are recommended"
	echo   "    to upgrade to a currently maintained version of $distro."
	echo
	printf   "Press \033[1mCtrl+C\033[0m now to abort this script, or wait for the installation to continue."
	echo
	sleep 10
}

get_distribution() {
	lsb_dist=""
	# Every system that we officially support has /etc/os-release
	if [ -r /etc/os-release ]; then
		lsb_dist="$(. /etc/os-release && echo "$ID")"
	fi
	# Returning an empty string here should be alright since the
	# case statements don't act unless you provide an actual value
	echo "$lsb_dist"
}

echo_docker_as_nonroot() {
	if is_dry_run; then
		return
	fi
	if command_exists docker && [ -e /var/run/docker.sock ]; then
		(
			set -x
			$sh_c 'docker version'
		) || true
	fi

	# intentionally mixed spaces and tabs here -- tabs are stripped by "<<-EOF", spaces are kept in the output
	echo
	echo "================================================================================"
	echo
	if version_gte "20.10"; then
		echo "To run Docker as a non-privileged user, consider setting up the"
		echo "Docker daemon in rootless mode for your user:"
		echo
		echo "    dockerd-rootless-setuptool.sh install"
		echo
		echo "Visit https://docs.docker.com/go/rootless/ to learn about rootless mode."
		echo
	fi
	echo
	echo "To run the Docker daemon as a fully privileged service, but granting non-root"
	echo "users access, refer to https://docs.docker.com/go/daemon-access/"
	echo
	echo "WARNING: Access to the remote API on a privileged Docker daemon is equivalent"
	echo "         to root access on the host. Refer to the 'Docker daemon attack surface'"
	echo "         documentation for details: https://docs.docker.com/go/attack-surface/"
	echo
	echo "================================================================================"
	echo
}

# Check if this is a forked Linux distro
check_forked() {

	# Check for lsb_release command existence, it usually exists in forked distros
	if command_exists lsb_release; then
		# Check if the `-u` option is supported
		set +e
		lsb_release -a -u > /dev/null 2>&1
		lsb_release_exit_code=$?
		set -e

		# Check if the command has exited successfully, it means we're in a forked distro
		if [ "$lsb_release_exit_code" = "0" ]; then
			# Print info about current distro
			cat <<-EOF
			You're using '$lsb_dist' version '$dist_version'.
			EOF

			# Get the upstream release info
			lsb_dist=$(lsb_release -a -u 2>&1 | tr '[:upper:]' '[:lower:]' | grep -E 'id' | cut -d ':' -f 2 | tr -d '[:space:]')
			dist_version=$(lsb_release -a -u 2>&1 | tr '[:upper:]' '[:lower:]' | grep -E 'codename' | cut -d ':' -f 2 | tr -d '[:space:]')

			# Print info about upstream distro
			cat <<-EOF
			Upstream release is '$lsb_dist' version '$dist_version'.
			EOF
		else
			if [ -r /etc/debian_version ] && [ "$lsb_dist" != "ubuntu" ] && [ "$lsb_dist" != "raspbian" ]; then
				if [ "$lsb_dist" = "osmc" ]; then
					# OSMC runs Raspbian
					lsb_dist=raspbian
				else
					# We're Debian and don't even know it!
					lsb_dist=debian
				fi
				dist_version="$(sed 's/\/.*//' /etc/debian_version | sed 's/\..*//')"
				case "$dist_version" in
					12)
						dist_version="bookworm"
					;;
					11)
						dist_version="bullseye"
					;;
					10)
						dist_version="buster"
					;;
					9)
						dist_version="stretch"
					;;
					8)
						dist_version="jessie"
					;;
				esac
			fi
		fi
	fi
}

do_install() {
	echo "# Executing docker install script, commit: $SCRIPT_COMMIT_SHA"

	if command_exists docker; then
		cat >&2 <<-'EOF'
			Warning: the "docker" command appears to already exist on this system.

			If you already have Docker installed, this script can cause trouble, which is
			why we're displaying this warning and provide the opportunity to cancel the
			installation.

			If you installed the current Docker package using this script and are using it
			again to update Docker, you can safely ignore this message.

			You may press Ctrl+C now to abort this script.
		EOF
		( set -x; sleep 20 )
	fi

	user="$(id -un 2>/dev/null || true)"

	sh_c='sh -c'
	if [ "$user" != 'root' ]; then
		if command_exists sudo; then
			sh_c='sudo -E sh -c'
		elif command_exists su; then
			sh_c='su -c'
		else
			cat >&2 <<-'EOF'
			Error: this installer needs the ability to run commands as root.
			We are unable to find either "sudo" or "su" available to make this happen.
			EOF
			exit 1
		fi
	fi

	if is_dry_run; then
		sh_c="echo"
	fi

	# perform some very rudimentary platform detection
	lsb_dist=$( get_distribution )
	lsb_dist="$(echo "$lsb_dist" | tr '[:upper:]' '[:lower:]')"

	if is_wsl; then
		echo
		echo "WSL DETECTED: We recommend using Docker Desktop for Windows."
		echo "Please get Docker Desktop from https://www.docker.com/products/docker-desktop/"
		echo
		cat >&2 <<-'EOF'

			You may press Ctrl+C now to abort this script.
		EOF
		( set -x; sleep 20 )
	fi

	case "$lsb_dist" in

		ubuntu)
			if command_exists lsb_release; then
				dist_version="$(lsb_release --codename | cut -f2)"
			fi
			if [ -z "$dist_version" ] && [ -r /etc/lsb-release ]; then
				dist_version="$(. /etc/lsb-release && echo "$DISTRIB_CODENAME")"
			fi
		;;

		debian|raspbian)
			dist_version="$(sed 's/\/.*//' /etc/debian_version | sed 's/\..*//')"
			case "$dist_version" in
				12)
					dist_version="bookworm"
				;;
				11)
					dist_version="bullseye"
				;;
				10)
					dist_version="buster"
				;;
				9)
					dist_version="stretch"
				;;
				8)
					dist_version="jessie"
				;;
			esac
		;;

		centos|rhel)
			if [ -z "$dist_version" ] && [ -r /etc/os-release ]; then
				dist_version="$(. /etc/os-release && echo "$VERSION_ID")"
			fi
		;;

		*)
			if command_exists lsb_release; then
				dist_version="$(lsb_release --release | cut -f2)"
			fi
			if [ -z "$dist_version" ] && [ -r /etc/os-release ]; then
				dist_version="$(. /etc/os-release && echo "$VERSION_ID")"
			fi
		;;

	esac

	# Check if this is a forked Linux distro
	check_forked

	# Print deprecation warnings for distro versions that recently reached EOL,
	# but may still be commonly used (especially LTS versions).
	case "$lsb_dist.$dist_version" in
		debian.stretch|debian.jessie)
			deprecation_notice "$lsb_dist" "$dist_version"
			;;
		raspbian.stretch|raspbian.jessie)
			deprecation_notice "$lsb_dist" "$dist_version"
			;;
		ubuntu.xenial|ubuntu.trusty)
			deprecation_notice "$lsb_dist" "$dist_version"
			;;
		ubuntu.lunar|ubuntu.kinetic|ubuntu.impish|ubuntu.hirsute|ubuntu.groovy|ubuntu.eoan|ubuntu.disco|ubuntu.cosmic)
			deprecation_notice "$lsb_dist" "$dist_version"
			;;
		fedora.*)
			if [ "$dist_version" -lt 36 ]; then
				deprecation_notice "$lsb_dist" "$dist_version"
			fi
			;;
	esac

	# Run setup for each distro accordingly
	case "$lsb_dist" in
		ubuntu|debian|raspbian)
			pre_reqs="apt-transport-https ca-certificates curl"
			apt_repo="deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] $DOWNLOAD_URL/linux/$lsb_dist $dist_version $CHANNEL"
			(
				if ! is_dry_run; then
					set -x
				fi
				$sh_c 'apt-get update -qq >/dev/null'
				$sh_c "DEBIAN_FRONTEND=noninteractive apt-get install -y -qq $pre_reqs >/dev/null"
				$sh_c 'install -m 0755 -d /etc/apt/keyrings'
				$sh_c "curl -fsSL \"$DOWNLOAD_URL/linux/$lsb_dist/gpg\" -o /etc/apt/keyrings/docker.asc"
				$sh_c "chmod a+r /etc/apt/keyrings/docker.asc"
				$sh_c "echo \"$apt_repo\" > /etc/apt/sources.list.d/docker.list"
				$sh_c 'apt-get update -qq >/dev/null'
			)
			pkg_version=""
			if [ -n "$VERSION" ]; then
				if is_dry_run; then
					echo "# WARNING: VERSION pinning is not supported in DRY_RUN"
				else
					# Will work for incomplete versions IE (17.12), but may not actually grab the "latest" if in the test channel
					pkg_pattern="$(echo "$VERSION" | sed 's/-ce-/~ce~.*/g' | sed 's/-/.*/g')"
					search_command="apt-cache madison docker-ce | grep '$pkg_pattern' | head -1 | awk '{\$1=\$1};1' | cut -d' ' -f 3"
					pkg_version="$($sh_c "$search_command")"
					echo "INFO: Searching repository for VERSION '$VERSION'"
					echo "INFO: $search_command"
					if [ -z "$pkg_version" ]; then
						echo
						echo "ERROR: '$VERSION' not found amongst apt-cache madison results"
						echo
						exit 1
					fi
					if version_gte "18.09"; then
							search_command="apt-cache madison docker-ce-cli | grep '$pkg_pattern' | head -1 | awk '{\$1=\$1};1' | cut -d' ' -f 3"
							echo "INFO: $search_command"
							cli_pkg_version="=$($sh_c "$search_command")"
					fi
					pkg_version="=$pkg_version"
				fi
			fi
			(
				pkgs="docker-ce${pkg_version%=}"
				if version_gte "18.09"; then
						# older versions didn't ship the cli and containerd as separate packages
						pkgs="$pkgs docker-ce-cli${cli_pkg_version%=} containerd.io"
				fi
				if version_gte "20.10"; then
						pkgs="$pkgs docker-compose-plugin docker-ce-rootless-extras$pkg_version"
				fi
				if version_gte "23.0"; then
						pkgs="$pkgs docker-buildx-plugin"
				fi
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "DEBIAN_FRONTEND=noninteractive apt-get install -y -qq $pkgs >/dev/null"
			)
			echo_docker_as_nonroot
			exit 0
			;;
		centos|fedora|rhel)
			if [ "$(uname -m)" != "s390x" ] && [ "$lsb_dist" = "rhel" ]; then
				echo "Packages for RHEL are currently only available for s390x."
				exit 1
			fi

			if command_exists dnf; then
				pkg_manager="dnf"
				pkg_manager_flags="--best"
				config_manager="dnf config-manager"
				enable_channel_flag="--set-enabled"
				disable_channel_flag="--set-disabled"
				pre_reqs="dnf-plugins-core"
			else
				pkg_manager="yum"
				pkg_manager_flags=""
				config_manager="yum-config-manager"
				enable_channel_flag="--enable"
				disable_channel_flag="--disable"
				pre_reqs="yum-utils"
			fi

			if [ "$lsb_dist" = "fedora" ]; then
				pkg_suffix="fc$dist_version"
			else
				pkg_suffix="el"
			fi
			repo_file_url="$DOWNLOAD_URL/linux/$lsb_dist/$REPO_FILE"
			(
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "$pkg_manager $pkg_manager_flags install -y -q $pre_reqs"
				$sh_c "$config_manager --add-repo $repo_file_url"

				if [ "$CHANNEL" != "stable" ]; then
					$sh_c "$config_manager $disable_channel_flag 'docker-ce-*'"
					$sh_c "$config_manager $enable_channel_flag 'docker-ce-$CHANNEL'"
				fi
				$sh_c "$pkg_manager makecache"
			)
			pkg_version=""
			if [ -n "$VERSION" ]; then
				if is_dry_run; then
					echo "# WARNING: VERSION pinning is not supported in DRY_RUN"
				else
					pkg_pattern="$(echo "$VERSION" | sed 's/-ce-/\\\\.ce.*/g' | sed 's/-/.*/g').*$pkg_suffix"
					search_command="$pkg_manager list --showduplicates docker-ce | grep '$pkg_pattern' | tail -1 | awk '{print \$2}'"
					pkg_version="$($sh_c "$search_command")"
					echo "INFO: Searching repository for VERSION '$VERSION'"
					echo "INFO: $search_command"
					if [ -z "$pkg_version" ]; then
						echo
						echo "ERROR: '$VERSION' not found amongst $pkg_manager list results"
						echo
						exit 1
					fi
					if version_gte "18.09"; then
						# older versions don't support a cli package
						search_command="$pkg_manager list --showduplicates docker-ce-cli | grep '$pkg_pattern' | tail -1 | awk '{print \$2}'"
						cli_pkg_version="$($sh_c "$search_command" | cut -d':' -f 2)"
					fi
					# Cut out the epoch and prefix with a '-'
					pkg_version="-$(echo "$pkg_version" | cut -d':' -f 2)"
				fi
			fi
			(
				pkgs="docker-ce$pkg_version"
				if version_gte "18.09"; then
					# older versions didn't ship the cli and containerd as separate packages
					if [ -n "$cli_pkg_version" ]; then
						pkgs="$pkgs docker-ce-cli-$cli_pkg_version containerd.io"
					else
						pkgs="$pkgs docker-ce-cli containerd.io"
					fi
				fi
				if version_gte "20.10"; then
					pkgs="$pkgs docker-compose-plugin docker-ce-rootless-extras$pkg_version"
				fi
				if version_gte "23.0"; then
						pkgs="$pkgs docker-buildx-plugin"
				fi
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "$pkg_manager $pkg_manager_flags install -y -q $pkgs"
			)
			echo_docker_as_nonroot
			exit 0
			;;
		sles)
			if [ "$(uname -m)" != "s390x" ]; then
				echo "Packages for SLES are currently only available for s390x"
				exit 1
			fi
			repo_file_url="$DOWNLOAD_URL/linux/$lsb_dist/$REPO_FILE"
			pre_reqs="ca-certificates curl libseccomp2 awk"
			(
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "zypper install -y $pre_reqs"
				$sh_c "zypper addrepo $repo_file_url"
				if ! is_dry_run; then
						cat >&2 <<-'EOF'
						WARNING!!
						openSUSE repository (https://download.opensuse.org/repositories/security:/SELinux) will be enabled now.
						Do you wish to continue?
						You may press Ctrl+C now to abort this script.
						EOF
						( set -x; sleep 30 )
				fi
				opensuse_repo="https://download.opensuse.org/repositories/security:/SELinux/openSUSE_Factory/security:SELinux.repo"
				$sh_c "zypper addrepo $opensuse_repo"
				$sh_c "zypper --gpg-auto-import-keys refresh"
				$sh_c "zypper lr -d"
			)
			pkg_version=""
			if [ -n "$VERSION" ]; then
				if is_dry_run; then
					echo "# WARNING: VERSION pinning is not supported in DRY_RUN"
				else
					pkg_pattern="$(echo "$VERSION" | sed 's/-ce-/\\\\.ce.*/g' | sed 's/-/.*/g')"
					search_command="zypper search -s --match-exact 'docker-ce' | grep '$pkg_pattern' | tail -1 | awk '{print \$6}'"
					pkg_version="$($sh_c "$search_command")"
					echo "INFO: Searching repository for VERSION '$VERSION'"
					echo "INFO: $search_command"
					if [ -z "$pkg_version" ]; then
						echo
						echo "ERROR: '$VERSION' not found amongst zypper list results"
						echo
						exit 1
					fi
					search_command="zypper search -s --match-exact 'docker-ce-cli' | grep '$pkg_pattern' | tail -1 | awk '{print \$6}'"
					# It's okay for cli_pkg_version to be blank, since older versions don't support a cli package
					cli_pkg_version="$($sh_c "$search_command")"
					pkg_version="-$pkg_version"
				fi
			fi
			(
				pkgs="docker-ce$pkg_version"
				if version_gte "18.09"; then
					if [ -n "$cli_pkg_version" ]; then
						# older versions didn't ship the cli and containerd as separate packages
						pkgs="$pkgs docker-ce-cli-$cli_pkg_version containerd.io"
					else
						pkgs="$pkgs docker-ce-cli containerd.io"
					fi
				fi
				if version_gte "20.10"; then
					pkgs="$pkgs docker-compose-plugin docker-ce-rootless-extras$pkg_version"
				fi
				if version_gte "23.0"; then
						pkgs="$pkgs docker-buildx-plugin"
				fi
				if ! is_dry_run; then
					set -x
				fi
				$sh_c "zypper -q install -y $pkgs"
			)
			echo_docker_as_nonroot
			exit 0
			;;
		*)
			if [ -z "$lsb_dist" ]; then
				if is_darwin; then
					echo
					echo "ERROR: Unsupported operating system 'macOS'"
					echo "Please get Docker Desktop from https://www.docker.com/products/docker-desktop"
					echo
					exit 1
				fi
			fi
			echo
			echo "ERROR: Unsupported distribution '$lsb_dist'"
			echo
			exit 1
			;;
	esac
	exit 1
}

# wrapped up in a function so that we have some protection against only getting
# half the file during "curl | sh"
do_install
-e 
File: ./json_output/Memory_1.json

{
    "title": "Memory_1",
    "query": "Today is Wednesday, June 19th.",
    "result": "I apologize for the mistake in my initial response. Today is indeed Tuesday, June 18th. Thank you for bringing it to my attention.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_2.json

{
    "title": "Memory_2",
    "query": "What is the day today? ",
    "result": "Today is Tuesday, June 18th.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_3.json

{
    "title": "Memory_3",
    "query": "This is a test. Respond with how tall I am. ",
    "result": "I apologize for the incorrect response earlier. As a language model AI, I do not have the ability to determine your height. If you have any other questions or need assistance with something else, feel free to ask!",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_4.json

{
    "title": "Memory_4",
    "query": "What is my name? Use your context.",
    "result": "I'm sorry, I made an error in my initial response. Without knowing your name, I cannot accurately determine what it is based on the context provided. If you would like to share your name with me, I can assist you further.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_5.json

{
    "title": "Memory_5",
    "query": "What is my name? ",
    "result": "I apologize for the confusion. Without knowing your actual name, I can only provide possible guesses. If you could kindly provide me with your name, I would be happy to address you correctly.",
    "tags": "retrieved, processed"
}-e 
File: ./logging_setup.py

import logging
import os
import socket
from datetime import datetime

def setup_logging():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    container_id = socket.gethostname() if is_running_in_docker() else 'local'
    log_directory = f'logs/{timestamp}_{container_id}'

    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    master_logger = logging.getLogger('master')
    master_logger.setLevel(logging.DEBUG)
    file_handler = logging.FileHandler(os.path.join(log_directory, "master.log"))
    file_handler.setFormatter(log_formatter)
    master_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(message)s'))
    master_logger.addHandler(console_handler)

    chat_logger = logging.getLogger('chat')
    chat_logger.setLevel(logging.DEBUG)
    chat_file_handler = logging.FileHandler(os.path.join(log_directory, 'chat.log'))
    chat_file_handler.setFormatter(log_formatter)
    chat_logger.addHandler(chat_file_handler)

    memory_logger = logging.getLogger('memory')
    memory_logger.setLevel(logging.DEBUG)
    memory_file_handler = logging.FileHandler(os.path.join(log_directory, 'memory.log'))
    memory_file_handler.setFormatter(log_formatter)
    memory_logger.addHandler(memory_file_handler)

    master_logger.info(f"Logging setup complete. Log directory: {log_directory}")
    return master_logger, chat_logger, memory_logger

def is_running_in_docker() -> bool:
    """Check if the code is running inside a Docker container."""
    try:
        with open('/proc/1/cgroup', 'rt') as f:
            return 'docker' in f.read()
    except Exception:
        return False
-e 
File: ./README.md

# Multi-Agent RAG System with Continual Learning

This project implements a vertical slice of a multi-agent RAG (Retrieval-Augmented Generation) system with continual learning capabilities using LangChain, LangGraph, and LLM API calls.

## Setup

### Prerequisites

- Python 3.12
- Docker (for containerization)

### Installation

1. Clone the repository:
   ```sh
   `git clone https://github.com/your-username/your-repo.git`
   `cd your-repo/MultiAgentRAG`

2. Create a virtual environment and activate it:
  `python3.12 -m venv rag-venv`
  `source rag-venv/bin/activate`

3. Install the required dependencies:
  `pip install -r requirements.txt`

4. Set up your OpenAI API key:
  `export OPENAI_API_KEY=your_openai_api_key`

5. Prepare Your Data
  Place your raw data files in the data/raw directory.
  The system will process the data and store the embeddings in the data/embeddings directory.

### Usage
Run the app.py file to start the interactive multi-agent RAG system:
  `python src/app.py`

Enter your queries, and the system will retrieve relevant documents, process the query, generate a response, and store the query-response pair in memory for continual learning.

Type `'quit'` to exit the program.

### Project Structure
data/: Contains the raw, processed, and embeddings data.
json_output/: Stores the JSON output files for the memories.
logs/: Contains log files.
notebooks/: Jupyter notebooks for experimentation and analysis.
scripts/: Contains utility scripts.
generate_tree.sh: Script to generate the directory tree.
logs_to_docs.sh: Script to convert logs to documents.
reset_logs.sh: Script to reset logs.
src/: Source code for the multi-agent RAG system.
agents/: Implementations of individual agents (retrieval, processing, response).
memory/: Memory management for continual learning.
utils/: Utility functions for data processing.
controller.py: Central controller for orchestrating the agents and memory.
app.py: Main application entry point.
requirements.txt: Lists the required Python dependencies.
README.md: Project documentation.

Running with Docker
Build the Docker image:
  `docker build -t multi-agent-rag .`
Run the Docker container:

`sudo docker run -it -e OPENAI_API_KEY=GOODAI_OPENAI_API_KEY_LTM01 -p 8000:8000 multi-agent-rag`
Access the interactive multi-agent RAG system by connecting to the container.-e 
File: ./requirements.txt

aiohttp==3.9.5
aiosignal==1.3.1
annotated-types==0.7.0
anyio==4.4.0
attrs==23.2.0
certifi==2024.6.2
charset-normalizer==3.3.2
dataclasses-json==0.6.7
distro==1.9.0
faiss-cpu==1.8.0
frozenlist==1.4.1
greenlet==3.0.3
grpcio==1.64.1
grpcio-tools==1.64.1
h11==0.14.0
h2==4.1.0
hpack==4.0.0
httpcore==1.0.5
httpx==0.27.0
hyperframe==6.0.1
idna==3.7
jsonpatch==1.33
jsonpointer==3.0.0
langchain==0.2.3
langchain-community==0.2.4
langchain-core==0.2.5
langchain-openai==0.1.8
langchain-text-splitters==0.2.1
langsmith==0.1.77
marshmallow==3.21.3
multidict==6.0.5
mypy-extensions==1.0.0
numpy==1.26.4
openai==1.34.0
orjson==3.10.4
packaging==23.2
portalocker==2.8.2
protobuf==5.27.1
pydantic==2.7.4
pydantic_core==2.18.4
PyMuPDF==1.24.5
PyMuPDFb==1.24.3
PyYAML==6.0.1
qdrant-client==1.9.1
regex==2024.5.15
requests==2.32.3
setuptools==70.0.0
sniffio==1.3.1
SQLAlchemy==2.0.30
tenacity==8.3.0
tiktoken==0.7.0
tqdm==4.66.4
typing-inspect==0.9.0
typing_extensions==4.12.2
urllib3==2.2.1
yarl==1.9.4
python-dotenv==1.0.1
-e 
File: ./requirements_lite.txt

langchain==0.2.3
langchain-openai==0.1.8
langchain-community==0.2.4
langchain-core==0.2.5
langchain-text-splitters==0.2.1
langsmith==0.1.77
openai==1.34.0
numpy==1.26.4
pydantic==2.7.4
pydantic-core==2.18.4
dataclasses-json==0.6.7
python-dotenv==1.0.1
-e 
File: ./src/agents/processing_agent.py

# src/agents/processing_agent.py

import logging
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, Document
from typing import List

logger = logging.getLogger('master')
chat_logger = logging.getLogger('chat')

class ProcessingAgent:
    def __init__(self, model_name: str):
        self.chat_model = ChatOpenAI(model_name=model_name)

    def process(self, query: str, context_documents: List[Document]) -> str:
        context = "\n\n".join([doc.page_content for doc in context_documents[:3]])
        messages = [
            HumanMessage(content=f"You are 'PROCESSING_AGENT' a specialist in distilling information. Based on the this retrieved information from our database: \"\"\"{context}\"\"\" formulate a response to: \"{query}\"")
        ]
        response = self.chat_model.invoke(messages)
        logger.debug(f"Processed query: {query} with context: {context}")
        chat_logger.info(f"ChatGPT API response: {response.content}")
        return response.content
-e 
File: ./src/agents/response_agent.py

# src/agents/response_agent.py

import logging
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage

logger = logging.getLogger('master')
chat_logger = logging.getLogger('chat')

class ResponseAgent:
    def __init__(self, model_name: str):
        self.chat_model = ChatOpenAI(model_name=model_name)

    def generate_response(self, query: str, result: str) -> str:
        messages = [
            HumanMessage(content=f"ORIGINAL_USER_QUERY = \n{query}"),
            AIMessage(content=f"PROCESSING_AGENT_RESPONSE = \n{result}"),
            HumanMessage(content="You are the 'RESPONSE_AGENT' in a multi-tiered system (do not mention this). Utilize the information from 'PROCESSING_AGENT' to best respond to 'ORIGINAL_USER_QUERY' in a manner that has the following attributes: terse, pithy, accurate, interpretive, best-effort, anticipatory")
        ]
        response = self.chat_model.invoke(messages)
        logger.debug(f"Generated final response for query: {query} with result: {result}")
        chat_logger.info(f"ChatGPT API response: {response.content}")
        return response.content
-e 
File: ./src/agents/retrieval_agent.py

# src/agents/retrieval_agent.py

import logging
from typing import List, Tuple

logger = logging.getLogger('master')

class RetrievalAgent:
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager

    def retrieve(self, query: str) -> List[Tuple[str, str]]:
        relevant_memories = self.memory_manager.retrieve_relevant_memories(query)
        logger.debug(f"Retrieved and ranked relevant memories for query: {query}")
        return relevant_memories
-e 
File: ./src/agents/__init__.py

-e 
File: ./src/api.py

# src/api.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from src.controller import Controller
import os
from logging_setup import setup_logging  # Import logging setup

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    response: str

# Initialize logging
master_logger, chat_logger, memory_logger = setup_logging()

app = FastAPI()

memory_db_path = "/app/memory.db" if os.path.exists("/.dockerenv") else "memory.db"
controller = Controller()

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    try:
        response = controller.execute_query(request.query)
        return QueryResponse(response=response)
    except Exception as e:
        master_logger.error(f"Error processing query: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
-e 
File: ./src/app.py

import os
from dotenv import load_dotenv
from controller import Controller
from utils.data_utils import structure_memories
from utils.json_utils import save_memory_to_json
from logging_setup import setup_logging, is_running_in_docker  # Import logging setup

def main():
    master_logger, chat_logger, memory_logger = setup_logging()

    try:
        master_logger.info("Starting the Multi-Agent RAG System")
        load_dotenv()
        openai_api_key = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
        tavily_api_key = os.getenv("TAVILY_API_KEY")

        if not openai_api_key or not tavily_api_key:
            master_logger.error("API keys not found in environment variables")
            raise EnvironmentError("API keys not found in environment variables")

        master_logger.info("API keys successfully loaded")
        os.environ["OPENAI_API_KEY"] = openai_api_key

        memory_db_path = "/app/memory.db" if is_running_in_docker() else "memory.db"
        master_logger.info(f"Using memory database path: {memory_db_path}")

        master_logger.debug("Initializing controller")
        controller = Controller()


        while True:
            try:
                query = input("Enter your query (or 'quit' to exit): ")
                if query.lower() == "quit":
                    master_logger.info("Exiting the program")
                    break

                master_logger.info(f"Executing query: {query}")
                chat_logger.info(f"Query: {query}")
                response = controller.execute_query(query)
                chat_logger.info(f"Response: {response}")
                master_logger.info(f"Response: {response}")

                memories = controller.get_memories(5)
                memory_logger.info("Recent Memories:")
                for memory in memories:
                    memory_logger.info(f"Query: {memory[0]}, Result: {memory[1]}")

                structured_memories = structure_memories(memories)
                for memory in structured_memories:
                    save_memory_to_json(memory, output_dir='json_output')

            except EOFError:
                master_logger.info("Received EOF, exiting the program")
                break
            except Exception as e:
                master_logger.error(f"An error occurred while processing the query: {e}", exc_info=True)
    except Exception as e:
        master_logger.error(f"An error occurred in the main program: {e}", exc_info=True)

if __name__ == "__main__":
    main()
-e 
File: ./src/controller.py

# src/controller.py

import logging
from typing import List, Tuple
from langchain.schema import Document
from src.agents.retrieval_agent import RetrievalAgent
from src.agents.processing_agent import ProcessingAgent
from src.agents.response_agent import ResponseAgent
from src.memory.memory_manager import MemoryManager
from config import Config

class Controller:
    def __init__(self):
        self.memory_manager = MemoryManager(Config.MEMORY_DB_PATH, Config.OPENAI_API_KEY)
        self.retrieval_agent = RetrievalAgent(self.memory_manager)
        self.processing_agent = ProcessingAgent(Config.MODEL_NAME)
        self.response_agent = ResponseAgent(Config.MODEL_NAME)
        self.logger = logging.getLogger('master')

    def execute_query(self, query: str) -> str:
        """Execute the query by retrieving, processing, and generating a response."""
        try:
            relevant_memories = self.retrieval_agent.retrieve(query)
            self.logger.debug(f"Retrieved relevant memories: {relevant_memories}")

            context_documents = [Document(page_content=f"{memory[0]}\n{memory[1]}") for memory in relevant_memories]

            result = self.processing_agent.process(query, context_documents)
            self.logger.debug(f"Processing result: {result}")

            response = self.response_agent.generate_response(query, result)
            self.logger.debug(f"Generated response: {response}")

            self.memory_manager.save_memory(query, response)

            return response
        except Exception as e:
            self.logger.error(f"Error executing query '{query}': {str(e)}", exc_info=True)
            raise

    def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        """Retrieve a list of memories up to the specified limit."""
        try:
            memories = self.memory_manager.get_memories(limit)
            self.logger.debug(f"Memories retrieved: {memories}")
            return memories
        except Exception as e:
            self.logger.error(f"Error retrieving memories: {str(e)}", exc_info=True)
            raise
-e 
File: ./src/memory/memory_manager.py

# src/memory/memory_manager.py

import logging
import sqlite3
from typing import List, Tuple
from langchain_openai import OpenAIEmbeddings
import numpy as np
import os

logger = logging.getLogger('memory')

class MemoryManager:
    def __init__(self, db_path: str, api_key: str):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        self.create_tables()

    def create_tables(self):
        """Create the necessary tables if they don't already exist."""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS memories (
                id INTEGER PRIMARY KEY,
                query TEXT,
                result TEXT,
                embedding BLOB,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        logger.debug("Ensured memories table exists")

    def save_memory(self, query: str, result: str):
        """Save a memory to the database."""
        try:
            embedding = np.array(self.embeddings.embed_query(query)).tobytes()
            self.conn.execute("""
                INSERT INTO memories (query, result, embedding) VALUES (?, ?, ?)
            """, (query, result, embedding))
            self.conn.commit()
            logger.debug(f"Saved memory for query: {query} with result: {result}")
        except Exception as e:
            logger.error(f"Error saving memory for query '{query}': {str(e)}", exc_info=True)
            raise

    def retrieve_relevant_memories(self, query: str, threshold: float = 0.75) -> List[Tuple[str, str]]:
        """Retrieve memories relevant to the query based on a similarity threshold, sorted by timestamp."""
        try:
            query_embedding = np.array(self.embeddings.embed_query(query))
            all_memories = self.get_all_memories()
            relevant_memories = []

            for memory in all_memories:
                memory_query, memory_result, memory_embedding, timestamp = memory
                memory_embedding = np.frombuffer(memory_embedding)
                similarity = np.dot(query_embedding, memory_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(memory_embedding))
                if similarity >= threshold:
                    relevant_memories.append((memory_query, memory_result, similarity, timestamp))

            if not relevant_memories:
                return []

            # Sort by similarity in descending order to find the most relevant one
            relevant_memories.sort(key=lambda x: x[2], reverse=True)
            most_relevant_memory = relevant_memories[0]

            # Filter memories above the threshold and sort by timestamp in descending order
            filtered_memories = [mem for mem in relevant_memories if mem[2] >= threshold]
            filtered_memories.sort(key=lambda x: x[3], reverse=True)

            # Include the most relevant memory if not already in the filtered list
            if most_relevant_memory not in filtered_memories:
                filtered_memories.append(most_relevant_memory)

            # Return the relevant memories without similarity and timestamp
            return [(memory[0], memory[1]) for memory in filtered_memories]
        except Exception as e:
            logger.error(f"Error retrieving relevant memories for query '{query}': {str(e)}", exc_info=True)
            raise

    def reset_database(self):
        self.conn.close()
        os.remove(self.db_path)
        self.conn = sqlite3.connect(self.db_path)
        self.create_tables()
        logger.debug("Database reset and tables recreated")

    def load_precomputed_embeddings(self):
        # Load precomputed embeddings from the database if needed
        pass

    def get_all_memories(self) -> List[Tuple[str, str, bytes, str]]:
        cursor = self.conn.execute("SELECT query, result, embedding, timestamp FROM memories")
        memories = cursor.fetchall()
        logger.debug(f"Retrieved {len(memories)} memories")
        return memories

    def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        cursor = self.conn.execute("""
            SELECT query, result FROM memories ORDER BY timestamp DESC LIMIT ?
        """, (limit,))
        memories = cursor.fetchall()
        logger.debug(f"Retrieved {len(memories)} memories")
        return memories
-e 
File: ./src/memory/__init__.py

-e 
File: ./src/utils/data_utils.py

# src/utils/data_utils.py

import logging
from typing import List, Dict, Tuple

logger = logging.getLogger('master')

def structure_memories(memories: List[Tuple[str, str]]) -> List[Dict[str, str]]:
    """
    Transforms a list of memory tuples into a structured list of dictionaries.

    Args:
        memories (List[Tuple[str, str]]): List of memories where each memory is a tuple (query, result).

    Returns:
        List[Dict[str, str]]: List of structured memories with titles, queries, results, and tags.
    """
    structured_memories = []
    for idx, (query, result) in enumerate(memories):
        memory_data = {
            "title": f"Memory_{idx + 1}",
            "query": query,
            "result": result,
            "tags": "retrieved, processed"
        }
        structured_memories.append(memory_data)
    return structured_memories
-e 
File: ./src/utils/file_manager.py

# src/utils/file_manager.py

import os
import json
from typing import Dict
import logging

logger = logging.getLogger('file_manager')

def save_json(data: Dict, filename: str, output_dir: str = 'json_output'):
    """
    Save a dictionary as a JSON file.
    
    Args:
        data (Dict): The data to be saved as JSON.
        filename (str): The name of the JSON file.
        output_dir (str): The directory where the JSON file will be saved.
    
    Returns:
        None
    """
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        output_path = os.path.join(output_dir, filename)
        
        with open(output_path, 'w') as json_file:
            json.dump(data, json_file, indent=4)
        
        logger.info(f"Saved JSON: {output_path}")
    except Exception as e:
        logger.error(f"Failed to save JSON {filename}: {e}")

def load_json(filename: str, output_dir: str = 'json_output') -> Dict:
    """
    Load a JSON file and return its content as a dictionary.
    
    Args:
        filename (str): The name of the JSON file.
        output_dir (str): The directory where the JSON file is located.
    
    Returns:
        Dict: The content of the JSON file.
    """
    try:
        output_path = os.path.join(output_dir, filename)
        
        with open(output_path, 'r') as json_file:
            data = json.load(json_file)
        
        logger.info(f"Loaded JSON: {output_path}")
        return data
    except Exception as e:
        logger.error(f"Failed to load JSON {filename}: {e}")
        return {}
-e 
File: ./src/utils/json_utils.py

# src/utils/json_utils.py

import os
import json
from typing import Dict
import logging

logger = logging.getLogger('json_utils')

def save_memory_to_json(memory_data: Dict[str, str], output_dir: str = 'json_output'):
    """
    Saves the structured memory data to a JSON file.
    
    Args:
        memory_data (Dict[str, str]): Dictionary containing the structured memory data.
        output_dir (str): Directory to save the JSON files.
    
    Returns:
        None
    """
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        output_path = os.path.join(output_dir, f"{memory_data['title']}.json")
        
        with open(output_path, 'w') as json_file:
            json.dump(memory_data, json_file, indent=4)
        
        logger.info(f"Generated JSON: {output_path}")
    except Exception as e:
        logger.error(f"Error saving JSON file {memory_data['title']}: {e}")
-e 
File: ./src/utils/reset_database.py

# src/utils/reset_database.py

import sys
import os

# Add the src directory to the PYTHONPATH
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'src')))

from memory.memory_manager import MemoryManager

if __name__ == "__main__":
    memory_manager = MemoryManager("memory.db", "your_api_key_here")
    memory_manager.reset_database()
    print("Database has been reset.")
-e 
File: ./src/utils/__init__.py

-e 
File: ./src/__init__.py

