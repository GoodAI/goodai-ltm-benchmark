(rag-venv) goose@LAPTOP-PUFQ2IJ7:/mnt/c/Users/fkgde/Desktop/GoodAI/__FULLTIME/LTM-Benchmark/goodai-ltm-benchmark/MultiAgentRAG$ tree
.
├── README.md     
├── data
│   ├── embeddings
│   ├── processed    
│   └── raw
├── json_output      
│   ├── Memory_1.json
│   ├── Memory_2.json
│   ├── Memory_3.json
│   ├── Memory_4.json
│   └── Memory_5.json
├── logs
│   ├── app.log
│   ├── chat.log
│   ├── master.log
│   └── memory.log
├── memory.db
├── notebooks
│   └── Multi_Agent_RAG_LangGraph.ipynb
├── requirements.txt
├── scripts
│   ├── generate_tree.sh
│   ├── logs_to_docs.sh
│   └── reset_logs.sh
├── src
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── controller.cpython-310.pyc
│   │   └── controller.cpython-312.pyc
│   ├── agents
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-310.pyc
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   ├── processing_agent.cpython-312.pyc
│   │   │   ├── response_agent.cpython-312.pyc
│   │   │   ├── retrieval_agent.cpython-310.pyc
│   │   │   └── retrieval_agent.cpython-312.pyc
│   │   ├── processing_agent.py
│   │   ├── response_agent.py
│   │   └── retrieval_agent.py
│   ├── app.py
│   ├── controller.py
│   ├── memory
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   └── memory_manager.cpython-312.pyc
│   │   └── memory_manager.py
│   └── utils
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-312.pyc
│       │   ├── data_utils.cpython-312.pyc
│       │   └── json_utils.cpython-312.pyc
│       ├── data_utils.py
│       ├── file_manager.py
│       └── json_utils.py
├── text_docs
└── tree.txt

File Contents:

-e 
File: ./json_output/Memory_1.json

{
    "title": "Memory_1",
    "query": "What is my profession?",
    "result": "I apologize for any confusion, but without knowing your profession, I am unable to provide you with that information. If you would like to share your profession with me, I would be happy to assist you further.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_2.json

{
    "title": "Memory_2",
    "query": "List all the things I like to do? ",
    "result": "It appears that you enjoy watching movies and playing video games as your primary activities. If there are any other hobbies or interests you have, please share them so I can provide a more detailed list of things you like to do. Feel free to provide more information for a personalized response.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_3.json

{
    "title": "Memory_3",
    "query": "What do I like to do?",
    "result": "It seems like you enjoy watching movies and playing video games as your preferred activities. If you have any specific preferences or interests in these areas, feel free to share more details for a more personalized response.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_4.json

{
    "title": "Memory_4",
    "query": "What do I like to do? ",
    "result": "It appears that you enjoy watching movies and playing video games. These activities may bring you entertainment and relaxation. Keep exploring new movies and games to continue enjoying your hobbies.",
    "tags": "retrieved, processed"
}-e 
File: ./json_output/Memory_5.json

{
    "title": "Memory_5",
    "query": "7",
    "result": "I apologize for the confusion. It seems there was a misunderstanding. If you have any specific questions or need assistance with anything else, please let me know and I'll do my best to help.",
    "tags": "retrieved, processed"
}-e 
File: ./README.md

# Useful commands:
python3.12 ./src/app.py
./generate_tree.sh
source rag-venv/bin/activate

# Multi-Agent RAG System with Continual Learning

This project implements a vertical slice of a multi-agent RAG (Retrieval-Augmented Generation) system with continual learning capabilities using LangChain, LangGraph, and LLM API calls.

## Setup

1. Install the required dependencies: "pip install -r .\MultiAgentRAG\requirements.txt"
2. Set up your OpenAI API key: "export OPENAI_API_KEY=your_api_key"
3. Prepare your data:
- Place your raw data files in the `data/raw` directory.
- The system will process the data and store the embeddings in the `data/embeddings` directory.

## Usage

Run the `app.py` file to start the interactive multi-agent RAG system: "python .\MultiAgentRAG\src\app.py"

Enter your queries and the system will retrieve relevant documents, process the query, generate a response, and store the query-response pair in memory for continual learning.

Type 'quit' to exit the program.

## Project Structure

- `data/`: Contains the raw, processed, and embeddings data.
- `notebooks/`: Jupyter notebooks for experimentation and analysis.
- `src/`: Source code for the multi-agent RAG system.
  - `agents/`: Implementations of individual agents (retrieval, processing, response).
  - `memory/`: Memory management for continual learning.
  - `utils/`: Utility functions for data processing.
  - `controller.py`: Central controller for orchestrating the agents and memory.
  - `app.py`: Main application entry point.
- `tests/`: Unit tests for the system (not implemented in this vertical slice).
- `requirements.txt`: Lists the required Python dependencies.
- `README.md`: Project documentation.-e 
File: ./requirements.txt

aiohttp==3.9.5
aiosignal==1.3.1
annotated-types==0.7.0
anyio==4.4.0
attrs==23.2.0
certifi==2024.6.2
charset-normalizer==3.3.2
dataclasses-json==0.6.7
distro==1.9.0
faiss-cpu==1.8.0
frozenlist==1.4.1
greenlet==3.0.3
grpcio==1.64.1
grpcio-tools==1.64.1
h11==0.14.0
h2==4.1.0
hpack==4.0.0
httpcore==1.0.5
httpx==0.27.0
hyperframe==6.0.1
idna==3.7
jsonpatch==1.33
jsonpointer==3.0.0
langchain==0.2.3
langchain-community==0.2.4
langchain-core==0.2.5
langchain-openai==0.1.8
langchain-text-splitters==0.2.1
langsmith==0.1.77
marshmallow==3.21.3
multidict==6.0.5
mypy-extensions==1.0.0
numpy==1.26.4
openai==1.34.0
orjson==3.10.4
packaging==23.2
portalocker==2.8.2
protobuf==5.27.1
pydantic==2.7.4
pydantic_core==2.18.4
PyMuPDF==1.24.5
PyMuPDFb==1.24.3
PyYAML==6.0.1
qdrant-client==1.9.1
regex==2024.5.15
requests==2.32.3
setuptools==70.0.0
sniffio==1.3.1
SQLAlchemy==2.0.30
tenacity==8.3.0
tiktoken==0.7.0
tqdm==4.66.4
typing-inspect==0.9.0
typing_extensions==4.12.2
urllib3==2.2.1
yarl==1.9.4
python-dotenv==1.0.1
-e 
File: ./src/agents/processing_agent.py

# src/agents/processing_agent.py

import logging
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, Document
from typing import List

logger = logging.getLogger('master')

class ProcessingAgent:
    def __init__(self, model_name: str):
        self.chat_model = ChatOpenAI(model_name=model_name)

    def process(self, query: str, context_documents: List[Document]) -> str:
        context = "\n\n".join([doc.page_content for doc in context_documents[:3]])
        messages = [
            HumanMessage(content=f"Given the following context:\n{context}\n\nAnswer the question: {query}")
        ]
        response = self.chat_model.invoke(messages)
        logger.debug(f"Processed query: {query} with context: {context}")
        return response.content

-e 
File: ./src/agents/response_agent.py

# src/agents/response_agent.py

import logging
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage

logger = logging.getLogger('master')

class ResponseAgent:
    def __init__(self, model_name: str):
        self.chat_model = ChatOpenAI(model_name=model_name)

    def generate_response(self, query: str, result: str) -> str:
        messages = [
            HumanMessage(content=f"User Query:\n{query}"),
            AIMessage(content=f"Assistant Response:\n{result}"),
            HumanMessage(content="Generate a final response based on the above interaction.")
        ]
        response = self.chat_model.invoke(messages)
        logger.debug(f"Generated final response for query: {query} with result: {result}")
        return response.content
-e 
File: ./src/agents/retrieval_agent.py

# src/agents/retrieval_agent.py

import logging
# from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from typing import List

logger = logging.getLogger('master')

class RetrievalAgent:
    def __init__(self, vectorstore: FAISS):
        self.vectorstore = vectorstore

    def retrieve(self, query: str) -> List[Document]:
        results = self.vectorstore.similarity_search(query)
        # Rank documents based on relevance score
        ranked_results = sorted(results, key=lambda doc: doc.metadata.get('relevance_score', 0), reverse=True)
        logger.debug(f"Retrieved and ranked documents for query: {query}")
        return ranked_results
-e 
File: ./src/agents/__init__.py

-e 
File: ./src/app.py

import os
import logging
from dotenv import load_dotenv
from utils.data_utils import structure_memories
from utils.json_utils import save_memory_to_json
from controller import Controller

# Setup logging
master_logger = logging.getLogger('master')
master_logger.setLevel(logging.DEBUG)
file_handler = logging.FileHandler("logs/master.log")
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
master_logger.addHandler(file_handler)

# Console handler for query and response logs
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(message)s'))
master_logger.addHandler(console_handler)

# Separate loggers for different types of logs
chat_logger = logging.getLogger('chat')
chat_logger.setLevel(logging.DEBUG)
chat_file_handler = logging.FileHandler('logs/chat.log')
chat_file_handler.setLevel(logging.DEBUG)
chat_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
chat_logger.addHandler(chat_file_handler)

memory_logger = logging.getLogger('memory')
memory_logger.setLevel(logging.DEBUG)
memory_file_handler = logging.FileHandler('logs/memory.log')
memory_file_handler.setLevel(logging.DEBUG)
memory_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
memory_logger.addHandler(memory_file_handler)

def main():
    try:
        master_logger.info("Starting the Multi-Agent RAG System")
        load_dotenv()
        openai_api_key = os.getenv("GOODAI_OPENAI_API_KEY_LTM01")
        tavily_api_key = os.getenv("TAVILY_API_KEY")

        if not openai_api_key or not tavily_api_key:
            master_logger.error("API keys not found in environment variables")
            raise EnvironmentError("API keys not found in environment variables")

        master_logger.info("API keys successfully loaded")
        os.environ["OPENAI_API_KEY"] = openai_api_key

        master_logger.debug("Initializing controller")
        controller = Controller("gpt-3.5-turbo", "memory.db")

        while True:
            query = input("Enter your query (or 'quit' to exit): ")
            if query.lower() == "quit":
                master_logger.info("Exiting the program")
                break

            try:
                master_logger.info(f"Executing query: {query}")
                chat_logger.info(f"Query: {query}")
                response = controller.execute_query(query)
                chat_logger.info(f"Response: {response}")
                console_handler.setLevel(logging.INFO)
                master_logger.info(f"Response: {response}")

                memories = controller.get_memories(5)
                memory_logger.info("Recent Memories:")
                for memory in memories:
                    memory_logger.info(f"Query: {memory[0]}, Result: {memory[1]}")

                structured_memories = structure_memories(memories)
                for memory in structured_memories:
                    save_memory_to_json(memory, output_dir='json_output')

            except Exception as e:
                master_logger.error(f"An error occurred while processing the query: {e}", exc_info=True)
    except Exception as e:
        master_logger.error(f"An error occurred in the main program: {e}", exc_info=True)

if __name__ == "__main__":
    main()
-e 
File: ./src/controller.py

# src/controller.py

import logging
from typing import List, Tuple
from langchain.schema import Document
from agents.processing_agent import ProcessingAgent
from agents.response_agent import ResponseAgent
from memory.memory_manager import MemoryManager

class Controller:
    def __init__(self, model_name: str, memory_db_path: str):
        self.processing_agent = ProcessingAgent(model_name)
        self.response_agent = ResponseAgent(model_name)
        self.memory_manager = MemoryManager(memory_db_path)
        self.logger = logging.getLogger('master')

    def execute_query(self, query: str) -> str:
        # Retrieve relevant memories
        relevant_memories = self.memory_manager.retrieve_relevant_memories(query)
        self.logger.debug(f"Retrieved relevant memories: {relevant_memories}")
        
        # Convert relevant memories to Document objects
        context_documents = [Document(page_content=f"{memory[0]}\n{memory[1]}") for memory in relevant_memories]
        
        # Process query with context
        result = self.processing_agent.process(query, context_documents)
        self.logger.debug(f"Processing result: {result}")

        # Generate final response
        response = self.response_agent.generate_response(query, result)
        self.logger.debug(f"Generated response: {response}")

        # Save memory
        self.memory_manager.save_memory(query, response)

        return response

    def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]:
        memories = self.memory_manager.get_memories(limit)
        self.logger.debug(f"Memories retrieved: {memories}")
        return memories
-e 
File: ./src/memory/memory_manager.py

import logging
import sqlite3
from typing import List, Tuple
from langchain_openai import OpenAIEmbeddings
import numpy as np
import glob
import json

logger = logging.getLogger('memory')

class MemoryManager:
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
        self.create_tables()
        self.embeddings = OpenAIEmbeddings()
        self.load_memories()

    def create_tables(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS memories (
                id INTEGER PRIMARY KEY,
                query TEXT,
                result TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        logger.debug("Ensured memories table exists")

    def save_memory(self, query: str, result: str):
        self.conn.execute("""
            INSERT INTO memories (query, result) VALUES (?, ?)
        """, (query, result))
        self.conn.commit()
        logger.debug(f"Saved memory for query: {query} with result: {result}")

    def load_memories(self):
        json_files = glob.glob("json_output/*.json")
        
        for file_path in json_files:
            with open(file_path, 'r') as json_file:
                memory_data = json.load(json_file)
                query = memory_data['query']
                result = memory_data['result']
                self.save_memory(query, result)
        
        logger.debug(f"Loaded {len(json_files)} memories from JSON files")

    def get_memories(self, limit: int = 10) -> List[Tuple[str, str]]: #? is this used? 
        cursor = self.conn.execute("""
            SELECT query, result FROM memories ORDER BY timestamp DESC LIMIT ?
        """, (limit,))
        memories = cursor.fetchall()
        logger.debug(f"Retrieved {len(memories)} memories")
        return memories

    def retrieve_relevant_memories(self, query: str, threshold: float = 0.75) -> List[Tuple[str, str]]:
        #! This is as of yet not scalable as the entire database is returned. 
        # To address this we will need to sort the sematic title search. Potentially use this as a hierarchy structure or temporal structure instead. 
        cursor = self.conn.execute("SELECT query, result FROM memories")
        all_memories = cursor.fetchall()
        
        query_embedding = self.embeddings.embed_query(query)
        relevant_memories = []
        memory_texts = set()
        
        for memory in all_memories:
            memory_query = memory[0]
            memory_result = memory[1]
            memory_text = f"{memory_query}\n{memory_result}"
            if memory_text in memory_texts:
                continue
            
            memory_embedding = self.embeddings.embed_query(memory_query)
            similarity = np.dot(query_embedding, memory_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(memory_embedding))
            
            if similarity >= threshold:
                relevant_memories.append((memory, similarity))
                memory_texts.add(memory_text)
        
        relevant_memories.sort(key=lambda x: x[1], reverse=True)
        return [memory[0] for memory in relevant_memories]
-e 
File: ./src/memory/__init__.py

-e 
File: ./src/utils/data_utils.py

# src/utils/data_utils.py

import logging
from typing import List, Dict, Tuple

logger = logging.getLogger('master')

def structure_memories(memories: List[Tuple[str, str]]) -> List[Dict[str, str]]:
    """
    Transforms a list of memory tuples into a structured list of dictionaries.

    Args:
        memories (List[Tuple[str, str]]): List of memories where each memory is a tuple (query, result).

    Returns:
        List[Dict[str, str]]: List of structured memories with titles, queries, results, and tags.
    """
    structured_memories = []
    for idx, (query, result) in enumerate(memories):
        memory_data = {
            "title": f"Memory_{idx + 1}",
            "query": query,
            "result": result,
            "tags": "retrieved, processed"
        }
        structured_memories.append(memory_data)
    return structured_memories
-e 
File: ./src/utils/file_manager.py

# src/utils/file_manager.py

import os
import json
from typing import Dict
import logging

logger = logging.getLogger('file_manager')

def save_json(data: Dict, filename: str, output_dir: str = 'json_output'):
    """
    Save a dictionary as a JSON file.
    
    Args:
        data (Dict): The data to be saved as JSON.
        filename (str): The name of the JSON file.
        output_dir (str): The directory where the JSON file will be saved.
    
    Returns:
        None
    """
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        output_path = os.path.join(output_dir, filename)
        
        with open(output_path, 'w') as json_file:
            json.dump(data, json_file, indent=4)
        
        logger.info(f"Saved JSON: {output_path}")
    except Exception as e:
        logger.error(f"Failed to save JSON {filename}: {e}")

def load_json(filename: str, output_dir: str = 'json_output') -> Dict:
    """
    Load a JSON file and return its content as a dictionary.
    
    Args:
        filename (str): The name of the JSON file.
        output_dir (str): The directory where the JSON file is located.
    
    Returns:
        Dict: The content of the JSON file.
    """
    try:
        output_path = os.path.join(output_dir, filename)
        
        with open(output_path, 'r') as json_file:
            data = json.load(json_file)
        
        logger.info(f"Loaded JSON: {output_path}")
        return data
    except Exception as e:
        logger.error(f"Failed to load JSON {filename}: {e}")
        return {}
-e 
File: ./src/utils/json_utils.py

# src/utils/json_utils.py

import os
import json
from typing import Dict
import logging

logger = logging.getLogger('json_utils')

def save_memory_to_json(memory_data: Dict[str, str], output_dir: str = 'json_output'):
    """
    Saves the structured memory data to a JSON file.
    
    Args:
        memory_data (Dict[str, str]): Dictionary containing the structured memory data.
        output_dir (str): Directory to save the JSON files.
    
    Returns:
        None
    """
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        output_path = os.path.join(output_dir, f"{memory_data['title']}.json")
        
        with open(output_path, 'w') as json_file:
            json.dump(memory_data, json_file, indent=4)
        
        logger.info(f"Generated JSON: {output_path}")
    except Exception as e:
        logger.error(f"Error saving JSON file {memory_data['title']}: {e}")
-e 
File: ./src/utils/__init__.py

-e 
File: ./src/__init__.py

