.
├── _Deprecated
├── app
│   ├── __init__.py
│   ├── agent.py
│   ├── api.py
│   ├── config.py
│   ├── db
│   │   ├── __init__.py
│   │   └── memory_manager.py
│   └── utils
│       ├── __init__.py
│       └── logging.py
├── data
├── logging_config.yaml
├── logs
│   └── app.log
├── project_structure.txt
├── requirements.txt
├── run.py
├── tests
│   ├── __init__.py
│   ├── test_agent.py
│   └── test_memory_manager.py
└── tree.sh

7 directories, 17 files

Note: The following list of files and directories are excluded only from the appended file contents section:

__pycache__, .git, .env, MLP_venv, *.db

File Contents:

-e 
File: ./app/agent.py

import json
import re
from together import Together
from app.db.memory_manager import MemoryManager
from app.config import config
from app.utils.logging import get_logger

logger = get_logger(__name__)

class Agent:
    def __init__(self, api_key: str, memory_manager: MemoryManager):
        self.together_client = Together(api_key=api_key)
        self.memory_manager = memory_manager

    async def process_query(self, query: str) -> str:
        # Check if the query is a special trivia request
        if self._is_trivia_request(query):
            return await self._process_trivia_request(query)
        
        relevant_memories = await self._retrieve_relevant_memories(query)
        response = await self._generate_response(query, relevant_memories)
        await self._update_memory(query, response)
        return response

    def _is_trivia_request(self, query: str) -> bool:
        pattern = r'^\s*Here are some trivia questions and answers for you to process\.'
        return bool(re.match(pattern, query, re.IGNORECASE))

    async def _process_trivia_request(self, query: str) -> str:
        # Extract answers from the query
        answers = re.findall(r'\bAnswer:\s*(.+?)(?=\n|$)', query, re.IGNORECASE)
        return json.dumps(answers)

    async def _retrieve_relevant_memories(self, query: str) -> list:
        relevant_memories = await self.memory_manager.get_relevant_memories(query, top_k=5)
        return [memory[0] for memory in relevant_memories]  # Return only the content

    async def _generate_response(self, query: str, relevant_memories: list) -> str:
        prompt = self._construct_prompt(query, relevant_memories)
        response = self.together_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=config.MODEL_NAME,
            # max_tokens=1000  # Add this to limit the response length
        )
        return response.choices[0].message.content

    async def _update_memory(self, query: str, response: str):
        memory_id = await self.memory_manager.add_memory(f"Query: {query}\nResponse: {response}")
        logger.info(f"Memory updated with ID: {memory_id}")

    def _construct_prompt(self, query: str, relevant_memories: list) -> str:
        memory_context = "\n".join([f"- {memory}" for memory in relevant_memories])
        return f"""Given the following context and query, provide a relevant and informative response:

Context:
{memory_context}

Query: {query}

Response:"""-e 
File: ./app/api.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from app.agent import Agent
from app.db.memory_manager import MemoryManager
from app.config import config
from app.utils.logging import get_logger

app = FastAPI()
logger = get_logger(__name__)

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    response: str

memory_manager = MemoryManager(config.DATABASE_URL)
memory_manager.initialize()
agent = Agent(config.TOGETHER_API_KEY, memory_manager)

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    try:
        logger.info(f"Received query: {request.query}")
        response = await agent.process_query(request.query)
        logger.info(f"Query processed successfully with response: {response}")
        return QueryResponse(response=response)
    except Exception as e:
        logger.error(f"Error processing query: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

@app.get("/health")
async def health_check():
    return {"status": "ok"}-e 
File: ./app/config.py

import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
    DATABASE_URL = 'sqlite:///./data/memories.db'
    LOG_FILE = './logs/app.log'
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
    MODEL_NAME = "meta-llama/Llama-3-70b-chat-hf"

config = Config()
-e 
File: ./app/db/memory_manager.py

from sqlalchemy import create_engine, Column, Integer, String, Float, ForeignKey, Table, select
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship, Session
from sqlalchemy.sql import func
from app.config import config
import numpy as np
from openai import AsyncOpenAI
from app.utils.logging import get_logger
from contextlib import contextmanager
import os

logger = get_logger(__name__)

Base = declarative_base()

memory_links = Table('memory_links', Base.metadata,
    Column('id', Integer, primary_key=True),
    Column('source_id', Integer, ForeignKey('memories.id')),
    Column('target_id', Integer, ForeignKey('memories.id'))
)

class Memory(Base):
    __tablename__ = 'memories'

    id = Column(Integer, primary_key=True)
    content = Column(String)
    embedding = Column(String)  # Store as comma-separated string
    timestamp = Column(Float, server_default=func.now())

    links = relationship('Memory', secondary=memory_links,
                         primaryjoin=id==memory_links.c.source_id,
                         secondaryjoin=id==memory_links.c.target_id)

class MemoryManager:
    def __init__(self, db_url: str):
        self.engine = create_engine(db_url)
        self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
        
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        self.openai_client = AsyncOpenAI(api_key=openai_api_key)
        
        logger.info("MemoryManager initialized with OpenAI embeddings")
        
    def initialize(self):
        Base.metadata.create_all(bind=self.engine)
        logger.info("Database initialized")

    @contextmanager
    def get_db(self):
        db = self.SessionLocal()
        try:
            yield db
        finally:
            db.close()

    async def add_memory(self, content: str):
        logger.debug(f"Adding new memory: {content[:50]}...")  # Log first 50 chars
        embedding = await self._get_embedding(content)
        with self.get_db() as db:
            new_memory = Memory(content=content, embedding=','.join(map(str, embedding)))
            db.add(new_memory)
            db.commit()
            db.refresh(new_memory)
            memory_id = new_memory.id  # Get the id within the session
            self._update_links(db, new_memory)
        logger.info(f"Memory added successfully: ID {memory_id}")
        return memory_id

    async def get_relevant_memories(self, query: str, top_k: int = 5):
        logger.debug(f"Retrieving relevant memories for query: {query[:50]}...")
        query_embedding = await self._get_embedding(query)
        with self.get_db() as db:
            memories = db.query(Memory).all()
            similarities = [self._cosine_similarity(query_embedding, np.fromstring(m.embedding, sep=',')) for m in memories]
            sorted_memories = sorted(zip(memories, similarities), key=lambda x: x[1], reverse=True)[:top_k]
            result = [(m.content, sim) for m, sim in sorted_memories]
        logger.info(f"Retrieved {len(result)} relevant memories")
        return result

    def _update_links(self, db: Session, new_memory: Memory):
        logger.debug(f"Updating links for memory: ID {new_memory.id}")
        all_memories = db.query(Memory).filter(Memory.id != new_memory.id).all()
        
        for memory in all_memories:
            similarity = self._cosine_similarity(
                np.fromstring(new_memory.embedding, sep=','),
                np.fromstring(memory.embedding, sep=',')
            )
            if similarity > 0.8:  # Threshold for linking
                new_memory.links.append(memory)
                logger.debug(f"Linked memory ID {memory.id} to new memory ID {new_memory.id}")
        db.commit()

    async def _get_embedding(self, text: str) -> np.ndarray:
        logger.debug("Generating embedding using OpenAI")
        try:
            response = await self.openai_client.embeddings.create(
                input=[text],
                model="text-embedding-ada-002"  # Use the appropriate OpenAI embedding model
            )
            embedding = response.data[0].embedding
            logger.debug("Embedding generated successfully")
            return np.array(embedding, dtype=np.float32)
        except Exception as e:
            logger.error(f"Error generating embedding: {str(e)}")
            raise

    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))-e 
File: ./app/db/__init__.py

-e 
File: ./app/utils/logging.py

import logging

def get_logger(name: str):
    return logging.getLogger(name)-e 
File: ./app/utils/__init__.py

-e 
File: ./app/__init__.py

-e 
File: ./logging_config.yaml

version: 1
disable_existing_loggers: False
formatters:
  default:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
handlers:
  console:
    class: logging.StreamHandler
    level: INFO
    formatter: default
    stream: ext://sys.stdout
  file:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: default
    filename: logs/app.log
    maxBytes: 10485760 # 10MB
    backupCount: 5
loggers:
  uvicorn:
    level: INFO
  uvicorn.access:
    level: INFO
root:
  level: DEBUG
  handlers: [console, file]-e 
File: ./logs/app.log

2024-07-17 11:51:49,854 - __main__ - INFO - Starting the application.
2024-07-17 11:51:49,860 - uvicorn.error - INFO - Will watch for changes in these directories: ['/mnt/c/Users/fkgde/Desktop/GoodAI/__FULLTIME/LTM-Benchmark/goodai-ltm-benchmark/MLP_LTM']
2024-07-17 11:51:49,860 - uvicorn.error - INFO - Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
2024-07-17 11:51:49,861 - uvicorn.error - INFO - Started reloader process [24475] using WatchFiles
2024-07-17 11:51:59,624 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-07-17 11:51:59,626 - httpx - DEBUG - load_verify_locations cafile='/mnt/c/Users/fkgde/Desktop/GoodAI/__FULLTIME/LTM-Benchmark/goodai-ltm-benchmark/MLP_LTM/MLP_venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-07-17 11:51:59,673 - app.db.memory_manager - INFO - MemoryManager initialized with OpenAI embeddings
2024-07-17 11:51:59,696 - app.db.memory_manager - INFO - Database initialized
2024-07-17 11:51:59,698 - uvicorn.error - INFO - Started server process [24481]
2024-07-17 11:51:59,699 - uvicorn.error - INFO - Waiting for application startup.
2024-07-17 11:51:59,702 - uvicorn.error - INFO - Application startup complete.
2024-07-17 11:52:12,550 - watchfiles.main - DEBUG - 5 changes detected: {(<Change.modified: 2>, '/mnt/c/Users/fkgde/Desktop/GoodAI/__FULLTIME/LTM-Benchmark/goodai-ltm-benchmark/MLP_LTM/app/db/__pycache__'), (<Change.modified: 2>, '/mnt/c/Users/fkgde/Desktop/GoodAI/__FULLTIME/LTM-Benchmark/goodai-ltm-benchmark/MLP_LTM/app/db/__pycache__/memory_manager.cpython-312.pyc'), (<Change.modified: 2>, '/mnt/c/Users/fkgde/Desktop/GoodAI/__FULLTIME/LTM-Benchmark/goodai-ltm-benchmark/MLP_LTM/logs/app.log'), (<Change.modified: 2>, '/mnt/c/Users/fkgde/Desktop/GoodAI/__FULLTIME/LTM-Benchmark/goodai-ltm-benchmark/MLP_LTM/data/memories.db'), (<Change.modified: 2>, '/mnt/c/Users/fkgde/Desktop/GoodAI/__FULLTIME/LTM-Benchmark/goodai-ltm-benchmark/MLP_LTM/data')}
2024-07-17 11:52:19,964 - app.api - INFO - Received query: I am going to subject you to a Long-Term Memory benchmark. In the following, I will be giving you different kinds of information and I expect you to answer extremely briefly, only providing the responses that you are required to provide. Otherwise, provide just short confirmations. Understood?
2024-07-17 11:52:19,965 - app.db.memory_manager - DEBUG - Retrieving relevant memories for query: I am going to subject you to a Long-Term Memory be...
2024-07-17 11:52:19,966 - app.db.memory_manager - DEBUG - Generating embedding using OpenAI
2024-07-17 11:52:20,006 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function AsyncEmbeddings.create.<locals>.parser at 0x7f52922ea480>, 'json_data': {'input': ['I am going to subject you to a Long-Term Memory benchmark. In the following, I will be giving you different kinds of information and I expect you to answer extremely briefly, only providing the responses that you are required to provide. Otherwise, provide just short confirmations. Understood?'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-07-17 11:52:20,014 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-07-17 11:52:20,075 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f52922d3c80>
2024-07-17 11:52:20,076 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f52922dc250> server_hostname='api.openai.com' timeout=5.0
2024-07-17 11:52:20,090 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f52922d0bc0>
2024-07-17 11:52:20,091 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-07-17 11:52:20,092 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-07-17 11:52:20,093 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-07-17 11:52:20,093 - httpcore.http11 - DEBUG - send_request_body.complete
2024-07-17 11:52:20,094 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-07-17 11:52:20,387 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 17 Jul 2024 10:52:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'goodai-research'), (b'openai-processing-ms', b'23'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999927'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_798cda9832c3096310af32db9ca5053e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=0y0lwp8vaTEIN1qK08Vn7v6FOqei4lm_FlsLCq3nMNs-1721213540-1.0.1.1-KxWwtgk5XB69Omjce.OdiIGz3rSanwUpE13UU2yQOzdqioIO3MRs3QX6vj2baMQ5sBTsnHIFwmuorOwZUGXVNg; path=/; expires=Wed, 17-Jul-24 11:22:20 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=gPuV.cmtuAtFhadVfsgc7g_lebLbWDEdgjM8nnNoriM-1721213540267-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a49ae90eca635ef-MAN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-07-17 11:52:20,388 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-07-17 11:52:20,389 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-07-17 11:52:20,430 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-07-17 11:52:20,431 - httpcore.http11 - DEBUG - response_closed.started
2024-07-17 11:52:20,431 - httpcore.http11 - DEBUG - response_closed.complete
2024-07-17 11:52:20,432 - openai._base_client - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2024-07-17 11:52:20,433 - app.db.memory_manager - DEBUG - Embedding generated successfully
2024-07-17 11:52:20,439 - app.db.memory_manager - INFO - Retrieved 0 relevant memories
2024-07-17 11:52:20,440 - together - DEBUG - headers='{"X-Together-Client-User-Agent": "{\\"bindings_version\\": \\"1.2.1\\", \\"httplib\\": \\"requests\\", \\"lang\\": \\"python\\", \\"lang_version\\": \\"3.12.4\\", \\"platform\\": \\"Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\\", \\"publisher\\": \\"together\\", \\"uname\\": \\"Linux 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64\\"}", "Authorization": "Bearer 4a26b3919abc69391758674bfbedb03fcf91281374318ae1046ba3b3623b3750", "User-Agent": "Together/v1 PythonBindings/1.2.1", "Content-Type": "application/json"}' message='Request to Together API' method=POST path=https://api.together.xyz/v1/chat/completions post_data='{"messages": [{"role": "user", "content": "Given the following context and query, provide a relevant and informative response:\\n\\nContext:\\n\\n\\nQuery: I am going to subject you to a Long-Term Memory benchmark. In the following, I will be giving you different kinds of information and I expect you to answer extremely briefly, only providing the responses that you are required to provide. Otherwise, provide just short confirmations. Understood?\\n\\nResponse:", "tool_calls": null}], "model": "meta-llama/Llama-3-70b-chat-hf", "max_tokens": null, "stop": null, "temperature": null, "top_p": null, "top_k": null, "repetition_penalty": null, "presence_penalty": null, "frequency_penalty": null, "min_p": null, "logit_bias": null, "stream": false, "logprobs": null, "echo": null, "n": null, "safety_model": null, "response_format": null, "tools": null, "tool_choice": null}'
2024-07-17 11:52:20,441 - urllib3.util.retry - DEBUG - Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)
2024-07-17 11:52:20,443 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.together.xyz:443
2024-07-17 11:52:21,687 - urllib3.connectionpool - DEBUG - https://api.together.xyz:443 "POST /v1/chat/completions HTTP/11" 200 None
2024-07-17 11:52:21,688 - together - DEBUG - message='Together API response' path=https://api.together.xyz/v1/chat/completions processing_ms=867 request_id=8a49ae93998ab314-MAN response_code=200
2024-07-17 11:52:21,689 - app.db.memory_manager - DEBUG - Adding new memory: Query: I am going to subject you to a Long-Term Me...
2024-07-17 11:52:21,689 - app.db.memory_manager - DEBUG - Generating embedding using OpenAI
2024-07-17 11:52:21,690 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function AsyncEmbeddings.create.<locals>.parser at 0x7f52923825c0>, 'json_data': {'input': ['Query: I am going to subject you to a Long-Term Memory benchmark. In the following, I will be giving you different kinds of information and I expect you to answer extremely briefly, only providing the responses that you are required to provide. Otherwise, provide just short confirmations. Understood?\nResponse: Understood.'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-07-17 11:52:21,691 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-07-17 11:52:21,692 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-07-17 11:52:21,693 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-07-17 11:52:21,693 - httpcore.http11 - DEBUG - send_request_body.complete
2024-07-17 11:52:21,694 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-07-17 11:52:21,900 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 17 Jul 2024 10:52:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'goodai-research'), (b'openai-processing-ms', b'29'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999920'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_ace2bf886fe78d11ff0823970e0ae5d7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a49ae9ae93b35ef-MAN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-07-17 11:52:21,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-07-17 11:52:21,901 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-07-17 11:52:21,914 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-07-17 11:52:21,915 - httpcore.http11 - DEBUG - response_closed.started
2024-07-17 11:52:21,916 - httpcore.http11 - DEBUG - response_closed.complete
2024-07-17 11:52:21,917 - openai._base_client - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2024-07-17 11:52:21,918 - app.db.memory_manager - DEBUG - Embedding generated successfully
2024-07-17 11:52:21,943 - app.db.memory_manager - DEBUG - Updating links for memory: ID 1
2024-07-17 11:52:21,946 - app.db.memory_manager - INFO - Memory added successfully: ID 1
2024-07-17 11:52:21,946 - app.agent - INFO - Memory updated with ID: 1
2024-07-17 11:52:21,947 - app.api - INFO - Query processed successfully with response: Understood.
2024-07-17 11:52:21,947 - uvicorn.access - INFO - 127.0.0.1:37546 - "POST /query HTTP/1.1" 200
2024-07-17 11:52:23,987 - app.api - INFO - Received query: My favourite colour is Purple.
2024-07-17 11:52:23,988 - app.db.memory_manager - DEBUG - Retrieving relevant memories for query: My favourite colour is Purple....
2024-07-17 11:52:23,988 - app.db.memory_manager - DEBUG - Generating embedding using OpenAI
2024-07-17 11:52:23,989 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function AsyncEmbeddings.create.<locals>.parser at 0x7f5292383e20>, 'json_data': {'input': ['My favourite colour is Purple.'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-07-17 11:52:23,990 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-07-17 11:52:23,991 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-07-17 11:52:23,992 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-07-17 11:52:23,992 - httpcore.http11 - DEBUG - send_request_body.complete
2024-07-17 11:52:23,993 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-07-17 11:52:24,213 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 17 Jul 2024 10:52:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'goodai-research'), (b'openai-processing-ms', b'24'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999993'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_7b268f6edf0cc6676447d2dd4ae2d852'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a49aea94a3335ef-MAN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-07-17 11:52:24,214 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-07-17 11:52:24,214 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-07-17 11:52:24,255 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-07-17 11:52:24,255 - httpcore.http11 - DEBUG - response_closed.started
2024-07-17 11:52:24,256 - httpcore.http11 - DEBUG - response_closed.complete
2024-07-17 11:52:24,257 - openai._base_client - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2024-07-17 11:52:24,258 - app.db.memory_manager - DEBUG - Embedding generated successfully
2024-07-17 11:52:24,260 - app.db.memory_manager - INFO - Retrieved 1 relevant memories
2024-07-17 11:52:24,261 - together - DEBUG - headers='{"X-Together-Client-User-Agent": "{\\"bindings_version\\": \\"1.2.1\\", \\"httplib\\": \\"requests\\", \\"lang\\": \\"python\\", \\"lang_version\\": \\"3.12.4\\", \\"platform\\": \\"Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\\", \\"publisher\\": \\"together\\", \\"uname\\": \\"Linux 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64\\"}", "Authorization": "Bearer 4a26b3919abc69391758674bfbedb03fcf91281374318ae1046ba3b3623b3750", "User-Agent": "Together/v1 PythonBindings/1.2.1", "Content-Type": "application/json"}' message='Request to Together API' method=POST path=https://api.together.xyz/v1/chat/completions post_data='{"messages": [{"role": "user", "content": "Given the following context and query, provide a relevant and informative response:\\n\\nContext:\\n- Query: I am going to subject you to a Long-Term Memory benchmark. In the following, I will be giving you different kinds of information and I expect you to answer extremely briefly, only providing the responses that you are required to provide. Otherwise, provide just short confirmations. Understood?\\nResponse: Understood.\\n\\nQuery: My favourite colour is Purple.\\n\\nResponse:", "tool_calls": null}], "model": "meta-llama/Llama-3-70b-chat-hf", "max_tokens": null, "stop": null, "temperature": null, "top_p": null, "top_k": null, "repetition_penalty": null, "presence_penalty": null, "frequency_penalty": null, "min_p": null, "logit_bias": null, "stream": false, "logprobs": null, "echo": null, "n": null, "safety_model": null, "response_format": null, "tools": null, "tool_choice": null}'
2024-07-17 11:52:24,809 - urllib3.connectionpool - DEBUG - https://api.together.xyz:443 "POST /v1/chat/completions HTTP/11" 200 None
2024-07-17 11:52:24,810 - together - DEBUG - message='Together API response' path=https://api.together.xyz/v1/chat/completions processing_ms=302 request_id=8a49aeaaf973b314-MAN response_code=200
2024-07-17 11:52:24,811 - app.db.memory_manager - DEBUG - Adding new memory: Query: My favourite colour is Purple.
Response: Co...
2024-07-17 11:52:24,811 - app.db.memory_manager - DEBUG - Generating embedding using OpenAI
2024-07-17 11:52:24,812 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function AsyncEmbeddings.create.<locals>.parser at 0x7f52c62acd60>, 'json_data': {'input': ['Query: My favourite colour is Purple.\nResponse: Confirmed.'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-07-17 11:52:24,813 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-07-17 11:52:24,814 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-07-17 11:52:24,814 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-07-17 11:52:24,815 - httpcore.http11 - DEBUG - send_request_body.complete
2024-07-17 11:52:24,816 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-07-17 11:52:25,020 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 17 Jul 2024 10:52:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'goodai-research'), (b'openai-processing-ms', b'25'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999986'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_ba92f823a2cb60668d671b8bd016c365'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a49aeae680e35ef-MAN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-07-17 11:52:25,021 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-07-17 11:52:25,021 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-07-17 11:52:25,022 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-07-17 11:52:25,023 - httpcore.http11 - DEBUG - response_closed.started
2024-07-17 11:52:25,023 - httpcore.http11 - DEBUG - response_closed.complete
2024-07-17 11:52:25,024 - openai._base_client - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2024-07-17 11:52:25,025 - app.db.memory_manager - DEBUG - Embedding generated successfully
2024-07-17 11:52:25,050 - app.db.memory_manager - DEBUG - Updating links for memory: ID 2
2024-07-17 11:52:25,052 - app.db.memory_manager - INFO - Memory added successfully: ID 2
2024-07-17 11:52:25,052 - app.agent - INFO - Memory updated with ID: 2
2024-07-17 11:52:25,053 - app.api - INFO - Query processed successfully with response: Confirmed.
2024-07-17 11:52:25,054 - uvicorn.access - INFO - 127.0.0.1:35710 - "POST /query HTTP/1.1" 200
2024-07-17 11:52:27,085 - app.api - INFO - Received query: Atheism is a non-prophet organization
2024-07-17 11:52:27,086 - app.db.memory_manager - DEBUG - Retrieving relevant memories for query: Atheism is a non-prophet organization...
2024-07-17 11:52:27,086 - app.db.memory_manager - DEBUG - Generating embedding using OpenAI
2024-07-17 11:52:27,087 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function AsyncEmbeddings.create.<locals>.parser at 0x7f5292383ce0>, 'json_data': {'input': ['Atheism is a non-prophet organization'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-07-17 11:52:27,088 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-07-17 11:52:27,089 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-07-17 11:52:27,089 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-07-17 11:52:27,090 - httpcore.http11 - DEBUG - send_request_body.complete
2024-07-17 11:52:27,090 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-07-17 11:52:27,314 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 17 Jul 2024 10:52:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'goodai-research'), (b'openai-processing-ms', b'80'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999991'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_2fddc174bd4122992481e3e3e3bbbd97'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a49aebcaa2935ef-MAN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-07-17 11:52:27,315 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-07-17 11:52:27,316 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-07-17 11:52:27,338 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-07-17 11:52:27,338 - httpcore.http11 - DEBUG - response_closed.started
2024-07-17 11:52:27,339 - httpcore.http11 - DEBUG - response_closed.complete
2024-07-17 11:52:27,340 - openai._base_client - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2024-07-17 11:52:27,340 - app.db.memory_manager - DEBUG - Embedding generated successfully
2024-07-17 11:52:27,343 - app.db.memory_manager - INFO - Retrieved 2 relevant memories
2024-07-17 11:52:27,343 - together - DEBUG - headers='{"X-Together-Client-User-Agent": "{\\"bindings_version\\": \\"1.2.1\\", \\"httplib\\": \\"requests\\", \\"lang\\": \\"python\\", \\"lang_version\\": \\"3.12.4\\", \\"platform\\": \\"Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\\", \\"publisher\\": \\"together\\", \\"uname\\": \\"Linux 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64\\"}", "Authorization": "Bearer 4a26b3919abc69391758674bfbedb03fcf91281374318ae1046ba3b3623b3750", "User-Agent": "Together/v1 PythonBindings/1.2.1", "Content-Type": "application/json"}' message='Request to Together API' method=POST path=https://api.together.xyz/v1/chat/completions post_data='{"messages": [{"role": "user", "content": "Given the following context and query, provide a relevant and informative response:\\n\\nContext:\\n- Query: I am going to subject you to a Long-Term Memory benchmark. In the following, I will be giving you different kinds of information and I expect you to answer extremely briefly, only providing the responses that you are required to provide. Otherwise, provide just short confirmations. Understood?\\nResponse: Understood.\\n- Query: My favourite colour is Purple.\\nResponse: Confirmed.\\n\\nQuery: Atheism is a non-prophet organization\\n\\nResponse:", "tool_calls": null}], "model": "meta-llama/Llama-3-70b-chat-hf", "max_tokens": null, "stop": null, "temperature": null, "top_p": null, "top_k": null, "repetition_penalty": null, "presence_penalty": null, "frequency_penalty": null, "min_p": null, "logit_bias": null, "stream": false, "logprobs": null, "echo": null, "n": null, "safety_model": null, "response_format": null, "tools": null, "tool_choice": null}'
2024-07-17 11:52:27,907 - urllib3.connectionpool - DEBUG - https://api.together.xyz:443 "POST /v1/chat/completions HTTP/11" 200 None
2024-07-17 11:52:27,908 - together - DEBUG - message='Together API response' path=https://api.together.xyz/v1/chat/completions processing_ms=306 request_id=8a49aebe3e25b314-MAN response_code=200
2024-07-17 11:52:27,908 - app.db.memory_manager - DEBUG - Adding new memory: Query: Atheism is a non-prophet organization
Respo...
2024-07-17 11:52:27,909 - app.db.memory_manager - DEBUG - Generating embedding using OpenAI
2024-07-17 11:52:27,910 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function AsyncEmbeddings.create.<locals>.parser at 0x7f52922ea2a0>, 'json_data': {'input': ['Query: Atheism is a non-prophet organization\nResponse: Confirmed.'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-07-17 11:52:27,911 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-07-17 11:52:27,912 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-07-17 11:52:27,912 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-07-17 11:52:27,913 - httpcore.http11 - DEBUG - send_request_body.complete
2024-07-17 11:52:27,914 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-07-17 11:52:28,115 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 17 Jul 2024 10:52:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'goodai-research'), (b'openai-processing-ms', b'19'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999984'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_c590325eaf6c265cf137f8eb3521a349'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a49aec1c81e35ef-MAN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-07-17 11:52:28,115 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-07-17 11:52:28,116 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-07-17 11:52:28,145 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-07-17 11:52:28,146 - httpcore.http11 - DEBUG - response_closed.started
2024-07-17 11:52:28,146 - httpcore.http11 - DEBUG - response_closed.complete
2024-07-17 11:52:28,147 - openai._base_client - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2024-07-17 11:52:28,148 - app.db.memory_manager - DEBUG - Embedding generated successfully
2024-07-17 11:52:28,170 - app.db.memory_manager - DEBUG - Updating links for memory: ID 3
2024-07-17 11:52:28,172 - app.db.memory_manager - INFO - Memory added successfully: ID 3
2024-07-17 11:52:28,173 - app.agent - INFO - Memory updated with ID: 3
2024-07-17 11:52:28,173 - app.api - INFO - Query processed successfully with response: Confirmed.
2024-07-17 11:52:28,174 - uvicorn.access - INFO - 127.0.0.1:35726 - "POST /query HTTP/1.1" 200
2024-07-17 11:52:30,205 - app.api - INFO - Received query: There is a Museum in the center of my hometown.
2024-07-17 11:52:30,206 - app.db.memory_manager - DEBUG - Retrieving relevant memories for query: There is a Museum in the center of my hometown....
2024-07-17 11:52:30,207 - app.db.memory_manager - DEBUG - Generating embedding using OpenAI
2024-07-17 11:52:30,207 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function AsyncEmbeddings.create.<locals>.parser at 0x7f5292383e20>, 'json_data': {'input': ['There is a Museum in the center of my hometown.'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-07-17 11:52:30,208 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-07-17 11:52:30,209 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-07-17 11:52:30,210 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-07-17 11:52:30,210 - httpcore.http11 - DEBUG - send_request_body.complete
2024-07-17 11:52:30,211 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-07-17 11:52:30,437 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 17 Jul 2024 10:52:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'goodai-research'), (b'openai-processing-ms', b'21'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999989'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_b17d4051df16ba4008ff641a86331f2c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a49aed029a435ef-MAN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-07-17 11:52:30,438 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-07-17 11:52:30,438 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-07-17 11:52:30,457 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-07-17 11:52:30,458 - httpcore.http11 - DEBUG - response_closed.started
2024-07-17 11:52:30,458 - httpcore.http11 - DEBUG - response_closed.complete
2024-07-17 11:52:30,459 - openai._base_client - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2024-07-17 11:52:30,460 - app.db.memory_manager - DEBUG - Embedding generated successfully
2024-07-17 11:52:30,463 - app.db.memory_manager - INFO - Retrieved 3 relevant memories
2024-07-17 11:52:30,463 - together - DEBUG - headers='{"X-Together-Client-User-Agent": "{\\"bindings_version\\": \\"1.2.1\\", \\"httplib\\": \\"requests\\", \\"lang\\": \\"python\\", \\"lang_version\\": \\"3.12.4\\", \\"platform\\": \\"Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\\", \\"publisher\\": \\"together\\", \\"uname\\": \\"Linux 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64\\"}", "Authorization": "Bearer 4a26b3919abc69391758674bfbedb03fcf91281374318ae1046ba3b3623b3750", "User-Agent": "Together/v1 PythonBindings/1.2.1", "Content-Type": "application/json"}' message='Request to Together API' method=POST path=https://api.together.xyz/v1/chat/completions post_data='{"messages": [{"role": "user", "content": "Given the following context and query, provide a relevant and informative response:\\n\\nContext:\\n- Query: My favourite colour is Purple.\\nResponse: Confirmed.\\n- Query: Atheism is a non-prophet organization\\nResponse: Confirmed.\\n- Query: I am going to subject you to a Long-Term Memory benchmark. In the following, I will be giving you different kinds of information and I expect you to answer extremely briefly, only providing the responses that you are required to provide. Otherwise, provide just short confirmations. Understood?\\nResponse: Understood.\\n\\nQuery: There is a Museum in the center of my hometown.\\n\\nResponse:", "tool_calls": null}], "model": "meta-llama/Llama-3-70b-chat-hf", "max_tokens": null, "stop": null, "temperature": null, "top_p": null, "top_k": null, "repetition_penalty": null, "presence_penalty": null, "frequency_penalty": null, "min_p": null, "logit_bias": null, "stream": false, "logprobs": null, "echo": null, "n": null, "safety_model": null, "response_format": null, "tools": null, "tool_choice": null}'
2024-07-17 11:52:31,118 - urllib3.connectionpool - DEBUG - https://api.together.xyz:443 "POST /v1/chat/completions HTTP/11" 200 None
2024-07-17 11:52:31,118 - together - DEBUG - message='Together API response' path=https://api.together.xyz/v1/chat/completions processing_ms=228 request_id=8a49aed1bb64b314-MAN response_code=200
2024-07-17 11:52:31,119 - app.db.memory_manager - DEBUG - Adding new memory: Query: There is a Museum in the center of my homet...
2024-07-17 11:52:31,120 - app.db.memory_manager - DEBUG - Generating embedding using OpenAI
2024-07-17 11:52:31,120 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function AsyncEmbeddings.create.<locals>.parser at 0x7f52c62acd60>, 'json_data': {'input': ['Query: There is a Museum in the center of my hometown.\nResponse: Confirmed.'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-07-17 11:52:31,122 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-07-17 11:52:31,122 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-07-17 11:52:31,123 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-07-17 11:52:31,123 - httpcore.http11 - DEBUG - send_request_body.complete
2024-07-17 11:52:31,124 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-07-17 11:52:31,331 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 17 Jul 2024 10:52:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'goodai-research'), (b'openai-processing-ms', b'21'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999982'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_07a7ee5dd749382f445db2cf81ce9105'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8a49aed5d86b35ef-MAN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-07-17 11:52:31,332 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-07-17 11:52:31,333 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-07-17 11:52:31,334 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-07-17 11:52:31,334 - httpcore.http11 - DEBUG - response_closed.started
2024-07-17 11:52:31,335 - httpcore.http11 - DEBUG - response_closed.complete
2024-07-17 11:52:31,335 - openai._base_client - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2024-07-17 11:52:31,336 - app.db.memory_manager - DEBUG - Embedding generated successfully
2024-07-17 11:52:31,360 - app.db.memory_manager - DEBUG - Updating links for memory: ID 4
2024-07-17 11:52:31,365 - app.db.memory_manager - DEBUG - Linked memory ID 2 to new memory ID 4
2024-07-17 11:52:31,375 - app.db.memory_manager - INFO - Memory added successfully: ID 4
2024-07-17 11:52:31,375 - app.agent - INFO - Memory updated with ID: 4
2024-07-17 11:52:31,376 - app.api - INFO - Query processed successfully with response: Confirmed.
2024-07-17 11:52:31,376 - uvicorn.access - INFO - 127.0.0.1:35734 - "POST /query HTTP/1.1" 200
2024-07-17 11:52:33,025 - uvicorn.error - INFO - Shutting down
2024-07-17 11:52:33,126 - uvicorn.error - INFO - Waiting for application shutdown.
2024-07-17 11:52:33,126 - uvicorn.error - INFO - Application shutdown complete.
2024-07-17 11:52:33,127 - uvicorn.error - INFO - Finished server process [24481]
2024-07-17 11:52:33,148 - uvicorn.error - INFO - Stopping reloader process [24475]
-e 
File: ./requirements.txt

aiohttp==3.9.5
aiosignal==1.3.1
annotated-types==0.7.0
anyio==4.4.0
asyncio==3.4.3
attrs==23.2.0
certifi==2024.7.4
charset-normalizer==3.3.2
click==8.1.7
dnspython==2.6.1
email_validator==2.2.0
eval_type_backport==0.2.0
fastapi==0.111.1
fastapi-cli==0.0.4
filelock==3.15.4
frozenlist==1.4.1
greenlet==3.0.3
h11==0.14.0
httpcore==1.0.5
httptools==0.6.1
httpx==0.27.0
idna==3.7
iniconfig==2.0.0
Jinja2==3.1.4
markdown-it-py==3.0.0
MarkupSafe==2.1.5
mdurl==0.1.2
multidict==6.0.5
numpy==2.0.0
packaging==24.1
pillow==10.4.0
pluggy==1.5.0
pyarrow==16.1.0
pydantic==2.8.2
pydantic_core==2.20.1
Pygments==2.18.0
pytest==8.2.2
pytest-asyncio==0.23.7
python-dotenv==1.0.1
python-multipart==0.0.9
PyYAML==6.0.1
requests==2.32.3
rich==13.7.1
shellingham==1.5.4
sniffio==1.3.1
SQLAlchemy==2.0.31
starlette==0.37.2
tabulate==0.9.0
together==1.2.1
tqdm==4.66.4
typer==0.12.3
typing_extensions==4.12.2
urllib3==2.2.2
uvicorn==0.30.1
uvloop==0.19.0
watchfiles==0.22.0
websockets==12.0
yarl==1.9.4
python-json-logger==2.0.7-e 
File: ./run.py

import os
import shutil
import argparse
import datetime
import uvicorn
from app.utils.logging import get_logger
import yaml
import logging.config

def move_old_files_to_deprecated():
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    deprecated_dir = os.path.join("../_Deprecated", timestamp)
    
    os.makedirs(deprecated_dir, exist_ok=True)
    
    items_to_move = [
        "./data/memories.db",  # Update this to match your actual DB file pattern if needed
        "./logs/app.log"
    ]
    
    for item in items_to_move:
        if os.path.exists(item):
            shutil.move(item, deprecated_dir)

def main():
    parser = argparse.ArgumentParser(description="Run the application with optional fresh start.")
    parser.add_argument("--new", action="store_true", help="Move old logs and database to a deprecated folder and start fresh.")
    
    args = parser.parse_args()
    
    if args.new:
        move_old_files_to_deprecated()
    
    # Ensure directories exist
    os.makedirs("./data", exist_ok=True)
    os.makedirs("./logs", exist_ok=True)

    # Create a new empty database file if needed
    new_db_path = "./data/memories.db"
    if not os.path.exists(new_db_path):
        open(new_db_path, 'w').close()

    with open('logging_config.yaml', 'r') as f:
        config = yaml.safe_load(f.read())
        logging.config.dictConfig(config)

    logger = get_logger(__name__)
    logger.info("Starting the application.")

    # Run the application with uvicorn
    uvicorn.run(
        "app.api:app",
        host="0.0.0.0",
        port=8080,
        reload=True,
        log_config="logging_config.yaml"
    )
if __name__ == "__main__":
    main()
-e 
File: ./tests/test_agent.py

import pytest
from unittest.mock import AsyncMock, patch
from app.agent import Agent
from app.db.memory_manager import MemoryManager

@pytest.fixture
def agent():
    memory_manager = AsyncMock(spec=MemoryManager)
    memory_manager.get_relevant_memories.return_value = [
        ("Memory 1 content", 0.9),
        ("Memory 2 content", 0.8)
    ]
    return Agent("fake_api_key", memory_manager)

@pytest.mark.asyncio
async def test_process_query(agent):
    with patch.object(agent.together_client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value.choices[0].message.content = "Mocked response"
        
        response = await agent.process_query("What is the capital of France?")
        
        assert response == "Mocked response"
        agent.memory_manager.get_relevant_memories.assert_called_once()
        agent.memory_manager.add_memory.assert_called_once()
        mock_create.assert_called_once()-e 
File: ./tests/test_memory_manager.py

import pytest
from app.db.memory_manager import MemoryManager

@pytest.fixture
def memory_manager():
    return MemoryManager("sqlite:///:memory:")

@pytest.mark.asyncio
async def test_add_and_retrieve_memory(memory_manager):
    await memory_manager.add_memory("Test memory content")
    memories = await memory_manager.get_relevant_memories("Test", top_k=1)
    assert len(memories) == 1
    assert memories[0][0] == "Test memory content"

@pytest.mark.asyncio
async def test_memory_linking(memory_manager):
    await memory_manager.add_memory("The capital of France is Paris.")
    await memory_manager.add_memory("Paris is known for the Eiffel Tower.")
    
    memories = await memory_manager.get_relevant_memories("What is the capital of France?", top_k=2)
    assert len(memories) == 2
    assert any("capital of France" in memory[0] for memory in memories)
    assert any("Eiffel Tower" in memory[0] for memory in memories)
-e 
File: ./tests/__init__.py

-e 
File: ./tree.sh

#!/bin/bash

# File where the tree structure will be stored
OUTPUT_FILE="project_structure.txt"

# Clear the previous contents of the output file
> "$OUTPUT_FILE"

# Generate tree structure and append to the output file
tree -I 'MLP_venv|__pycache__|*.db' >> "$OUTPUT_FILE"

# Add a note about the exclusions for appended file contents
echo -e "\nNote: The following list of files and directories are excluded only from the appended file contents section:\n" >> "$OUTPUT_FILE"
echo -e "__pycache__, .git, .env, MLP_venv, *.db\n" >> "$OUTPUT_FILE"

# Append file contents, excluding specified files and directories
echo -e "File Contents:\n" >> "$OUTPUT_FILE"
find . \( -name 'project_structure.txt' -o -name '*.db' -o -name '.env' -o -path '*/__pycache__/*' -o -path '*/.git/*' -o -path '*/MLP_venv/*' \) -prune -o -type f -print0 | xargs -0 -I {} sh -c 'echo -e "\nFile: {}\n"; cat "{}"' >> "$OUTPUT_FILE"

# Copy the output file contents to the clipboard
if command -v xclip &> /dev/null; then
    # Use xclip for Linux
    xclip -selection clipboard < "$OUTPUT_FILE"
elif command -v pbcopy &> /dev/null; then
    # Use pbcopy for macOS
    pbcopy < "$OUTPUT_FILE"
else
    echo "Clipboard copy command not found. Please install xclip (Linux) or pbcopy (macOS) to enable this feature."
fi

echo "Project structure and file contents have been written to $OUTPUT_FILE and copied to the clipboard."
