### File: agent.py ###
import json
import re
import uuid
from dataclasses import dataclass, field
import datetime
from math import ceil
import unicodedata
from typing import Optional, Callable, Any, List, Tuple, Union

from goodai.helpers.json_helper import sanitize_and_parse_json, SimpleJSONEncoder, SimpleJSONDecoder
from goodai.ltm.mem.base import RetrievedMemory
from goodai.ltm.mem.config import TextMemoryConfig
from litellm import token_counter
from model_interfaces.base_ltm_agent import Message

from utils.llm import make_system_message, make_user_message, ask_llm, log_llm_call
from utils.text import td_format
from utils.ui import colour_print
from .memory.hybrid_memory import HybridMemory
from .utils.config import Config

@dataclass
class InsertedContextAgent:
    max_completion_tokens: Optional[int] = None
    hybrid_memory: HybridMemory = None
    max_prompt_size: int = 16384
    is_local: bool = True
    defined_kws: list = field(default_factory=list)
    llm_call_idx: int = 0
    # model: str = "together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
    model: str ="gpt-4o-mini",
    temperature: float = 0.01
    system_message: str = "You are a helpful AI assistant."
    debug_level: int = 1
    session_id: Optional[str] = None
    now: datetime.datetime = None   # Set in `reply` to keep a consistent "now" timestamp
    run_name: str = ""
    num_tries: int = 5

    @property
    def save_name(self) -> str:
        sanitized_model = re.sub(r'[<>:"/\\|?*]', '_', self.model.replace('/', '-'))
        sanitized_timestamp = re.sub(r'[<>:"/\\|?*]', '_', self.init_timestamp.replace(':', '_'))
        return f"{sanitized_model}-{self.max_prompt_size}-{sanitized_timestamp}"

    def __post_init__(self):
        self.hybrid_memory = HybridMemory(Config.DATABASE_URL, Config.SEMANTIC_MEMORY_CONFIG, max_retrieve_capacity=2000)
        self.max_message_size = 1000
        self.defined_kws = []
        self.new_session()
        self.init_timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H:%M:%S")

    def save_interaction(self, user_message, response_message, keywords):
        self.hybrid_memory.add_interaction(self.session_id, user_message, response_message, self.now.timestamp(), keywords)        
  
    def new_session(self) -> str:
        self.session_id = str(uuid.uuid4())
        created_at = datetime.datetime.now().timestamp()
        self.hybrid_memory.create_session(self.session_id, created_at)
        if not self.hybrid_memory.is_empty():
            self.hybrid_memory.semantic_memory.add_separator()
        return self.session_id

    def reply(self, user_message: str, agent_response: Optional[str] = None, cost_callback: Callable = None) -> str:
        colour_print("CYAN", f"DEALING WITH USER MESSAGE: {user_message}")
        self.now = datetime.datetime.now()

        keywords = self.keywords_for_message(user_message, cost_cb=cost_callback)
        context = self.create_context(user_message, max_prompt_size=self.max_prompt_size, previous_interactions=0, cost_cb=cost_callback)
        response_text = ask_llm(context, model=self.model, max_overall_tokens=self.max_prompt_size, cost_callback=cost_callback, temperature=self.temperature)
        
        # Sanitize the response text
        sanitized_response = self.sanitize_string(response_text)
        
        try:
            log_llm_call(self.run_name, self.save_name, self.debug_level, label=f"reply-{self.llm_call_idx}")
        except UnicodeEncodeError:
            print(f"Warning: Unable to log LLM call due to encoding issues. LLM call index: {self.llm_call_idx}")
        
        self.llm_call_idx += 1

        # Save interaction to memory
        self.save_interaction(user_message, sanitized_response, keywords)

        return sanitized_response

    @staticmethod
    def sanitize_string(s: str) -> str:
        """Remove or replace problematic characters."""
        return ''.join(ch for ch in s if unicodedata.category(ch)[0] != 'C')

    def keywords_for_message(self, user_message, cost_cb):
        
        prompt = """Create two keywords to describe the topic of this message:
        "{user_message}".

Focus on the topic and tone of the message. Produce the keywords in JSON like: `["keyword_1", "keyword_2"]`

Choose keywords that would aid in retrieving this message from memory in the future.

Reuse these keywords if appropriate: {keywords}"""

        context = [make_system_message(prompt.format(user_message=user_message, keywords=self.defined_kws))]
        for _ in range(self.num_tries):
            try:
                print("Keyword gen")
                response = ask_llm(context, model=self.model, max_overall_tokens=self.max_prompt_size, cost_callback=cost_cb, temperature=self.temperature)

                keywords = [k.lower() for k in sanitize_and_parse_json(response)]
                break
            except Exception as e:
                print(repr(e) + response)
                continue

        # Update known list of keywords
        for k in keywords:
            if k not in self.defined_kws:
                self.defined_kws.append(k)

        print(f"Interaction keywords: {keywords}")
        return keywords

    def create_context(self, user_message, max_prompt_size, previous_interactions, cost_cb):
        context = [make_system_message(self.system_message), make_user_message(f"{str(self.now)[:-7]} ({td_format(datetime.timedelta(seconds=1))}) " + user_message)]
        relevant_interactions = self.get_relevant_memories(user_message, cost_cb)

        # Get interactions from the memories
        for m in relevant_interactions:
            if "trivia" in m[0].content:
                colour_print("YELLOW", f"<*** trivia ***>")
            else:
                colour_print("YELLOW", f"{m[0].content}")

        # Add the previous messages
        recent_messages = self.hybrid_memory.get_recent_messages(self.session_id, limit=previous_interactions)
        relevant_interactions.extend([(Message(**msg.__dict__), Message(**msg.__dict__)) for msg in recent_messages])

        # Add in memories up to the max prompt size
        current_size = token_counter(self.model, messages=context)
        shown_mems = 0
        target_size = max_prompt_size - self.max_message_size

        for interaction in reversed(relevant_interactions):
            user_interaction, assistant_interaction = interaction
            future_size = token_counter(self.model, messages=context + [user_interaction.as_llm_dict(), assistant_interaction.as_llm_dict()])

            # If this message is going to be too big, then skip it
            if shown_mems >= 100:
                break

            if future_size > target_size:
                continue

            # Add the interaction and count the tokens
            context.insert(1, assistant_interaction.as_llm_dict())

            ts = datetime.datetime.fromtimestamp(user_interaction.timestamp)
            et_descriptor = f"{str(ts)[:-7]} ({td_format(self.now - ts)}) "
            context.insert(1, user_interaction.as_llm_dict())
            context[1]["content"] = et_descriptor + context[1]["content"]

            shown_mems += 1

            current_size = future_size

        print(f"current context size: {current_size}")

        return context

    def llm_memory_filter(self, memories, queries, cost_cb):

        situation_prompt = """You are a part of an agent. Another part of the agent is currently searching for memories using the statements below.
Based on these statements, describe what is currently happening external to the agent in general terms:
{queries}  
"""

        prompt = """Here are a number of interactions, each is given a number:
{passages}         
*****************************************

Each of these interactions might be related to the general situation below. Your task is to judge if these interaction have any relation to the general situation.
Filter out interactions that very clearly do not have any relation. But keep in interactions that have any kind of relationship to the situation such as in: topic, characters, locations, setting, etc.

SITUATION:
{situation}

Express your answer in this JSON: 
[
    {{
        "number": int  // The number of the interaction.
        "justification": string  // Why the interaction is or is not related to the situation.
        "related": bool // Whether the interaction is related to the situation.
    }},
    ...
]
"""

        if len(memories) == 0:
            return []

        splice_length = 10

        filtered_interactions = []

        # Get the situation
        queries_txt = "- " + "\n- ".join(queries)
        context = [make_user_message(situation_prompt.format(queries=queries_txt))]
        situation = ask_llm(context, model=self.model, max_overall_tokens=self.max_prompt_size, cost_callback=cost_cb, temperature=self.temperature)
        colour_print("MAGENTA", f"Filtering situation: {situation}")

        # Map retrieved memory fac
        interactions_to_filter, interaction_keywords = self.interactions_from_retrieved_memories(memories)

        num_splices = ceil(len(interactions_to_filter) / splice_length)
        # Iterate through the interactions_to_filter list and create the passage
        call_count = 0
        for splice in range(num_splices):
            start_idx = splice * splice_length
            end_idx = (splice + 1) * splice_length

            memories_passages = []
            memory_counter = 0

            for interaction, keywords in zip(interactions_to_filter[start_idx:end_idx], interaction_keywords[start_idx:end_idx]):
                um, am = interaction
                memories_passages.append(f"[MEMORY NUMBER {memory_counter} START].\n (User): {um.content}\n(You): {am.content}\nKeywords: {keywords}\n[MEMORY NUMBER {memory_counter} END]")
                memory_counter += 1

            passages = "\n\n------------------------\n\n".join(memories_passages)
            context = [make_user_message(prompt.format(passages=passages, situation=situation))]

            for _ in range(self.num_tries):
                try:
                    print("Attempting filter")
                    result = ask_llm(context, model=self.model, max_overall_tokens=self.max_prompt_size, cost_callback=cost_cb, temperature=self.temperature)
                    log_llm_call(self.run_name, self.save_name, self.debug_level, label=f"reply-{self.llm_call_idx}-filter-{call_count}")

                    json_list = sanitize_and_parse_json(result)
                    for idx, selected_object in enumerate(json_list):
                        if selected_object["related"]:
                            filtered_interactions.append(interactions_to_filter[idx + start_idx])

                    call_count += 1
                    break
                except Exception as e:
                    print(e)
                    continue

        return filtered_interactions

    def get_relevant_memories(self, user_message, cost_cb):
        prompt = """Message from user: "{user_message}"
        ... (rest of the prompt remains the same)
        """
        context = [make_user_message(prompt.format(user_message=user_message, time=self.now, keywords=self.defined_kws))]
        all_retrieved_memories = []
        query_keywords = []

        for _ in range(self.num_tries):
            print("generating queries")
            response = ask_llm(context, model=self.model, max_overall_tokens=self.max_prompt_size, cost_callback=cost_cb, temperature=self.temperature)

            try:
                query_dict = sanitize_and_parse_json(response)
                query_keywords = [k.lower() for k in query_dict["keywords"]]
                print(f"Query keywords: {query_keywords}")

                all_retrieved_memories = []
                for q in query_dict["queries"] + [user_message]:
                    print(f"Querying with: {q}")
                    for mem in self.hybrid_memory.semantic_memory.retrieve(q, k=100):
                        all_retrieved_memories.append(mem)
                break
            except Exception:
                continue

        # Filter by both relevance and keywords
        all_keywords = query_keywords
        relevance_filtered_mems = [x for x in all_retrieved_memories if x.relevance > 0.6]
        keyword_filtered_mems = self.retrieve_from_keywords(all_keywords)
        
        combined_memories = relevance_filtered_mems + keyword_filtered_mems
        
        keyword_filtered_mems = []
        for m in combined_memories:
            if isinstance(m, RetrievedMemory):
                for kw in m.metadata.get("keywords", []):
                    if kw in all_keywords:
                        keyword_filtered_mems.append(m)
                        break
            elif isinstance(m, tuple) and len(m) == 2:
                user_message, _ = m
                for kw in user_message.metadata.get("keywords", []):
                    if kw in all_keywords:
                        keyword_filtered_mems.append(m)
                        break

        combined_memories = keyword_filtered_mems

        # Spreading activation
        for mem in combined_memories[:10]:
            if isinstance(mem, RetrievedMemory):
                query = mem.passage
            elif isinstance(mem, tuple) and len(mem) == 2:
                query = mem[0].content
            else:
                continue
            
            for r_mem in self.hybrid_memory.semantic_memory.retrieve(query, k=5):
                if r_mem.relevance > 0.6 and r_mem not in combined_memories:
                    combined_memories.append(r_mem)

        # # TODO: Uncomment all this stuff when doing dev stuff
        # trivia_skip = False
        # for kw in all_keywords:
        #     if "trivia" in kw:
        #         trivia_skip = True
        #
        # if trivia_skip:
        #     llm_filtered_interactions, _ = self.interactions_from_retrieved_memories(keyword_filtered_mems)
        # else:
        #     llm_filtered_interactions = self.llm_memory_filter(keyword_filtered_mems, query_dict["queries"], cost_cb)

        # TODO: ....And comment this one out
        llm_filtered_interactions = self.llm_memory_filter(combined_memories, query_dict["queries"], cost_cb)

        sorted_interactions = sorted(llm_filtered_interactions, key=lambda x: x[0].timestamp)
        return sorted_interactions


    def interactions_from_retrieved_memories(self, memory_chunks: List[Union[RetrievedMemory, Tuple[Message, Message]]]) -> Tuple[List[Tuple[Message, Message]], List[List[str]]]:
        interactions = []
        keywords = []
        for m in memory_chunks:
            if isinstance(m, RetrievedMemory):
                interaction = self.hybrid_memory.get_interaction_by_semantic_key(str(m.textKeys[0]))
                if interaction and interaction not in interactions:
                    interactions.append(interaction)
                    keywords.append(m.metadata.get("keywords", []))
            elif isinstance(m, tuple) and len(m) == 2:
                if m not in interactions:
                    interactions.append(m)
                    keywords.append(m[0].metadata.get("keywords", []))
        return interactions, keywords

    def retrieve_from_keywords(self, keywords):
        return self.hybrid_memory.retrieve_from_keywords(keywords)

    def reset(self):
        self.hybrid_memory.clear()
        self.new_session()

    def state_as_text(self) -> str:
        state = dict(
            model=self.model,
            max_prompt_size=self.max_prompt_size,
            max_completion_tokens=self.max_completion_tokens,
            hybrid_memory=self.hybrid_memory.state_as_text(),
            defined_kws=self.defined_kws,
            llm_call_idx=self.llm_call_idx
        )
        return json.dumps(state, cls=SimpleJSONEncoder)

    def from_state_text(self, state_text: str, prompt_callback: Callable[[str, str, list[dict], str], Any] = None):
        state = json.loads(state_text, cls=SimpleJSONDecoder)
        self.max_prompt_size = state["max_prompt_size"]
        self.max_completion_tokens = state["max_completion_tokens"]
        self.model = state["model"]
        self.hybrid_memory.set_state(state["hybrid_memory"])
        self.defined_kws = state["defined_kws"]
        self.llm_call_idx = state["llm_call_idx"]

    def get_message_history(self):
        return self.hybrid_memory.get_all_messages(self.session_id)

    def get_session(self):
        return self.hybrid_memory.get_session(self.session_id)

    def add_separator(self):
        self.hybrid_memory.semantic_memory.add_separator()

    @property
    def message_count(self):
        return self.hybrid_memory.get_message_count(self.session_id)

    def interaction_from_timestamp(self, timestamp: float) -> Tuple[Message, Message]:
        return self.hybrid_memory.get_interaction_by_timestamp(self.session_id, timestamp)

    def by_index(self, idx):
        messages = self.get_message_history()
        if 0 <= idx < len(messages) - 1:
            return (Message(**messages[idx].__dict__), Message(**messages[idx+1].__dict__))
        return None

### File: database\manager.py ###
from sqlalchemy import create_engine, desc
from sqlalchemy.orm import sessionmaker
from sqlalchemy.inspection import inspect
from .models import Base, Session, Message
from typing import List, Tuple, Optional
import json

class DatabaseManager:
    def __init__(self, db_url):
        self.engine = create_engine(db_url)
        Base.metadata.create_all(self.engine)
        self.SessionMaker = sessionmaker(bind=self.engine)

    def create_session(self, session_id, created_at):
        with self.SessionMaker() as db_session:
            new_session = Session(session_id=session_id, created_at=created_at)
            db_session.add(new_session)
            db_session.commit()
            return new_session

    def add_message(self, session_id, role, content, timestamp, semantic_key):
        with self.SessionMaker() as db_session:
            new_message = Message(
                session_id=session_id,
                role=role,
                content=content,
                timestamp=timestamp,
                semantic_key=semantic_key
            )
            db_session.add(new_message)
            db_session.commit()
            return new_message

    def get_recent_messages(self, session_id, limit=10) -> List[Message]:
        with self.SessionMaker() as db_session:
            return db_session.query(Message).filter(Message.session_id == session_id) \
                .order_by(desc(Message.timestamp)).limit(limit).all()

    def get_session(self, session_id) -> Optional[Session]:
        with self.SessionMaker() as db_session:
            return db_session.query(Session).filter(Session.session_id == session_id).first()

    def get_all_messages(self, session_id) -> List[Message]:
        with self.SessionMaker() as db_session:
            return db_session.query(Message).filter(Message.session_id == session_id) \
                .order_by(Message.timestamp).all()

    def get_interaction_by_semantic_key(self, semantic_key) -> Optional[Tuple[Message, Message]]:
        with self.SessionMaker() as db_session:
            messages = db_session.query(Message).filter(Message.semantic_key == semantic_key).order_by(Message.timestamp).limit(2).all()
            if len(messages) == 2:
                return (messages[0], messages[1])
            return None

    def get_messages_by_timestamp(self, session_id, timestamp) -> List[Message]:
        with self.SessionMaker() as db_session:
            return db_session.query(Message).filter(Message.session_id == session_id, Message.timestamp == timestamp).all()

    def clear_all(self):
        with self.SessionMaker() as db_session:
            db_session.query(Message).delete()
            db_session.query(Session).delete()
            db_session.commit()

    def export_data(self) -> str:
        with self.SessionMaker() as db_session:
            sessions = db_session.query(Session).all()
            messages = db_session.query(Message).all()
            data = {
                "sessions": [self.object_as_dict(s) for s in sessions],
                "messages": [self.object_as_dict(m) for m in messages]
            }
            return json.dumps(data)

    @staticmethod
    def object_as_dict(obj):
        return {c.key: getattr(obj, c.key)
                for c in inspect(obj).mapper.column_attrs}

    def import_data(self, data_str: str):
        data = json.loads(data_str)
        with self.SessionMaker() as db_session:
            for session_data in data["sessions"]:
                session = Session(**session_data)
                db_session.add(session)
            for message_data in data["messages"]:
                message = Message(**message_data)
                db_session.add(message)
            db_session.commit()

    def get_message_count(self, session_id) -> int:
        with self.SessionMaker() as db_session:
            return db_session.query(Message).filter(Message.session_id == session_id).count()

### File: database\models.py ###
from sqlalchemy import Column, Integer, String, Float, ForeignKey
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship

Base = declarative_base()

class Session(Base):
    __tablename__ = 'sessions'

    id = Column(Integer, primary_key=True)
    session_id = Column(String, unique=True, nullable=False)
    created_at = Column(Float, nullable=False)

    messages = relationship("Message", back_populates="session")

    def __repr__(self):
        return f"<Session(id={self.id}, session_id={self.session_id}, created_at={self.created_at})>"

class Message(Base):
    __tablename__ = 'messages'

    id = Column(Integer, primary_key=True)
    session_id = Column(String, ForeignKey('sessions.session_id'), nullable=False)
    role = Column(String, nullable=False)
    content = Column(String, nullable=False)
    timestamp = Column(Float, nullable=False)
    semantic_key = Column(String, nullable=False)

    session = relationship("Session", back_populates="messages")

    def __repr__(self):
        return f"<Message(id={self.id}, session_id={self.session_id}, role={self.role}, timestamp={self.timestamp})>"

### File: memory\hybrid_memory.py ###
from goodai.ltm.mem.auto import AutoTextMemory
from goodai.ltm.mem.config import TextMemoryConfig
from goodai.ltm.mem.base import RetrievedMemory
from ..database.manager import DatabaseManager
from typing import List, Tuple, Optional, Union
from model_interfaces.base_ltm_agent import Message
import json

class HybridMemory:
    def __init__(self, db_url: str, semantic_memory_config: TextMemoryConfig, max_retrieve_capacity: int = 1000):
        self.db_manager = DatabaseManager(db_url)
        self.semantic_memory = AutoTextMemory.create(config=semantic_memory_config)
        self.max_retrieve_capacity = max_retrieve_capacity
        
    def create_session(self, session_id: str, created_at: float):
        return self.db_manager.create_session(session_id, created_at)

    def add_interaction(self, session_id: str, user_message: str, assistant_message: str, timestamp: float, keywords: list[str]):
        user_semantic_key = self.semantic_memory.add_text(user_message, timestamp=timestamp, metadata={"keywords": keywords})
        self.semantic_memory.add_separator()
        assistant_semantic_key = self.semantic_memory.add_text(assistant_message, timestamp=timestamp, metadata={"keywords": keywords})
        self.semantic_memory.add_separator()

        self.db_manager.add_message(session_id, "user", user_message, timestamp, str(user_semantic_key))
        self.db_manager.add_message(session_id, "assistant", assistant_message, timestamp, str(assistant_semantic_key))

    def get_relevant_memories(self, query: str, all_keywords: List[str], cost_cb, k: int = 100) -> List[Tuple[Message, Message]]:
        all_retrieved_memories = []
        for q in [query]:  # You might want to add more queries here
            all_retrieved_memories.extend(self.semantic_memory.retrieve(q, k=k))
        
        # Relevance and keyword filtering
        relevance_filtered_mems = [x for x in all_retrieved_memories if x.relevance > 0.6]
        keyword_filtered_mems = self.retrieve_from_keywords(all_keywords)
        
        combined_memories = relevance_filtered_mems + keyword_filtered_mems
        
        # Additional keyword filtering
        keyword_filtered_mems = []
        for m in combined_memories:
            if any(kw in m.metadata.get("keywords", []) for kw in all_keywords):
                keyword_filtered_mems.append(m)
        
        # Spreading activation
        for mem in keyword_filtered_mems[:10]:
            for r_mem in self.semantic_memory.retrieve(mem.passage, k=5):
                if r_mem.relevance > 0.6 and r_mem not in keyword_filtered_mems:
                    keyword_filtered_mems.append(r_mem)
        
        # Convert RetrievedMemory objects to Message tuples
        converted_results = []
        for mem in keyword_filtered_mems:
            interaction = self.get_interaction_by_semantic_key(str(mem.textKeys[0]))
            if interaction:
                converted_results.append(interaction)
        
        # Implement LLM memory filter here (you'll need to add this method to the class)
        filtered_interactions = self.llm_memory_filter(converted_results, [query], cost_cb)
        
        return filtered_interactions

    def get_interaction_by_semantic_key(self, semantic_key: str) -> Optional[Tuple[Message, Message]]:
        try:
            interaction = self.db_manager.get_interaction_by_semantic_key(semantic_key)
            if interaction:
                user_message = Message(
                    role=interaction[0].role,
                    content=interaction[0].content,
                    timestamp=interaction[0].timestamp
                )
                assistant_message = Message(
                    role=interaction[1].role,
                    content=interaction[1].content,
                    timestamp=interaction[1].timestamp
                )
                return (user_message, assistant_message)
        except Exception as e:
            print(f"Error retrieving interaction: {e}")
        return None

    def get_interaction_by_timestamp(self, session_id: str, timestamp: float) -> Optional[Tuple[Message, Message]]:
        messages = self.db_manager.get_messages_by_timestamp(session_id, timestamp)
        if len(messages) == 2:
            return (Message(**messages[0].__dict__), Message(**messages[1].__dict__))
        return None

    def get_recent_messages(self, session_id: str, limit: int) -> List[Message]:
        messages = self.db_manager.get_recent_messages(session_id, limit)
        return [Message(role=m.role, content=m.content, timestamp=m.timestamp) for m in messages]

    def get_all_messages(self, session_id: str) -> List[Message]:
        messages = self.db_manager.get_all_messages(session_id)
        return [Message(role=m.role, content=m.content, timestamp=m.timestamp) for m in messages]

    def get_session(self, session_id: str):
        return self.db_manager.get_session(session_id)

    def is_empty(self) -> bool:
        return self.semantic_memory.is_empty()

    def clear(self):
        self.semantic_memory.clear()
        self.db_manager.clear_all()

    def state_as_text(self) -> str:
        try:
            return json.dumps({
                "semantic_memory": self.semantic_memory.state_as_text(),
                "database": self.db_manager.export_data()
            })
        except Exception as e:
            print(f"Error in state_as_text: {e}")
            return json.dumps({"error": str(e)})

    def set_state(self, state_text: str):
        try:
            state = json.loads(state_text)
            if "error" in state:
                print(f"Error in previous state: {state['error']}")
                return
            self.semantic_memory.set_state(state["semantic_memory"])
            self.db_manager.import_data(state["database"])
        except Exception as e:
            print(f"Error setting state: {e}")

    def retrieve_from_keywords(self, keywords: List[str]) -> List[Union[RetrievedMemory, Tuple[Message, Message]]]:
        all_memories = self.semantic_memory.retrieve("", k=self.max_retrieve_capacity)
        keyword_filtered_mems = []
        for memory in all_memories:
            if any(kw in memory.metadata.get("keywords", []) for kw in keywords):
                keyword_filtered_mems.append(memory)
        return keyword_filtered_mems

    def get_message_count(self, session_id: str) -> int:
        return self.db_manager.get_message_count(session_id)
    

### File: utils\config.py ###
from goodai.ltm.mem.config import TextMemoryConfig

class Config:
    DATABASE_URL = "sqlite:///ltm_agent.db"
    SEMANTIC_MEMORY_CONFIG = TextMemoryConfig(
        chunk_capacity=50,
        chunk_overlap_fraction=0.0,
    )

